{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b912966-4dce-4685-a5b2-a39c5229a0f1",
   "metadata": {},
   "source": [
    "# Storm Research Assistant\n",
    "\n",
    "Reference\n",
    "https://github.com/langchain-ai/langgraph/blob/main/examples/storm/storm.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54832538-aa97-4e40-9713-eaae1e62852a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prereqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60732872-a587-4384-ad96-3ba16facbbe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %pip install -U langchain_community langchain_openai langgraph wikipedia  scikit-learn  langchain_fireworks\n",
    "# We use one or the other search engine below\n",
    "# %pip install -U tavily-python\n",
    "# %pip install -U duckduckgo-search\n",
    "# ! apt-get install graphviz graphviz-dev\n",
    "# %pip install pygraphviz\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62647209-990c-4cc1-a1ac-22e46e7ecc31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from storm import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f32108c2-977f-450d-82cb-90aa21f09171",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "fast_llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "# long_context_llm = ChatOpenAI(model=\"gpt-4-turbo-preview\")\n",
    "long_context_llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\n",
    "\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "vectorstore = Chroma\n",
    "\n",
    "\n",
    "interview_config = InterviewConfig(long_llm=long_context_llm, fast_llm=fast_llm, \n",
    "                                   max_conversations=5, tags_to_extract=[ \"p\", \"h1\", \"h2\", \"h3\"],\n",
    "                                   vectorstore=None,\n",
    "                                   embeddings=embeddings\n",
    "                                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e530943c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_name(name: str) -> str:\n",
    "\n",
    "    # Remove all non-alphanumeric characters\n",
    "    name = re.sub(r\"[^a-zA-Z0-9_-]\", \"\", name)\n",
    "\n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b278d8c-9e34-42ab-9649-bc7b3570bb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Generate Initial Outline\n",
    "\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from typing import List, Optional\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "\n",
    "direct_gen_outline_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a Wikipedia writer. Write an outline for a Wikipedia page about a user-provided topic. Be comprehensive and specific.\",\n",
    "        ),\n",
    "        (\"user\", \"{topic}\\n{format_instructions}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "class Subsection(BaseModel):\n",
    "    subsection_title: str = Field(..., title=\"Title of the subsection\")\n",
    "    description: str = Field(..., title=\"Content of the subsection\")\n",
    "\n",
    "    @property\n",
    "    def as_str(self) -> str:\n",
    "        return f\"### {self.subsection_title}\\n\\n{self.description}\".strip()\n",
    "\n",
    "\n",
    "class Section(BaseModel):\n",
    "    section_title: str = Field(..., title=\"Title of the section\")\n",
    "    description: str = Field(..., title=\"Content of the section\")\n",
    "    subsections: Optional[List[Subsection]] = Field(\n",
    "        default=None,\n",
    "        title=\"Titles and descriptions for each subsection of the Wikipedia page.\",\n",
    "    )\n",
    "\n",
    "    @property\n",
    "    def as_str(self) -> str:\n",
    "        subsections = \"\\n\\n\".join(\n",
    "            f\"### {subsection.subsection_title}\\n\\n{subsection.description}\"\n",
    "            for subsection in self.subsections or []\n",
    "        )\n",
    "        return f\"## {self.section_title}\\n\\n{self.description}\\n\\n{subsections}\".strip()\n",
    "\n",
    "\n",
    "class Outline(BaseModel):\n",
    "    page_title: str = Field(..., title=\"Title of the Wikipedia page\")\n",
    "    sections: List[Section] = Field(\n",
    "        default_factory=list,\n",
    "        title=\"Titles and descriptions for each section of the Wikipedia page.\",\n",
    "    )\n",
    "\n",
    "    @property\n",
    "    def as_str(self) -> str:\n",
    "        sections = \"\\n\\n\".join(section.as_str for section in self.sections)\n",
    "        return f\"# {self.page_title}\\n\\n{sections}\".strip()\n",
    "\n",
    "\n",
    "outline_parser = PydanticOutputParser(pydantic_object=Outline)\n",
    "\n",
    "generate_outline_direct = direct_gen_outline_prompt.partial(format_instructions=outline_parser.get_format_instructions()) | fast_llm | outline_parser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90ee8329-896b-4085-a1fa-fec0a15937ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Impact of million-plus token context window language models on RAG\n",
      "\n",
      "## Introduction\n",
      "\n",
      "Overview of million-plus token context window language models and RAG (Retrieval-Augmented Generation) framework.\n",
      "\n",
      "## Background\n",
      "\n",
      "Explanation of language models with million-plus token context windows and their impact on natural language processing tasks.\n",
      "\n",
      "## RAG Framework\n",
      "\n",
      "Detailed description of the Retrieval-Augmented Generation framework and its components.\n",
      "\n",
      "## Impact on RAG\n",
      "\n",
      "Analysis of how million-plus token context window language models enhance the performance of the RAG framework.\n",
      "\n",
      "## Applications\n",
      "\n",
      "Exploration of the practical applications of integrating million-plus token context window language models with the RAG framework.\n",
      "\n",
      "## Challenges and Considerations\n",
      "\n",
      "Discussion on the challenges and considerations when using million-plus token context window language models in the RAG framework.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "example_topic = \"Impact of million-plus token context window language models on RAG\"\n",
    "\n",
    "initial_outline = generate_outline_direct.invoke({\"topic\": example_topic})\n",
    "\n",
    "print(initial_outline.as_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "64c37266-d5d5-4fbb-831f-8f809e966236",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Expand Topics\\\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "27f63ad0-7e07-48ac-85a9-80a53b528c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_related_topics_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"I'm writing a Wikipedia page for a topic mentioned below. Please identify and recommend some Wikipedia pages on closely related subjects. I'm looking for examples that provide insights into interesting aspects commonly associated with this topic, or examples that help me understand the typical content and structure included in Wikipedia pages for similar topics.\n",
    "\n",
    "Please list the as many subjects and urls as you can.\n",
    "\n",
    "Topic of interest: {topic}\n",
    "{format_instructions}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "class RelatedSubjects(BaseModel):\n",
    "    topics: List[str] = Field(\n",
    "        description=\"Comprehensive list of related subjects as background research.\",\n",
    "    )\n",
    "\n",
    "\n",
    "related_topics_parser = PydanticOutputParser(pydantic_object=RelatedSubjects)\n",
    "\n",
    "expand_chain = gen_related_topics_prompt.partial(format_instructions=related_topics_parser.get_format_instructions()) | fast_llm | related_topics_parser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f801936-f6f7-44a0-bc79-4f0132fba79b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RelatedSubjects(topics=['Language model', 'Retriever-Reader model', 'Natural language processing', 'Context window', 'Transformer (machine learning model)', 'Information retrieval', 'Knowledge graph', 'Question answering', 'BERT (language model)', 'GPT-3 (language model)'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "related_subjects = await expand_chain.ainvoke({\"topic\": example_topic})\n",
    "related_subjects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95cc42a4-45f1-470c-a3f6-20a5661d5b43",
   "metadata": {},
   "source": [
    "## Generate Perspectives\n",
    "\n",
    "From these related subjects, we can select representative Wikipedia editors as \"subject matter experts\" with distinct backgrounds and affiliations. These will help distribute the search process to encourage a more well-rounded final report.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d79ea824-c561-4949-bbd4-127281f3eeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Editor(BaseModel):\n",
    "    affiliation: str = Field(\n",
    "        description=\"Primary affiliation of the editor.\",\n",
    "    )\n",
    "    name: str = Field(\n",
    "        description=\"Name of the editor.\",\n",
    "    )\n",
    "    role: str = Field(\n",
    "        description=\"Role of the editor in the context of the topic.\",\n",
    "    )\n",
    "    description: str = Field(\n",
    "        description=\"Description of the editor's focus, concerns, and motives.\",\n",
    "    )\n",
    "\n",
    "    @property\n",
    "    def persona(self) -> str:\n",
    "        return f\"Name: {self.name}\\nRole: {self.role}\\nAffiliation: {self.affiliation}\\nDescription: {self.description}\\n\"\n",
    "\n",
    "\n",
    "class Perspectives(BaseModel):\n",
    "    editors: List[Editor] = Field(\n",
    "        description=\"Comprehensive list of editors with their roles and affiliations.\",\n",
    "        # Add a pydantic validation/restriction to be at most M editors\n",
    "    )\n",
    "\n",
    "gen_perspectives_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"You need to select a diverse (and distinct) group of Wikipedia editors who will work together to create a comprehensive article on the topic. Each of them represents a different perspective, role, or affiliation related to this topic.\\\n",
    "    You can use other Wikipedia pages of related topics for inspiration. For each editor, add a description of what they will focus on.\n",
    "\n",
    "    Wiki page outlines of related topics for inspiration:\n",
    "    {examples}\"\"\",\n",
    "        ),\n",
    "        (\"user\", \"Topic of interest: {topic}\\n\\n{format_instructions}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "perspectives_parser = PydanticOutputParser(pydantic_object=Perspectives)\n",
    "\n",
    "gen_perspectives_chain = gen_perspectives_prompt.partial(format_instructions=perspectives_parser.get_format_instructions()) | fast_llm | perspectives_parser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "df5a87f7-a867-42f7-ac98-da7ce1110daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.retrievers import WikipediaRetriever\n",
    "from langchain_core.runnables import RunnableLambda, chain as as_runnable\n",
    "\n",
    "wikipedia_retriever = WikipediaRetriever(load_all_available_meta=True, top_k_results=1)\n",
    "\n",
    "\n",
    "def format_doc(doc, max_length=1000)-> str:\n",
    "    related = \"- \".join(doc.metadata[\"categories\"])\n",
    "    return f\"### {doc.metadata['title']}\\n\\nSummary: {doc.page_content}\\n\\nRelated\\n{related}\"[\n",
    "        :max_length\n",
    "    ]\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(format_doc(doc) for doc in docs)\n",
    "\n",
    "\n",
    "@as_runnable\n",
    "async def survey_subjects(topic: str)-> Perspectives:\n",
    "    print(f\"Survey Subjects for Topic: {topic}\")\n",
    "    related_subjects = await expand_chain.ainvoke({\"topic\": topic})\n",
    "    retrieved_docs = await wikipedia_retriever.abatch(\n",
    "        related_subjects.topics, return_exceptions=True\n",
    "    )\n",
    "    all_docs = []\n",
    "    for docs in retrieved_docs:\n",
    "        if isinstance(docs, BaseException):\n",
    "            continue\n",
    "        all_docs.extend(docs)\n",
    "    print(f\"Retrieved {len(all_docs)} docs for Topic: {topic}\")\n",
    "    \n",
    "    formatted = format_docs(all_docs)\n",
    "    return await gen_perspectives_chain.ainvoke({\"examples\": formatted, \"topic\": topic})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8ff2b5eb-46cb-410f-8b15-6fc8ead58382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Survey Subjects for Topic: Impact of million-plus token context window language models on RAG\n",
      "Retrieved 5 docs for Topic: Impact of million-plus token context window language models on RAG\n"
     ]
    }
   ],
   "source": [
    "perspectives = await survey_subjects.ainvoke(example_topic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fc90dd94-b215-4a5d-83b2-498e969ff7b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'editors': [{'affiliation': 'Research Institution',\n",
       "   'name': 'Dr. Data Scientist',\n",
       "   'role': 'Researcher',\n",
       "   'description': 'Dr. Data Scientist specializes in analyzing the impact of million-plus token context window language models on the Retrieval-Augmented Generation (RAG) approach. They focus on evaluating the effectiveness of these models in enhancing RAG capabilities and understanding how they influence information retrieval and generation processes.'},\n",
       "  {'affiliation': 'Tech Company',\n",
       "   'name': 'AI Engineer',\n",
       "   'role': 'Engineer',\n",
       "   'description': 'AI Engineer works on implementing million-plus token context window language models in RAG systems. Their role involves optimizing the integration of these models into existing RAG frameworks, ensuring efficient computation and performance while utilizing the extended context window for improved retrieval and generation tasks.'},\n",
       "  {'affiliation': 'Academic Institution',\n",
       "   'name': 'Professor Linguist',\n",
       "   'role': 'Linguistics Expert',\n",
       "   'description': 'Professor Linguist contributes a linguistic perspective to the impact analysis of million-plus token context window language models on RAG. They explore how these models handle linguistic nuances, syntactic structures, and semantic coherence within the context of information retrieval and generation, aiming to enhance the linguistic quality of RAG outputs.'},\n",
       "  {'affiliation': 'Open Source Community',\n",
       "   'name': 'Tech Enthusiast',\n",
       "   'role': 'Contributor',\n",
       "   'description': 'Tech Enthusiast actively participates in the development and optimization of open-source tools that leverage million-plus token context window language models for RAG applications. They focus on fostering collaboration, enhancing accessibility, and promoting transparency in the utilization of these advanced models within the open-source community.'}]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "perspectives.dict()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35abdf40-80dc-434c-8116-d6b9cbde5572",
   "metadata": {},
   "source": [
    "## Expert Dialog\n",
    "\n",
    "Each wikipedia writer is primed to role-play using the perspectives presented above. It will ask a series of questions of a second \"domain expert\" with access to a search engine. This generate content to generate a refined outline as well as an updated index of reference documents.\n",
    "\n",
    "### Interview State\n",
    "\n",
    "The conversation is cyclic, so we will construct it within its own graph. The State will contain messages, the reference docs, and the editor (with its own \"persona\") to make it easy to parallelize these conversations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9a1fa250-8b81-465b-ba06-fe7b33e4d8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "from typing_extensions import TypedDict\n",
    "from langchain_core.messages import AnyMessage\n",
    "from typing import Annotated, Sequence\n",
    "\n",
    "\n",
    "def add_messages(left, right):\n",
    "    if not isinstance(left, list):\n",
    "        left = [left]\n",
    "    if not isinstance(right, list):\n",
    "        right = [right]\n",
    "    return left + right\n",
    "\n",
    "\n",
    "def update_references(references, new_references):\n",
    "    if not references:\n",
    "        references = {}\n",
    "    references.update(new_references)\n",
    "    return references\n",
    "\n",
    "\n",
    "def update_editor(editor, new_editor):\n",
    "    # Can only set at the outset\n",
    "    if not editor:\n",
    "        return new_editor\n",
    "    return editor\n",
    "\n",
    "\n",
    "class InterviewState(TypedDict):\n",
    "    messages: Annotated[List[AnyMessage], add_messages]\n",
    "    references: Annotated[Optional[dict], update_references]\n",
    "    editor: Annotated[Optional[Editor], update_editor]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0639d2-e8a6-43d4-8e9a-eb5f4051578c",
   "metadata": {},
   "source": [
    "# Dialog Roles\n",
    "\n",
    "The graph will have two participants: the wikipedia editor (generate_question), who asks questions based on its assigned role, and a domain expert (`gen_answer_chain), who uses a search engine to answer the questions as accurately as possible.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d644c46c-4d23-49e1-9093-39b4f6c8c157",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "from langchain_core.messages import AIMessage, BaseMessage, HumanMessage, ToolMessage\n",
    "\n",
    "\n",
    "gen_qn_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"You are an experienced Wikipedia writer and want to edit a specific page. \\\n",
    "Besides your identity as a Wikipedia writer, you have a specific focus when researching the topic. \\\n",
    "Now, you are chatting with an expert to get information. Ask good questions to get more useful information.\n",
    "\n",
    "When you have no more questions to ask, say \"Thank you so much for your help!\" to end the conversation.\\\n",
    "Please only ask one question at a time and don't ask what you have asked before.\\\n",
    "Your questions should be related to the topic you want to write.\n",
    "Be comprehensive and curious, gaining as much unique insight from the expert as possible.\\\n",
    "\n",
    "Stay true to your specific perspective:\n",
    "\n",
    "{persona}\"\"\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\", optional=True),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "def tag_with_name(ai_message: AIMessage, name: str) -> AIMessage:\n",
    "    ai_message.name = name\n",
    "    return ai_message\n",
    "\n",
    "\n",
    "def swap_roles(state: InterviewState, name: str) -> InterviewState:\n",
    "\n",
    "    # Normalize name\n",
    "    name = cleanup_name(name)\n",
    "\n",
    "    print(f'Swapping roles for {name}')\n",
    "\n",
    "    converted = []\n",
    "    for message in state[\"messages\"]:\n",
    "        if isinstance(message, AIMessage) and message.name != name:\n",
    "            message = HumanMessage(**message.dict(exclude={\"type\"}))\n",
    "        converted.append(message)\n",
    "    \n",
    "    print(f'Converted messages for {name} while swapping roles: {len(converted)} messages')\n",
    "\n",
    "    return {\"messages\": converted}\n",
    "\n",
    "\n",
    "@as_runnable\n",
    "async def generate_question(state: InterviewState) -> InterviewState:\n",
    "    editor = state[\"editor\"]\n",
    "\n",
    "    name = cleanup_name(editor.name)\n",
    "\n",
    "    print(f'Generating question for {name}')\n",
    "\n",
    "    gn_chain = (\n",
    "        RunnableLambda(swap_roles).bind(name=name)\n",
    "        | gen_qn_prompt.partial(persona=editor.persona)\n",
    "        | fast_llm\n",
    "        | RunnableLambda(tag_with_name).bind(name=name)\n",
    "    )\n",
    "    result:AIMessage = await gn_chain.ainvoke(state)\n",
    "\n",
    "    print(f'Generated question for {name}')\n",
    "    return {\"messages\": [result]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "29ac2384-a123-467f-bbc6-828be9dc04fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating question for DrDataScientist\n",
      "Swapping roles for DrDataScientist\n",
      "Converted messages for DrDataScientist while swapping roles: 1 messages\n",
      "Generated question for DrDataScientist\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Yes, that's correct. I am researching the impact of million-plus token context window language models on the Retrieval-Augmented Generation (RAG) approach. These language models have the capability to analyze vast amounts of text data within a wide context window, which can potentially enhance the performance of RAG systems. I am interested in understanding how these models influence information retrieval and generation processes within the RAG framework. Is there any specific aspect of this topic that you would like me to elaborate on?\", response_metadata={'token_usage': {'completion_tokens': 99, 'prompt_tokens': 238, 'total_tokens': 337}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_3bc1b5746c', 'finish_reason': 'stop', 'logprobs': None}, name='DrDataScientist')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "    HumanMessage(f\"So you said you were writing an article on {example_topic}?\")\n",
    "]\n",
    "question = await generate_question.ainvoke(\n",
    "    {\n",
    "        \"editor\": perspectives.editors[0],\n",
    "        \"messages\": messages,\n",
    "    }\n",
    ")\n",
    "\n",
    "question[\"messages\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95e50f7-4e2c-4307-853d-2fc8f0b3dd82",
   "metadata": {},
   "source": [
    "### Answer questions\n",
    "\n",
    "The `gen_answer_chain` first generates queries (query expansion) to answer the editor's question, then responds with citations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a7fa969e-920f-487e-b124-6495777a1466",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Queries(BaseModel):\n",
    "    queries: List[str] = Field(\n",
    "        description=\"Comprehensive list of search engine queries to answer the user's questions.\",\n",
    "    )\n",
    "\n",
    "\n",
    "gen_queries_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful research assistant. Query the search engine to answer the user's questions.\\n{format_instructions}\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\", optional=True),\n",
    "    ]\n",
    ")\n",
    "\n",
    "queries_parser = PydanticOutputParser(pydantic_object=Queries)\n",
    "\n",
    "gen_queries_chain = gen_queries_prompt.partial(format_instructions=queries_parser.get_format_instructions()) | fast_llm | queries_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b25baa82-da8e-41ec-b4e0-8a24c7cf737d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Queries(queries=['Impact of million-plus token context window language models on Retrieval-Augmented Generation (RAG) approach', 'Capabilities of million-plus token context window language models in analyzing vast amounts of text data within a wide context window', 'Potential enhancements in performance of RAG systems due to million-plus token context window language models', 'Influence of million-plus token context window language models on information retrieval process in RAG framework', 'Influence of million-plus token context window language models on generation process in RAG framework'])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "queries = await gen_queries_chain.ainvoke(\n",
    "    {\"messages\": [HumanMessage(content=question[\"messages\"][0].content)]}\n",
    ")\n",
    "\n",
    "queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "22273c9a-a505-40c3-bd6b-70f74393a61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class AnswerWithCitations(BaseModel):\n",
    "    answer: str = Field(\n",
    "        description=\"Comprehensive answer to the user's question with citations.\",\n",
    "    )\n",
    "    cited_urls: List[str] = Field(\n",
    "        description=\"List of urls cited in the answer.\",\n",
    "    )\n",
    "\n",
    "    @property\n",
    "    def as_str(self) -> str:\n",
    "        return f\"{self.answer}\\n\\nCitations:\\n\\n\" + \"\\n\".join(\n",
    "            f\"[{i+1}]: {url}\" for i, url in enumerate(self.cited_urls)\n",
    "        )\n",
    "\n",
    "\n",
    "gen_answer_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"You are an expert who can use information effectively. You are chatting with a Wikipedia writer who wants\\\n",
    " to write a Wikipedia page on the topic you know. You have gathered the related information and will now use the information to form a response.\n",
    "\n",
    "Make your response as informative as possible and make sure every sentence is supported by the gathered information.\n",
    "Each response must be backed up by a citation from a reliable source, formatted as a footnote, reproducing the URLS after your response.\n",
    "{format_instructions}\"\"\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\", optional=True),\n",
    "    ]\n",
    ")\n",
    "\n",
    "ac_parser = PydanticOutputParser(pydantic_object=AnswerWithCitations)\n",
    "\n",
    "gen_answer_chain = gen_answer_prompt.partial(format_instructions=ac_parser.get_format_instructions()) | fast_llm | ac_parser \n",
    "\n",
    "# .with_structured_output(\n",
    "#     AnswerWithCitations, include_raw=True\n",
    "# ).with_config(run_name=\"GenerateAnswer\")\n",
    "                                             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1a39c4f4-f2ca-4504-84f9-ff1a86b01d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.utilities.duckduckgo_search import DuckDuckGoSearchAPIWrapper\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "# DDG \n",
    "search_engine = DuckDuckGoSearchAPIWrapper()\n",
    "\n",
    "@tool\n",
    "async def search_engine(query: str):\n",
    "    \"\"\"Search engine to the internet.\"\"\"\n",
    "\n",
    "    print(f\"Searching DuckDuckGo for [{query}]\")\n",
    "\n",
    "    results = DuckDuckGoSearchAPIWrapper()._ddgs_text(query)\n",
    "\n",
    "    print(f\"Got search engine results: {len(results)} for [{query}]\")\n",
    "    \n",
    "    return [{\"content\": r[\"body\"], \"url\": r[\"href\"]} for r in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ca74edcc-a272-4ec1-92fc-756bf0690e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableConfig\n",
    "import json, re\n",
    "\n",
    "\n",
    "async def gen_answer(\n",
    "    state: InterviewState,\n",
    "    config: Optional[RunnableConfig] = None,\n",
    "    name: str = \"SubjectMatterExpert\",\n",
    "    max_str_len: int = 15000,\n",
    "):\n",
    "    name = cleanup_name(name)\n",
    "\n",
    "    print(f'Generating answers for [{name}]')\n",
    "\n",
    "\n",
    "    swapped_state = swap_roles(state, name)  # Convert all other AI messages\n",
    "    \n",
    "    queries:Queries = await gen_queries_chain.ainvoke(swapped_state)\n",
    "\n",
    "    print(f\"Got {len(queries.queries)} search engine queries for [{name}]\")\n",
    "\n",
    "    query_results = await search_engine.abatch(\n",
    "        queries.queries, config, return_exceptions=True\n",
    "    )\n",
    "    successful_results = [\n",
    "        res for res in query_results if not isinstance(res, Exception)\n",
    "    ]\n",
    "\n",
    "    print(f\"Got {len(successful_results)} search engine results for [{name}]\")\n",
    "\n",
    "    all_query_results = {\n",
    "        res[\"url\"]: res[\"content\"] for results in successful_results for res in results\n",
    "    }\n",
    "\n",
    "    # We could be more precise about handling max token length if we wanted to here\n",
    "    dumped = json.dumps(all_query_results)[:max_str_len]\n",
    "    \n",
    "    ai_message: AIMessage = str(queries)\n",
    "    # print(f\"Got {ai_message} for [{name}]\")\n",
    "    \n",
    "    # tool_call = queries[\"raw\"].additional_kwargs[\"tool_calls\"][0]\n",
    "    # tool_id = tool_call[\"id\"]\n",
    "\n",
    "    # tool_message = ToolMessage(tool_call_id=tool_id, content=dumped)\n",
    "    tool_message = HumanMessage(content=dumped)\n",
    "\n",
    "    swapped_state[\"messages\"].extend([ai_message, tool_message])\n",
    "    \n",
    "    # Only update the shared state with the final answer to avoid\n",
    "    # polluting the dialogue history with intermediate messages\n",
    "    try:\n",
    "        generated: AnswerWithCitations = await gen_answer_chain.ainvoke(swapped_state)\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating answer for [{name}] - {e}\")\n",
    "        generated = AnswerWithCitations(answer=\"\", cited_urls=[])\n",
    "    \n",
    "    cited_urls = set(generated.cited_urls)\n",
    "    \n",
    "    # Save the retrieved information to a the shared state for future reference\n",
    "    cited_references = {k: v for k, v in all_query_results.items() if k in cited_urls}\n",
    "    \n",
    "    formatted_message = AIMessage(name=name, content=generated.as_str)\n",
    "\n",
    "    print(f'Finished generating answer for [{name}]')\n",
    "    return {\"messages\": [formatted_message], \"references\": cited_references}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "934aafd6-7f0d-4a1b-8a52-45b89d8b3742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating answers for [SubjectMatterExpert]\n",
      "Swapping roles for SubjectMatterExpert\n",
      "Converted messages for SubjectMatterExpert while swapping roles: 1 messages\n",
      "Got 5 search engine queries for [SubjectMatterExpert]\n",
      "Searching DuckDuckGo for [Impact of million-plus token context window language models on the Retrieval-Augmented Generation (RAG) approach]\n",
      "Got search engine results: 5 for [Impact of million-plus token context window language models on the Retrieval-Augmented Generation (RAG) approach]\n",
      "Searching DuckDuckGo for [Capabilities of million-plus token context window language models in analyzing vast amounts of text data within a wide context window]\n",
      "Got search engine results: 5 for [Capabilities of million-plus token context window language models in analyzing vast amounts of text data within a wide context window]\n",
      "Searching DuckDuckGo for [Potential enhancements in performance of RAG systems due to million-plus token context window language models]\n",
      "Got search engine results: 5 for [Potential enhancements in performance of RAG systems due to million-plus token context window language models]\n",
      "Searching DuckDuckGo for [Influence of million-plus token context window language models on information retrieval process in the RAG framework]\n",
      "Got search engine results: 5 for [Influence of million-plus token context window language models on information retrieval process in the RAG framework]\n",
      "Searching DuckDuckGo for [Influence of million-plus token context window language models on generation process in the RAG framework]\n",
      "Got search engine results: 5 for [Influence of million-plus token context window language models on generation process in the RAG framework]\n",
      "Got 5 search engine results for [SubjectMatterExpert]\n",
      "Finished generating answer for [SubjectMatterExpert]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The introduction of Gemini 1.5, which features a 1 million token context window, has sparked discussions in the AI community regarding its impact on Retrieval-Augmented Generation (RAG). Some predict a negative effect on RAG systems due to the high costs associated with fine-tuning long context windows and potential catastrophic values introduced by new token positions. However, research has shown that RAG significantly improves the performance of Large Language Models (LLMs) by providing abundant data for GenAI applications. LongRoPE and Position Interpolation (PI) are examples of techniques that extend context window sizes of pre-trained LLMs, allowing them to process more tokens with minimal fine-tuning. These advancements aim to address the limitations of fixed context windows by combining the power of language models with information retrieval techniques. The ability of LLMs to process and generate coherent text is crucial, and Dual Chunk Attention (DCA) has been proposed to support context windows of more than 100k tokens without continual training.\\n\\nCitations:\\n\\n[1]: https://medium.com/enterprise-rag/why-gemini-1-5-and-other-large-context-models-are-bullish-for-rag-ce3218930bb4\\n[2]: https://arxiv.org/abs/2402.13753\\n[3]: https://www.pinecone.io/blog/rag-study/\\n[4]: https://arxiv.org/abs/2306.15595\\n[5]: https://www.linkedin.com/pulse/facing-extinction-rag-losing-ground-enhanced-context-windows-danter-w9i9f\\n[6]: https://blog.google/technology/ai/long-context-window-ai-models/\\n[7]: https://medium.com/@casahome2000/breaking-limits-how-googles-gemini-1-5-unlocks-a-million-token-horizon-in-ai-d6ea614617f6\\n[8]: https://arxiv.org/abs/2402.17463'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "example_answer = await gen_answer(\n",
    "    {\"messages\": [HumanMessage(content=question[\"messages\"][0].content)]}\n",
    ")\n",
    "example_answer[\"messages\"][-1].content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998671bf-958d-44c0-8421-523a71bea01a",
   "metadata": {},
   "source": [
    "# Construct the Interview Graph\n",
    "\n",
    "Now that we've defined the editor and domain expert, we can compose them in a graph.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "4800f958-00e0-4913-a246-c34dc3f0a00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_num_turns = 5\n",
    "\n",
    "\n",
    "def route_messages(state: InterviewState, name: str = \"SubjectMatterExpert\"):\n",
    "\n",
    "    name = cleanup_name(name)\n",
    "\n",
    "    print(f'Routing messages for [{name}]')\n",
    "\n",
    "    messages = state[\"messages\"]\n",
    "    num_responses = len(\n",
    "        [m for m in messages if isinstance(m, AIMessage) and m.name == name]\n",
    "    )\n",
    "\n",
    "    if num_responses >= max_num_turns:\n",
    "        return END\n",
    "    \n",
    "    last_question = messages[-2]\n",
    "    if last_question.content.endswith(\"Thank you so much for your help!\"):\n",
    "        return END\n",
    "    \n",
    "    print(f'Continue asking question for [{name}] as this is not the last end of the conversation')\n",
    "    return \"ask_question\"\n",
    "\n",
    "\n",
    "builder = StateGraph(InterviewState)\n",
    "\n",
    "builder.add_node(\"ask_question\", generate_question)\n",
    "builder.add_node(\"answer_question\", gen_answer)\n",
    "builder.add_conditional_edges(\"answer_question\", route_messages)\n",
    "builder.add_edge(\"ask_question\", \"answer_question\")\n",
    "\n",
    "builder.set_entry_point(\"ask_question\")\n",
    "interview_graph = builder.compile().with_config(run_name=\"Conduct Interviews\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "63c405fc-5b1c-44f5-b860-a10fe0d6616d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "# comment out if you have not installed pygraphviz\n",
    "# Image(interview_graph.get_graph().draw_png())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "96d47607-6c0b-493a-aebc-48356d0e0302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating question for DrDataScientist\n",
      "Swapping roles for DrDataScientist\n",
      "Converted messages for DrDataScientist while swapping roles: 1 messages\n",
      "Generated question for DrDataScientist\n",
      "ask_question\n",
      "Processing step: ask_question\n",
      "--  [AIMessage(content=\"Yes, that's correct. I'm focusing on how million-plus token context window language models impact the Retrieval-Augmented Generation (RAG) approach. As a researcher in this field, I'm particularly interested in understanding the effectiveness of these models in enhancing RAG capa\n",
      "Generating answers for [SubjectMatterExpert]\n",
      "Swapping roles for SubjectMatterExpert\n",
      "Converted messages for SubjectMatterExpert while swapping roles: 2 messages\n",
      "Got 4 search engine queries for [SubjectMatterExpert]\n",
      "Searching DuckDuckGo for [Impact of million-plus token context window language models on RAG]\n",
      "Got search engine results: 5 for [Impact of million-plus token context window language models on RAG]\n",
      "Searching DuckDuckGo for [Effectiveness of million-plus token context window language models in enhancing RAG capabilities]\n",
      "Got search engine results: 5 for [Effectiveness of million-plus token context window language models in enhancing RAG capabilities]\n",
      "Searching DuckDuckGo for [Influence of million-plus token context window language models on information retrieval in RAG]\n",
      "Got search engine results: 5 for [Influence of million-plus token context window language models on information retrieval in RAG]\n",
      "Searching DuckDuckGo for [Influence of million-plus token context window language models on generation processes in RAG]\n",
      "Got search engine results: 5 for [Influence of million-plus token context window language models on generation processes in RAG]\n",
      "Got 4 search engine results for [SubjectMatterExpert]\n",
      "Finished generating answer for [SubjectMatterExpert]\n",
      "answer_question\n",
      "Processing step: answer_question\n",
      "--  [AIMessage(content='The introduction of language models like Gemini 1.5, with a 1 million token context window, has sparked discussions in the AI community regarding its impact on Retrieval-Augmented Generation (RAG). While some predict a negative impact on RAG due to the large context window, other\n",
      "Routing messages for [SubjectMatterExpert]\n",
      "Continue asking question for [SubjectMatterExpert] as this is not the last end of the conversation\n",
      "Generating question for DrDataScientist\n",
      "Swapping roles for DrDataScientist\n",
      "Converted messages for DrDataScientist while swapping roles: 3 messages\n",
      "Generated question for DrDataScientist\n",
      "ask_question\n",
      "Processing step: ask_question\n",
      "--  [AIMessage(content=\"That was a very informative introduction to the impact of million-plus token context window language models on Retrieval-Augmented Generation (RAG). Your insights on the potential benefits and challenges posed by these large context window models are valuable. I'm particularly in\n",
      "Generating answers for [SubjectMatterExpert]\n",
      "Swapping roles for SubjectMatterExpert\n",
      "Converted messages for SubjectMatterExpert while swapping roles: 4 messages\n",
      "Got 4 search engine queries for [SubjectMatterExpert]\n",
      "Searching DuckDuckGo for [How do million-plus token context window language models process a larger amount of data simultaneously?]\n",
      "Got search engine results: 5 for [How do million-plus token context window language models process a larger amount of data simultaneously?]\n",
      "Searching DuckDuckGo for [In what ways do million-plus token context window language models enhance information retrieval processes?]\n",
      "Got search engine results: 5 for [In what ways do million-plus token context window language models enhance information retrieval processes?]\n",
      "Searching DuckDuckGo for [How do million-plus token context window language models improve generation processes in Retrieval-Augmented Generation (RAG)?]\n",
      "Got search engine results: 5 for [How do million-plus token context window language models improve generation processes in Retrieval-Augmented Generation (RAG)?]\n",
      "Searching DuckDuckGo for [Examples of how large context window language models like Gemini 1.5 utilize extensive context windows to enhance RAG capabilities]\n",
      "Got search engine results: 5 for [Examples of how large context window language models like Gemini 1.5 utilize extensive context windows to enhance RAG capabilities]\n",
      "Got 4 search engine results for [SubjectMatterExpert]\n",
      "Finished generating answer for [SubjectMatterExpert]\n",
      "answer_question\n",
      "Processing step: answer_question\n",
      "--  [AIMessage(content='Million-plus token context window language models, such as Gemini 1.5, process a larger amount of data simultaneously by expanding their context window capacity to handle an extensive number of tokens, enabling them to ingest and process more comprehensive information in a single\n",
      "Routing messages for [SubjectMatterExpert]\n",
      "Continue asking question for [SubjectMatterExpert] as this is not the last end of the conversation\n",
      "Generating question for DrDataScientist\n",
      "Swapping roles for DrDataScientist\n",
      "Converted messages for DrDataScientist while swapping roles: 5 messages\n",
      "Generated question for DrDataScientist\n",
      "ask_question\n",
      "Processing step: ask_question\n",
      "--  [AIMessage(content='DrDataScientist', response_metadata={'token_usage': {'completion_tokens': 4, 'prompt_tokens': 1576, 'total_tokens': 1580}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_3bc1b5746c', 'finish_reason': 'stop', 'logprobs': None}, name='DrDataScientist')]\n",
      "Generating answers for [SubjectMatterExpert]\n",
      "Swapping roles for SubjectMatterExpert\n",
      "Converted messages for SubjectMatterExpert while swapping roles: 6 messages\n",
      "Got 4 search engine queries for [SubjectMatterExpert]\n",
      "Searching DuckDuckGo for [Impact of million-plus token context window language models on RAG]\n",
      "Got search engine results: 5 for [Impact of million-plus token context window language models on RAG]\n",
      "Searching DuckDuckGo for [Specific ways in which million-plus token context window language models impact RAG]\n",
      "Got search engine results: 5 for [Specific ways in which million-plus token context window language models impact RAG]\n",
      "Searching DuckDuckGo for [Technical aspects of how million-plus token context window language models process a larger amount of data simultaneously]\n",
      "Got search engine results: 5 for [Technical aspects of how million-plus token context window language models process a larger amount of data simultaneously]\n",
      "Searching DuckDuckGo for [Examples of how million-plus token context window language models handle and utilize extensive context windows to improve RAG capabilities]\n",
      "Got search engine results: 5 for [Examples of how million-plus token context window language models handle and utilize extensive context windows to improve RAG capabilities]\n",
      "Got 4 search engine results for [SubjectMatterExpert]\n",
      "Finished generating answer for [SubjectMatterExpert]\n",
      "answer_question\n",
      "Processing step: answer_question\n",
      "--  [AIMessage(content='Million-plus token context window language models, such as Gemini 1.5, process a larger amount of data simultaneously by expanding their context window capacity to handle an extensive number of tokens, enabling them to ingest and process more comprehensive information in a single\n",
      "Routing messages for [SubjectMatterExpert]\n",
      "Continue asking question for [SubjectMatterExpert] as this is not the last end of the conversation\n",
      "Generating question for DrDataScientist\n",
      "Swapping roles for DrDataScientist\n",
      "Converted messages for DrDataScientist while swapping roles: 7 messages\n",
      "Generated question for DrDataScientist\n",
      "ask_question\n",
      "Processing step: ask_question\n",
      "--  [AIMessage(content='Thank you for providing such detailed information on how million-plus token context window language models, like Gemini 1.5, enhance information retrieval and generation processes through their expanded context window capacity. The insights on how these models can extract informa\n",
      "Generating answers for [SubjectMatterExpert]\n",
      "Swapping roles for SubjectMatterExpert\n",
      "Converted messages for SubjectMatterExpert while swapping roles: 8 messages\n",
      "Got 2 search engine queries for [SubjectMatterExpert]\n",
      "Searching DuckDuckGo for [Impact of million-plus token context window language models on Retrieval-Augmented Generation (RAG)]\n",
      "Got search engine results: 5 for [Impact of million-plus token context window language models on Retrieval-Augmented Generation (RAG)]\n",
      "Searching DuckDuckGo for [Technical aspects of how million-plus token context window language models process a larger amount of data simultaneously and enhance information retrieval and generation processes]\n",
      "Got search engine results: 5 for [Technical aspects of how million-plus token context window language models process a larger amount of data simultaneously and enhance information retrieval and generation processes]\n",
      "Got 2 search engine results for [SubjectMatterExpert]\n",
      "Finished generating answer for [SubjectMatterExpert]\n",
      "answer_question\n",
      "Processing step: answer_question\n",
      "--  [AIMessage(content=\"The introduction of million-plus token context window language models, exemplified by Gemini 1.5, has raised discussions in the AI community regarding their impact on Retrieval-Augmented Generation (RAG). These models, with extended context windows, are designed to process a subs\n",
      "Routing messages for [SubjectMatterExpert]\n",
      "__end__\n",
      "Processing step: __end__\n",
      "--  [AIMessage(content='So you said you were writing an article on Impact of million-plus token context window language models on RAG?', name='SubjectMatterExpert'), AIMessage(content=\"Yes, that's correct. I'm focusing on how million-plus token context window language models impact the Retrieval-Augment\n"
     ]
    }
   ],
   "source": [
    "\n",
    "final_step = None\n",
    "\n",
    "initial_state = {\n",
    "    \"editor\": perspectives.editors[0],\n",
    "    \"messages\": [\n",
    "        AIMessage(\n",
    "            content=f\"So you said you were writing an article on {example_topic}?\",\n",
    "            name=\"SubjectMatterExpert\",\n",
    "        )\n",
    "    ],\n",
    "}\n",
    "async for step in interview_graph.astream(initial_state):\n",
    "    name = next(iter(step))\n",
    "    print(name)\n",
    "    print(f\"Processing step: {name}\")\n",
    "    print(\"-- \", str(step[name][\"messages\"])[:300])\n",
    "    if END in step:\n",
    "        final_step = step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "8c7a97ec-09a1-4873-b559-275526971a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_state = next(iter(final_step.values()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "f22e50cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [AIMessage(content='So you said you were writing an article on Impact of million-plus token context window language models on RAG?', name='SubjectMatterExpert'),\n",
       "  AIMessage(content=\"Yes, that's correct. I'm focusing on how million-plus token context window language models impact the Retrieval-Augmented Generation (RAG) approach. As a researcher in this field, I'm particularly interested in understanding the effectiveness of these models in enhancing RAG capabilities and how they influence information retrieval and generation processes. Do you have insights or data that could help me understand the specific ways in which these language models impact RAG?\", response_metadata={'token_usage': {'completion_tokens': 87, 'prompt_tokens': 243, 'total_tokens': 330}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_3bc1b5746c', 'finish_reason': 'stop', 'logprobs': None}, name='DrDataScientist'),\n",
       "  AIMessage(content='The introduction of language models like Gemini 1.5, with a 1 million token context window, has sparked discussions in the AI community regarding its impact on Retrieval-Augmented Generation (RAG). While some predict a negative impact on RAG due to the large context window, others see potential benefits in terms of enhancing information retrieval and generation processes. These models aim to process a significantly larger amount of data simultaneously, allowing for more in-depth context understanding and potentially improving the overall performance of RAG. Additionally, the tokenization process in large language models plays a crucial role in converting textual characters into numeric representations, enabling efficient language processing. Furthermore, recent advancements in extending the context window of transformer-based models, such as Rotary Position Embeddings (RoPE) and YaRN (Yet another RoPE extensioN method), offer more efficient ways to encode positional information and extend the context window while requiring fewer tokens and training resources. Overall, the implications of million-plus token context window language models on RAG are multifaceted, with potential benefits in enhancing information retrieval and generation capabilities.\\n\\nCitations:\\n\\n[1]: https://medium.com/enterprise-rag/why-gemini-1-5-and-other-large-context-models-are-bullish-for-rag-ce3218930bb4\\n[2]: https://www.linkedin.com/pulse/understanding-tokens-context-windows-large-language-models-lepain-80p0e\\n[3]: https://ai88.substack.com/p/rag-vs-context-window-in-gpt4-accuracy-cost\\n[4]: https://www.allabtai.com/rag-vs-context-window/\\n[5]: https://www.linkedin.com/pulse/implications-mega-context-models-gemini-claude-chris-mann-fp8ve\\n[6]: https://arxiv.org/abs/2309.00071\\n[7]: https://productmann.medium.com/the-implications-of-mega-context-models-by-gemini-and-claude-da42f770018b\\n[8]: https://medium.com/@casahome2000/breaking-limits-how-googles-gemini-1-5-unlocks-a-million-token-horizon-in-ai-d6ea614617f6\\n[9]: https://www.linkedin.com/pulse/facing-extinction-rag-losing-ground-enhanced-context-windows-danter-w9i9f\\n[10]: https://medium.com/@jm_51428/long-context-window-models-vs-rag-a73c35a763f2\\n[11]: https://pyimagesearch.com/2024/03/04/google-gemini-1-5-review-million-token-ai-changes-everything/', name='SubjectMatterExpert'),\n",
       "  AIMessage(content=\"That was a very informative introduction to the impact of million-plus token context window language models on Retrieval-Augmented Generation (RAG). Your insights on the potential benefits and challenges posed by these large context window models are valuable. I'm particularly interested in understanding the specific technical aspects of these models, such as how they process a larger amount of data simultaneously and how they enhance information retrieval and generation processes. Could you provide more details or examples on how these language models handle and utilize such extensive context windows to improve RAG capabilities?\", response_metadata={'token_usage': {'completion_tokens': 105, 'prompt_tokens': 908, 'total_tokens': 1013}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_3bc1b5746c', 'finish_reason': 'stop', 'logprobs': None}, name='DrDataScientist'),\n",
       "  AIMessage(content='Million-plus token context window language models, such as Gemini 1.5, process a larger amount of data simultaneously by expanding their context window capacity to handle an extensive number of tokens, enabling them to ingest and process more comprehensive information in a single prompt. The context window determines the length of content a model processes, with tokens dividing words into workable segments. Techniques like Retrieval-Augmented Generation (RAG) leverage the benefits of large context windows to enhance language model capabilities, allowing for improved information retrieval and generation processes. Models like Gemini 1.5 with a 1 million token context window can understand and extract information from large documents, code repositories, and even lengthy videos directly, significantly expanding their capacity for multimodal understanding and analysis. The advancements in context windows and techniques like RoPE (Rotary Position Embeddings) contribute to extending the context window while efficiently encoding positional information, thus improving the overall performance of language models in information retrieval and generation tasks.\\n\\nCitations:\\n\\n[1]: https://blog.google/technology/ai/long-context-window-ai-models/\\n[2]: https://www.linkedin.com/pulse/understanding-tokens-context-windows-large-language-models-lepain-80p0e\\n[3]: https://medium.com/aimonks/how-a-larger-context-window-has-unlocked-llms-potential-c817f16d87b0\\n[4]: https://medium.com/@crskilpatrick807/context-windows-the-short-term-memory-of-large-language-models-ab878fc6f9b5\\n[5]: https://towardsdatascience.com/why-and-how-to-achieve-longer-context-windows-for-llms-5f76f8656ea9\\n[6]: https://towardsdatascience.com/de-coded-understanding-context-windows-for-transformer-models-cd1baca6427e\\n[7]: https://www.pinecone.io/blog/why-use-retrieval-instead-of-larger-context/\\n[8]: https://bdtechtalks.com/2023/11/27/streamingllm/\\n[9]: https://developers.googleblog.com/2024/02/gemini-15-available-for-private-preview-in-google-ai-studio.html\\n[10]: https://blog.google/technology/ai/google-gemini-next-generation-model-february-2024/\\n[11]: https://pub.towardsai.net/how-good-is-google-gemini-1-5-with-a-massive-1-million-context-window-b386d285845d\\n[12]: https://www.makeuseof.com/why-gemini-context-window-is-a-game-changer/', name='SubjectMatterExpert'),\n",
       "  AIMessage(content='DrDataScientist', response_metadata={'token_usage': {'completion_tokens': 4, 'prompt_tokens': 1576, 'total_tokens': 1580}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_3bc1b5746c', 'finish_reason': 'stop', 'logprobs': None}, name='DrDataScientist'),\n",
       "  AIMessage(content='Million-plus token context window language models, such as Gemini 1.5, process a larger amount of data simultaneously by expanding their context window capacity to handle an extensive number of tokens, enabling them to ingest and process more comprehensive information in a single prompt. The context window determines the length of content a model processes, with tokens dividing words into workable segments. Techniques like Retrieval-Augmented Generation (RAG) leverage the benefits of large context windows to enhance language model capabilities, allowing for improved information retrieval and generation processes. Models like Gemini 1.5 with a 1 million token context window can understand and extract information from large documents, code repositories, and even lengthy videos directly, significantly expanding their capacity for multimodal understanding and analysis. The advancements in context windows and techniques like RoPE (Rotary Position Embeddings) contribute to extending the context window while efficiently encoding positional information, thus improving the overall performance of language models in information retrieval and generation tasks.\\n\\nCitations:\\n\\n[1]: https://blog.google/technology/ai/long-context-window-ai-models/\\n[2]: https://www.linkedin.com/pulse/understanding-tokens-context-windows-large-language-models-lepain-80p0e\\n[3]: https://towardsdatascience.com/why-and-how-to-achieve-longer-context-windows-for-llms-5f76f8656ea9\\n[4]: https://www.linkedin.com/pulse/facing-extinction-rag-losing-ground-enhanced-context-windows-danter-w9i9f\\n[5]: https://towardsdatascience.com/de-coded-understanding-context-windows-for-transformer-models-cd1baca6427e', name='SubjectMatterExpert'),\n",
       "  AIMessage(content='Thank you for providing such detailed information on how million-plus token context window language models, like Gemini 1.5, enhance information retrieval and generation processes through their expanded context window capacity. The insights on how these models can extract information from various sources and improve multimodal understanding are invaluable. I will incorporate these points into my article. Thank you so much for your help!', response_metadata={'token_usage': {'completion_tokens': 74, 'prompt_tokens': 1942, 'total_tokens': 2016}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_3bc1b5746c', 'finish_reason': 'stop', 'logprobs': None}, name='DrDataScientist'),\n",
       "  AIMessage(content=\"The introduction of million-plus token context window language models, exemplified by Gemini 1.5, has raised discussions in the AI community regarding their impact on Retrieval-Augmented Generation (RAG). These models, with extended context windows, are designed to process a substantial amount of data simultaneously, allowing for a more comprehensive understanding of context. Techniques like Retrieval-Augmented Generation (RAG) leverage the advantages of these large context windows to enhance information retrieval and generation processes. For example, RAG pipelines may include language models, reranker models, and vector databases to optimize retrieval and generation. Extending context windows in language models, such as with the LongRoPE method, can significantly enhance the model's capacity to handle longer inputs and improve performance. Overall, the utilization of million-plus token context window language models in RAG shows promise in advancing the capabilities of language models in processing complex information and generating coherent outputs.\\n\\nCitations:\\n\\n[1]: https://medium.com/enterprise-rag/why-gemini-1-5-and-other-large-context-models-are-bullish-for-rag-ce3218930bb4\\n[2]: https://www.linkedin.com/pulse/understanding-tokens-context-windows-large-language-models-lepain-80p0e\\n[3]: https://towardsdatascience.com/advanced-retrieval-augmented-generation-from-theory-to-llamaindex-implementation-4de1464a9930\\n[4]: https://ai88.substack.com/p/rag-vs-context-window-in-gpt4-accuracy-cost\\n[5]: https://ai.plainenglish.io/context-window-size-and-language-model-performance-balancing-act-2ae2964e3ec1\\n[6]: https://blog.google/technology/ai/long-context-window-ai-models/\\n[7]: https://arxiv.org/abs/2402.13753\\n[8]: https://productmann.medium.com/the-implications-of-mega-context-models-by-gemini-and-claude-da42f770018b\\n[9]: https://towardsdatascience.com/extending-context-length-in-large-language-models-74e59201b51f\\n[10]: https://medium.com/aimonks/how-a-larger-context-window-has-unlocked-llms-potential-c817f16d87b0\", name='SubjectMatterExpert')],\n",
       " 'references': {'https://medium.com/enterprise-rag/why-gemini-1-5-and-other-large-context-models-are-bullish-for-rag-ce3218930bb4': 'The introduction of Gemini 1.5, boasting a 1 million token context window, has sparked discussions in the AI community, with some predicting a negative impact on Retrieval-Augmented Generation (RAG).',\n",
       "  'https://www.linkedin.com/pulse/understanding-tokens-context-windows-large-language-models-lepain-80p0e': 'Addressing the limitations posed by context window sizes, the concept of Retrieval Augmented Generation (RAG) presents a promising avenue for enhancing LLM capabilities.',\n",
       "  'https://ai88.substack.com/p/rag-vs-context-window-in-gpt4-accuracy-cost': 'The next stage of LLMs in production is all about making responses hyper-specific: to a dataset, to a user, to a use-case, even to a specific invocation.. This is typically achieved using one of 3 basic techniques:. Context-window-stuffing. RAG (Retrieval Augmented Generation). Fine-tuning. (If none of these mean anything to you - consider subscribing to the newsletter - I will cover each ...',\n",
       "  'https://www.allabtai.com/rag-vs-context-window/': 'A context window defines the amount of data a language model can consider at any one time. For example, a model might have an 8,000-token limit, blending input and output tokens within this boundary. RAG, on the other hand, circumvents this limitation by transforming input data into vector embeddings, stored in a database, and retrieves the ...',\n",
       "  'https://www.linkedin.com/pulse/implications-mega-context-models-gemini-claude-chris-mann-fp8ve': 'For example, a model might need to understand the context of a long conversation (benefiting from a large context window) and also answer questions based on the latest news or scientific research ...',\n",
       "  'https://arxiv.org/abs/2309.00071': 'Rotary Position Embeddings (RoPE) have been shown to effectively encode positional information in transformer-based language models. However, these models fail to generalize past the sequence length they were trained on. We present YaRN (Yet another RoPE extensioN method), a compute-efficient method to extend the context window of such models, requiring 10x less tokens and 2.5x less training ...',\n",
       "  'https://productmann.medium.com/the-implications-of-mega-context-models-by-gemini-and-claude-da42f770018b': '1 day ago. March 5, 2024. It\\'s been an exciting few weeks with the introduction of new models from Google\\'s Gemini on Feb 15th and, now on March 4th, Anthropic\\'s Claude — both now claiming the ability to effectively process a 1M token context window. After having road rash from experiencing GPT4\\'s all too often \"confusion ...',\n",
       "  'https://medium.com/@casahome2000/breaking-limits-how-googles-gemini-1-5-unlocks-a-million-token-horizon-in-ai-d6ea614617f6': \"Where previous models were constrained to processing information within a context window of around 4,096 tokens, Gemini 1.5's million-token capacity enables a depth and continuity of ...\",\n",
       "  'https://www.linkedin.com/pulse/facing-extinction-rag-losing-ground-enhanced-context-windows-danter-w9i9f': 'Context Window: Traditionally, language models like GPT (Generative Pre-trained Transformer) operate within a fixed context window, processing a maximum number of tokens (e.g., 8,000 tokens).This ...',\n",
       "  'https://medium.com/@jm_51428/long-context-window-models-vs-rag-a73c35a763f2': 'Long context window models vs. RAG. Jm. ·. Follow. 7 min read. ·. Feb 5, 2024. What is the best way to give a large language model the context it needs to answer a specific query or accomplish a ...',\n",
       "  'https://pyimagesearch.com/2024/03/04/google-gemini-1-5-review-million-token-ai-changes-everything/': 'Google DeepMind just pulled back the curtain on a giant leap forward in AI — Gemini 1.5 Pro, boasting the longest context window in any large-scale foundation language model. It can process up to a million tokens simultaneously, holding richer, more complex conversations and processing more significant amounts of information than ever before.',\n",
       "  'https://blog.google/technology/ai/long-context-window-ai-models/': \"In addition to big improvements to speed and efficiency, one of Gemini 1.5's innovations is its long context window, which measures how many tokens — the smallest building blocks, like part of a word, image or video — that the model can process at once. To help understand the significance of this milestone, we asked the Google DeepMind ...\",\n",
       "  'https://medium.com/aimonks/how-a-larger-context-window-has-unlocked-llms-potential-c817f16d87b0': 'A context window determines the length of content from a prompt an AI model will process to answer questions. Under the hood, it is the tokens that decide how LLMs segregate words into workable bits.',\n",
       "  'https://medium.com/@crskilpatrick807/context-windows-the-short-term-memory-of-large-language-models-ab878fc6f9b5': 'The number of tokens a model can process at once is defined by the size of its context window. ... Exploring retrieval augmented generation (RAG) and its use cases in the world of large language ...',\n",
       "  'https://towardsdatascience.com/why-and-how-to-achieve-longer-context-windows-for-llms-5f76f8656ea9': \"One solution could be training the model on a large amount of data on a relatively small window (e.g. 4K tokens) and then fine-tuning it on a bigger one (e.g. 64K tokens). This operation isn't straightforward, because even though the context length doesn't impact the number of model's weights, it does affect how positional information of ...\",\n",
       "  'https://towardsdatascience.com/de-coded-understanding-context-windows-for-transformer-models-cd1baca6427e': 'The context window is the maximum sequence length that a transformer can process at a time. With the rise of proprietary LLMs that limit the number of tokens and therefore the prompt size — as well as the growing interest in techniques such as Retrieval Augmented Generation (RAG)— understanding the key ideas around context windows and their implications is becoming increasingly important ...',\n",
       "  'https://www.pinecone.io/blog/why-use-retrieval-instead-of-larger-context/': 'As illustrated below, retrieval has the capability to pull relevant data from long documents and provide more focused contexts. In other words, retrieval improves the accuracy achieved per token. For instance, RAG preserved 95% of the original accuracy from when the model processed the entire documents, despite only using 25% of the tokens.',\n",
       "  'https://bdtechtalks.com/2023/11/27/streamingllm/': \"StreamingLLM is an innovative framework that allows large language models to handle text of infinite length without the need for finetuning. This technique preserves attention sinks to maintain a near-normal attention score distribution. When the sequence of the conversation with the LLM surpasses the model's context length, StreamingLLM ...\",\n",
       "  'https://developers.googleblog.com/2024/02/gemini-15-available-for-private-preview-in-google-ai-studio.html': \"Gemini 1.5 Pro will come with a 128,000 token context window by default, but today's Private Preview will have access to the experimental 1 million token context window. We're excited about the new possibilities that larger context windows enable. You can directly upload large PDFs, code repositories, or even lengthy videos as prompts in ...\",\n",
       "  'https://blog.google/technology/ai/google-gemini-next-generation-model-february-2024/': \"The bigger a model's context window, the more information it can take in and process in a given prompt — making its output more consistent, relevant and useful. Through a series of machine learning innovations, we've increased 1.5 Pro's context window capacity far beyond the original 32,000 tokens for Gemini 1.0.\",\n",
       "  'https://pub.towardsai.net/how-good-is-google-gemini-1-5-with-a-massive-1-million-context-window-b386d285845d': \"Gemini 1.5 can understand and extract useful information from large documents; Source: Google Dev Blog Unprecedented Context Window: With a default 128K token window expandable to 1M tokens, Gemini 1.5 can handle over 700K words of text, allowing for the analysis of entire codebases and large PDFs directly. Multimodal Understanding: Whether it's text, code, image or video, Gemini 1.5 can ...\",\n",
       "  'https://www.makeuseof.com/why-gemini-context-window-is-a-game-changer/': \"Key Takeaways. Google Gemini 1.5 introduces a one million token context window, surpassing competitors like Claude and ChatGPT. A larger context window enhances an AI model's performance and reduces errors, but may not guarantee overall success. Gemini 1.5's larger context window could greatly enhance accuracy, reduce errors, and improve ...\",\n",
       "  'https://towardsdatascience.com/advanced-retrieval-augmented-generation-from-theory-to-llamaindex-implementation-4de1464a9930': 'The RAG pipeline components were language models from OpenAI, a reranker model from BAAI hosted on Hugging Face, and a Weaviate vector database. We implemented the following selection of techniques using LlamaIndex in Python: Pre-retrieval optimization: Sentence window retrieval; Retrieval optimization: Hybrid search',\n",
       "  'https://ai.plainenglish.io/context-window-size-and-language-model-performance-balancing-act-2ae2964e3ec1': 'The context window size of large language models (LLMs) defines the scope of tokens the model can take into account when formulating responses to prompts. ... Making the context window bigger has a big impact on how well the model works. With this improvement, LLMs can handle more complex and longer inputs. ... Retrieval-augmented generation ...',\n",
       "  'https://arxiv.org/abs/2402.13753': 'Large context window is a desirable feature in large language models (LLMs). However, due to high fine-tuning costs, scarcity of long texts, and catastrophic values introduced by new token positions, current extended context windows are limited to around 128k tokens. This paper introduces LongRoPE that, for the first time, extends the context window of pre-trained LLMs to an impressive 2048k ...',\n",
       "  'https://towardsdatascience.com/extending-context-length-in-large-language-models-74e59201b51f': 'A longer context window allows the model to understand long-range dependencies in text better. Models with longer contexts can build connections between ideas far apart in the text, generating more globally coherent outputs. During training, the model processes the text data in chunks or fixed-length windows.'},\n",
       " 'editor': Editor(affiliation='Research Institution', name='Dr. Data Scientist', role='Researcher', description='Dr. Data Scientist specializes in analyzing the impact of million-plus token context window language models on the Retrieval-Augmented Generation (RAG) approach. They focus on evaluating the effectiveness of these models in enhancing RAG capabilities and understanding how they influence information retrieval and generation processes.')}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e14dcb",
   "metadata": {},
   "source": [
    "## Refine Outline\n",
    "\n",
    "At this point in STORM, we've conducted a large amount of research from different perspectives. It's time to refine the original outline based on these investigations. Below, create a chain using the LLM with a long context window to update the original outline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "c284eb72-3856-406d-8582-5a1c92fd292e",
   "metadata": {},
   "outputs": [],
   "source": [
    "refine_outline_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"You are a Wikipedia writer. You have gathered information from experts and search engines. Now, you are refining the outline of the Wikipedia page. \\\n",
    "You need to make sure that the outline is comprehensive and specific. \\\n",
    "Topic you are writing about: {topic} \n",
    "\n",
    "Old outline:\n",
    "\n",
    "{old_outline}\n",
    "\"\"\",\n",
    "        ),\n",
    "        (\n",
    "            \"user\",\n",
    "            \"Refine the outline based on your conversations with subject-matter experts:\\n\\nConversations:\\n\\n{conversations}\\n\\n{format_instructions}\\n\\nWrite the refined Wikipedia outline:\",\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# Using turbo preview since the context can get quite long\n",
    "refine_outline_chain = refine_outline_prompt.partial(format_instructions=outline_parser.get_format_instructions()) | long_context_llm | outline_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "d19e58c5-086f-49ba-b921-791669d04b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "refined_outline = refine_outline_chain.invoke(\n",
    "    {\n",
    "        \"topic\": example_topic,\n",
    "        \"old_outline\": initial_outline.as_str,\n",
    "        \"conversations\": \"\\n\\n\".join(\n",
    "            f\"### {m.name}\\n\\n{m.content}\" for m in final_state[\"messages\"]\n",
    "        ),\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c397dc5-e614-4a7f-9f78-67dffc9b8c59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Impact of million-plus token context window language models on RAG\n",
      "\n",
      "## Introduction\n",
      "\n",
      "Overview of how large context windows in language models, specifically exemplified by Gemini 1.5 with a 1 million token context window, are influencing the Retrieval-Augmented Generation (RAG) approach in natural language processing (NLP). Discussion on the potential benefits and concerns raised within the AI community.\n",
      "\n",
      "## Enhancements to RAG\n",
      "\n",
      "Explanation on how integrating million-plus token context window language models can enhance the capabilities of the Retrieval-Augmented Generation (RAG) framework. Insights into the potential improvements in response generation, understanding of context, and overall performance.\n",
      "\n",
      "## Practical Application: GraphRAG by Microsoft Research\n",
      "\n",
      "Case study on GraphRAG, developed by Microsoft Research, showcasing the practical application and impact of combining Large Language Models (LLMs) with tailored retrieval mechanisms to construct knowledge graphs from private datasets. Demonstrates the intelligence augmentation achieved through the integration of large context windows with RAG.\n",
      "\n",
      "## Challenges and Considerations\n",
      "\n",
      "Examination of the challenges and limitations researchers and developers may face when implementing million-plus token context window language models alongside the Retrieval-Augmented Generation (RAG) approach. Discussion on computational expenses, processing speeds, security risks, performance trade-offs, and the importance of understanding project-specific requirements.\n"
     ]
    }
   ],
   "source": [
    "print(refined_outline.as_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8b76b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generate Article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "563b2a40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of references: 26\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "from langchain_community.vectorstores import SKLearnVectorStore\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "reference_docs = [\n",
    "    Document(page_content=v, metadata={\"source\": k})\n",
    "    for k, v in final_state[\"references\"].items()\n",
    "]\n",
    "\n",
    "print(f\"Number of references: {len(reference_docs)}\")\n",
    "\n",
    "# This really doesn't need to be a vectorstore for this size of data.\n",
    "# It could just be a numpy matrix. Or you could store documents\n",
    "# across requests if you want.\n",
    "vectorstore = SKLearnVectorStore.from_documents(\n",
    "    reference_docs,\n",
    "    embedding=embeddings,\n",
    ")\n",
    "retriever = vectorstore.as_retriever(k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "b05496dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"StreamingLLM is an innovative framework that allows large language models to handle text of infinite length without the need for finetuning. This technique preserves attention sinks to maintain a near-normal attention score distribution. When the sequence of the conversation with the LLM surpasses the model's context length, StreamingLLM ...\", metadata={'id': 'ef137c5c-7b03-4e85-a336-5030048b329a', 'source': 'https://bdtechtalks.com/2023/11/27/streamingllm/'}),\n",
       " Document(page_content='The next stage of LLMs in production is all about making responses hyper-specific: to a dataset, to a user, to a use-case, even to a specific invocation.. This is typically achieved using one of 3 basic techniques:. Context-window-stuffing. RAG (Retrieval Augmented Generation). Fine-tuning. (If none of these mean anything to you - consider subscribing to the newsletter - I will cover each ...', metadata={'id': '67a6a276-7fd6-487b-ad7d-9b9461d959d6', 'source': 'https://ai88.substack.com/p/rag-vs-context-window-in-gpt4-accuracy-cost'}),\n",
       " Document(page_content='The context window size of large language models (LLMs) defines the scope of tokens the model can take into account when formulating responses to prompts. ... Making the context window bigger has a big impact on how well the model works. With this improvement, LLMs can handle more complex and longer inputs. ... Retrieval-augmented generation ...', metadata={'id': '36c842d5-987b-40fd-b16b-12b9372ca178', 'source': 'https://ai.plainenglish.io/context-window-size-and-language-model-performance-balancing-act-2ae2964e3ec1'}),\n",
       " Document(page_content='Large context window is a desirable feature in large language models (LLMs). However, due to high fine-tuning costs, scarcity of long texts, and catastrophic values introduced by new token positions, current extended context windows are limited to around 128k tokens. This paper introduces LongRoPE that, for the first time, extends the context window of pre-trained LLMs to an impressive 2048k ...', metadata={'id': '271bf5ab-3b89-4036-8935-e09eda28050a', 'source': 'https://arxiv.org/abs/2402.13753'})]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.invoke(\"What's a long context LLM anyway?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e708d31a",
   "metadata": {},
   "source": [
    "#### Generate Sections\n",
    "\n",
    "Now you can generate the sections using the indexed docs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "524f4c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SubSection(BaseModel):\n",
    "    subsection_title: str = Field(..., title=\"Title of the subsection\")\n",
    "    content: str = Field(\n",
    "        ...,\n",
    "        title=\"Full content of the subsection. Include [#] citations to the cited sources where relevant.\",\n",
    "    )\n",
    "\n",
    "    @property\n",
    "    def as_str(self) -> str:\n",
    "        return f\"### {self.subsection_title}\\n\\n{self.content}\".strip()\n",
    "\n",
    "\n",
    "class WikiSection(BaseModel):\n",
    "    section_title: str = Field(..., title=\"Title of the section\")\n",
    "    content: str = Field(..., title=\"Full content of the section\")\n",
    "    subsections: Optional[List[Subsection]] = Field(\n",
    "        default=None,\n",
    "        title=\"Titles and descriptions for each subsection of the Wikipedia page.\",\n",
    "    )\n",
    "    citations: List[str] = Field(default_factory=list)\n",
    "\n",
    "    @property\n",
    "    def as_str(self) -> str:\n",
    "        subsections = \"\\n\\n\".join(\n",
    "            subsection.as_str for subsection in self.subsections or []\n",
    "        )\n",
    "        citations = \"\\n\".join([f\" [{i}] {cit}\" for i, cit in enumerate(self.citations)])\n",
    "        return (\n",
    "            f\"## {self.section_title}\\n\\n{self.content}\\n\\n{subsections}\".strip()\n",
    "            + f\"\\n\\n{citations}\".strip()\n",
    "        )\n",
    "\n",
    "\n",
    "section_writer_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are an expert Wikipedia writer. Complete your assigned WikiSection from the following outline:\\n\\n\"\n",
    "            \"{outline}\\n\\nCite your sources, using the following references:\\n\\n<Documents>\\n{docs}\\n<Documents>\",\n",
    "        ),\n",
    "        (\"user\", \"Write the full WikiSection for the {section} section.\\n{format_instructions}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "async def retrieve(inputs: dict):\n",
    "    docs = await retriever.ainvoke(inputs[\"topic\"] + \": \" + inputs[\"section\"])\n",
    "    formatted = \"\\n\".join(\n",
    "        [\n",
    "            f'<Document href=\"{doc.metadata[\"source\"]}\"/>\\n{doc.page_content}\\n</Document>'\n",
    "            for doc in docs\n",
    "        ]\n",
    "    )\n",
    "    return {\"docs\": formatted, **inputs}\n",
    "\n",
    "wiki_parser = PydanticOutputParser(pydantic_object=WikiSection)\n",
    "\n",
    "section_writer = (\n",
    "    retrieve\n",
    "    | section_writer_prompt.partial(format_instructions=wiki_parser.get_format_instructions())\n",
    "    | long_context_llm\n",
    "    | wiki_parser\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "03723e96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Background\n",
      "\n",
      "Million-plus token context window language models refer to large language models with the capability to process a vast amount of data within a single context window. These models have revolutionized natural language processing tasks by enabling deeper understanding of text and context. In the context of the Retrieval-Augmented Generation (RAG) framework, the use of million-plus token context window language models has significant implications. By allowing the models to consider a larger number of tokens simultaneously, RAG benefits from enhanced information retrieval and generation capabilities.[0] https://www.allabtai.com/rag-vs-context-window/\n",
      " [1] https://medium.com/@crskilpatrick807/context-windows-the-short-term-memory-of-large-language-models-ab878fc6f9b5\n",
      " [2] https://medium.com/@jm_51428/long-context-window-models-vs-rag-a73c35a763f2\n",
      " [3] https://ai.plainenglish.io/context-window-size-and-language-model-performance-balancing-act-2ae2964e3ec1\n"
     ]
    }
   ],
   "source": [
    "section = await section_writer.ainvoke(\n",
    "    {\n",
    "        \"outline\": refined_outline.as_str,\n",
    "        \"section\": refined_outline.sections[1].section_title,\n",
    "        \"topic\": example_topic,\n",
    "    }\n",
    ")\n",
    "print(section.as_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0afd728d",
   "metadata": {},
   "source": [
    "#### Generate final article\n",
    "\n",
    "Now we can rewrite the draft to appropriately group all the citations and maintain a consistent voice.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "05089f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "writer_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are an expert Wikipedia author. Write the complete wiki article on {topic} using the following section drafts:\\n\\n\"\n",
    "            \"{draft}\\n\\nStrictly follow Wikipedia format guidelines.\",\n",
    "        ),\n",
    "        (\n",
    "            \"user\",\n",
    "            'Write the complete Wiki article using markdown format. Organize citations using footnotes like \"[1]\",\"\" avoiding duplicates in the footer. Include URLs in the footer.',\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "writer = writer_prompt | long_context_llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "0e6e6e5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Impact of Million-Plus Token Context Window Language Models on RAG\n",
      "\n",
      "## Background\n",
      "\n",
      "Million-plus token context window language models refer to large language models with the capability to process a vast amount of data within a single context window. These models have revolutionized natural language processing tasks by enabling deeper understanding of text and context. In the context of the Retrieval-Augmented Generation (RAG) framework, the use of million-plus token context window language models has significant implications. By allowing the models to consider a larger number of tokens simultaneously, RAG benefits from enhanced information retrieval and generation capabilities.[1][2][3]\n",
      "\n",
      "## Implications on RAG\n",
      "\n",
      "The integration of million-plus token context window language models within the RAG framework has led to notable improvements in information retrieval and text generation processes. These models can capture a broader context in a single window, enabling RAG to access and process more relevant information during retrieval tasks. This expanded context window enhances the accuracy and relevance of retrieved information, ultimately improving the overall performance of the RAG framework.\n",
      "\n",
      "Moreover, the use of million-plus token context window language models in RAG facilitates more nuanced and contextually appropriate text generation. By considering a larger context, these models can generate responses that are more coherent, relevant, and contextually accurate. This capability is particularly beneficial in tasks that require a deep understanding of the input context, such as question-answering and dialogue systems.\n",
      "\n",
      "Furthermore, the enhanced capabilities of million-plus token context window language models in RAG contribute to more efficient and effective natural language processing tasks. The ability to process a larger amount of text within a single window reduces the need for multiple iterations or context switches, streamlining the overall computation process and improving computational efficiency.\n",
      "\n",
      "## Future Directions\n",
      "\n",
      "The integration of million-plus token context window language models in the RAG framework represents a significant advancement in natural language processing. Moving forward, further research and development in this area could focus on optimizing the use of these models within RAG, exploring new applications and use cases, and enhancing the scalability and performance of such systems. Continued innovation in this field is likely to lead to even more sophisticated and capable language models, with broader implications for various NLP tasks and applications.\n",
      "\n",
      "In conclusion, the impact of million-plus token context window language models on RAG is substantial, offering improved information retrieval, text generation, and overall efficiency in natural language processing tasks. By leveraging the capabilities of these advanced language models, the RAG framework stands to benefit from enhanced performance and expanded functionality in various NLP applications.\n",
      "\n",
      "## References\n",
      "\n",
      "[1] https://www.allabtai.com/rag-vs-context-window/  \n",
      "[2] https://medium.com/@crskilpatrick807/context-windows-the-short-term-memory-of-large-language-models-ab878fc6f9b5  \n",
      "[3] https://medium.com/@jm_51428/long-context-window-models-vs-rag-a73c35a763f2"
     ]
    }
   ],
   "source": [
    "for tok in writer.stream({\"topic\": example_topic, \"draft\": section.as_str}):\n",
    "    print(tok, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6ab734",
   "metadata": {},
   "source": [
    "## Final Flow\n",
    "\n",
    "Now it's time to string everything together. We will have 6 main stages in sequence:\n",
    ".\n",
    "\n",
    "1. Generate the initial outline + perspectives\n",
    "2. Batch converse with each perspective to expand the content for the article\n",
    "3. Refine the outline based on the conversations\n",
    "4. Index the reference docs from the conversations\n",
    "5. Write the individual sections of the article\n",
    "6. Write the final wiki\n",
    "\n",
    "The state tracks the outputs of each stage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "e775ba8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResearchState(TypedDict):\n",
    "    topic: str\n",
    "    outline: Outline\n",
    "    editors: List[Editor]\n",
    "    interview_results: List[InterviewState]\n",
    "    # The final sections output\n",
    "    sections: List[WikiSection]\n",
    "    article: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "ed1854d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "\n",
    "async def initialize_research(state: ResearchState):\n",
    "    topic = state[\"topic\"]\n",
    "    coros = (\n",
    "        generate_outline_direct.ainvoke({\"topic\": topic}),\n",
    "        survey_subjects.ainvoke(topic),\n",
    "    )\n",
    "    results = await asyncio.gather(*coros)\n",
    "    return {\n",
    "        **state,\n",
    "        \"outline\": results[0],\n",
    "        \"editors\": results[1].editors,\n",
    "    }\n",
    "\n",
    "\n",
    "async def conduct_interviews(state: ResearchState):\n",
    "    topic = state[\"topic\"]\n",
    "    initial_states = [\n",
    "        {\n",
    "            \"editor\": editor,\n",
    "            \"messages\": [\n",
    "                AIMessage(\n",
    "                    content=f\"So you said you were writing an article on {topic}?\",\n",
    "                    name=\"SubjectMatterExpert\",\n",
    "                )\n",
    "            ],\n",
    "        }\n",
    "        for editor in state[\"editors\"]\n",
    "    ]\n",
    "    # We call in to the sub-graph here to parallelize the interviews\n",
    "    interview_results = await interview_graph.abatch(initial_states)\n",
    "\n",
    "    return {\n",
    "        **state,\n",
    "        \"interview_results\": interview_results,\n",
    "    }\n",
    "\n",
    "\n",
    "def format_conversation(interview_state):\n",
    "    messages = interview_state[\"messages\"]\n",
    "    convo = \"\\n\".join(f\"{m.name}: {m.content}\" for m in messages)\n",
    "    return f'Conversation with {interview_state[\"editor\"].name}\\n\\n' + convo\n",
    "\n",
    "\n",
    "async def refine_outline(state: ResearchState):\n",
    "    convos = \"\\n\\n\".join(\n",
    "        [\n",
    "            format_conversation(interview_state)\n",
    "            for interview_state in state[\"interview_results\"]\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    updated_outline = await refine_outline_chain.ainvoke(\n",
    "        {\n",
    "            \"topic\": state[\"topic\"],\n",
    "            \"old_outline\": state[\"outline\"].as_str,\n",
    "            \"conversations\": convos,\n",
    "        }\n",
    "    )\n",
    "    return {**state, \"outline\": updated_outline}\n",
    "\n",
    "\n",
    "async def index_references(state: ResearchState):\n",
    "    all_docs = []\n",
    "    for interview_state in state[\"interview_results\"]:\n",
    "        reference_docs = [\n",
    "            Document(page_content=v, metadata={\"source\": k})\n",
    "            for k, v in interview_state[\"references\"].items()\n",
    "        ]\n",
    "        all_docs.extend(reference_docs)\n",
    "    await vectorstore.aadd_documents(all_docs)\n",
    "    return state\n",
    "\n",
    "\n",
    "async def write_sections(state: ResearchState):\n",
    "    outline = state[\"outline\"]\n",
    "    sections = await section_writer.abatch(\n",
    "        [\n",
    "            {\n",
    "                \"outline\": refined_outline.as_str,\n",
    "                \"section\": section.section_title,\n",
    "                \"topic\": state[\"topic\"],\n",
    "            }\n",
    "            for section in outline.sections\n",
    "        ]\n",
    "    )\n",
    "    return {\n",
    "        **state,\n",
    "        \"sections\": sections,\n",
    "    }\n",
    "\n",
    "\n",
    "async def write_article(state: ResearchState):\n",
    "    topic = state[\"topic\"]\n",
    "    sections = state[\"sections\"]\n",
    "    draft = \"\\n\\n\".join([section.as_str for section in sections])\n",
    "    article = await writer.ainvoke({\"topic\": topic, \"draft\": draft})\n",
    "    return {\n",
    "        **state,\n",
    "        \"article\": article,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87881e3",
   "metadata": {},
   "source": [
    "#### Create the graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "af3b4be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "builder_of_storm = StateGraph(ResearchState)\n",
    "\n",
    "nodes = [\n",
    "    (\"init_research\", initialize_research),\n",
    "    (\"conduct_interviews\", conduct_interviews),\n",
    "    (\"refine_outline\", refine_outline),\n",
    "    (\"index_references\", index_references),\n",
    "    (\"write_sections\", write_sections),\n",
    "    (\"write_article\", write_article),\n",
    "]\n",
    "for i in range(len(nodes)):\n",
    "    name, node = nodes[i]\n",
    "    builder_of_storm.add_node(name, node)\n",
    "    if i > 0:\n",
    "        builder_of_storm.add_edge(nodes[i - 1][0], name)\n",
    "\n",
    "builder_of_storm.set_entry_point(nodes[0][0])\n",
    "builder_of_storm.set_finish_point(nodes[-1][0])\n",
    "storm = builder_of_storm.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "2a815f1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Survey Subjects for Topic: Building better slack bots using LLMs\n",
      "Retrieved 9 docs for Topic: Building better slack bots using LLMs\n",
      "init_research\n",
      "--  {'topic': 'Building better slack bots using LLMs', 'outline': Outline(page_title='Building Better Slack Bots Using Large Language Models (LLMs)', sections=[Section(section_title='Introduction', description='Overview of using Large Language Models (LLMs) to enhance Slack bots.', subsections=None), Se\n",
      "Generating question for AlexisChen\n",
      "Generating question for EthanPatel\n",
      "Generating question for SashaRodriguez\n",
      "Generating question for OliverKim\n",
      "Generating question for LunaChang\n",
      "Swapping roles for AlexisChen\n",
      "Converted messages for AlexisChen while swapping roles: 1 messages\n",
      "Swapping roles for EthanPatel\n",
      "Converted messages for EthanPatel while swapping roles: 1 messages\n",
      "Swapping roles for SashaRodriguez\n",
      "Converted messages for SashaRodriguez while swapping roles: 1 messages\n",
      "Swapping roles for OliverKim\n",
      "Converted messages for OliverKim while swapping roles: 1 messages\n",
      "Swapping roles for LunaChang\n",
      "Converted messages for LunaChang while swapping roles: 1 messages\n",
      "Generated question for OliverKim\n",
      "Generating answers for [SubjectMatterExpert]\n",
      "Swapping roles for SubjectMatterExpert\n",
      "Converted messages for SubjectMatterExpert while swapping roles: 2 messages\n",
      "Generated question for SashaRodriguez\n",
      "Generating answers for [SubjectMatterExpert]\n",
      "Swapping roles for SubjectMatterExpert\n",
      "Converted messages for SubjectMatterExpert while swapping roles: 2 messages\n",
      "Generated question for LunaChang\n",
      "Generating answers for [SubjectMatterExpert]\n",
      "Swapping roles for SubjectMatterExpert\n",
      "Converted messages for SubjectMatterExpert while swapping roles: 2 messages\n",
      "Generated question for EthanPatel\n",
      "Generating answers for [SubjectMatterExpert]\n",
      "Swapping roles for SubjectMatterExpert\n",
      "Converted messages for SubjectMatterExpert while swapping roles: 2 messages\n",
      "Got 3 search engine queries for [SubjectMatterExpert]\n",
      "Searching DuckDuckGo for [How can large language models (LLMs) introduce biases?]\n",
      "Got search engine results: 5 for [How can large language models (LLMs) introduce biases?]\n",
      "Searching DuckDuckGo for [What are some strategies to mitigate biases when using LLMs in Slack bots?]\n",
      "Got search engine results: 5 for [What are some strategies to mitigate biases when using LLMs in Slack bots?]\n",
      "Searching DuckDuckGo for [Examples of responsible deployment practices for LLM-powered Slack bots]\n",
      "Got search engine results: 5 for [Examples of responsible deployment practices for LLM-powered Slack bots]\n",
      "Got 3 search engine results for [SubjectMatterExpert]\n",
      "Generated question for AlexisChen\n",
      "Got 4 search engine queries for [SubjectMatterExpert]\n",
      "Got 4 search engine queries for [SubjectMatterExpert]\n",
      "Got 4 search engine queries for [SubjectMatterExpert]\n",
      "Searching DuckDuckGo for [Benefits of using Large Language Models (LLMs) in enhancing Slack bots' performance in natural language generation tasks]\n",
      "Got search engine results: 5 for [Benefits of using Large Language Models (LLMs) in enhancing Slack bots' performance in natural language generation tasks]\n",
      "Searching DuckDuckGo for [Specific ways LLMs improve the output quality and coherence of text generated by Slack bots]\n",
      "Got search engine results: 5 for [Specific ways LLMs improve the output quality and coherence of text generated by Slack bots]\n",
      "Searching DuckDuckGo for [Examples of successful applications of LLMs in improving the performance of Slack bots]\n",
      "Got search engine results: 5 for [Examples of successful applications of LLMs in improving the performance of Slack bots]\n",
      "Searching DuckDuckGo for [Comparison of text generation capabilities between Slack bots with and without LLM integration]\n",
      "Got search engine results: 5 for [Comparison of text generation capabilities between Slack bots with and without LLM integration]\n",
      "Searching DuckDuckGo for [How can Large Language Models (LLMs) enhance the conversational abilities of Slack bots?]\n",
      "Got search engine results: 5 for [How can Large Language Models (LLMs) enhance the conversational abilities of Slack bots?]\n",
      "Searching DuckDuckGo for [What are the benefits of incorporating LLMs into Slack bots?]\n",
      "Got search engine results: 5 for [What are the benefits of incorporating LLMs into Slack bots?]\n",
      "Searching DuckDuckGo for [How do LLMs improve the user experience of interacting with Slack bots?]\n",
      "Got search engine results: 5 for [How do LLMs improve the user experience of interacting with Slack bots?]\n",
      "Searching DuckDuckGo for [Examples of using LLMs to enhance the capabilities of Slack bots]\n",
      "Got search engine results: 5 for [Examples of using LLMs to enhance the capabilities of Slack bots]\n",
      "Searching DuckDuckGo for [How can Large Language Models (LLMs) enhance the functionality of Slack bots?]\n",
      "Got search engine results: 5 for [How can Large Language Models (LLMs) enhance the functionality of Slack bots?]\n",
      "Searching DuckDuckGo for [What are some specific applications of LLMs in optimizing the performance of Slack bots?]\n",
      "Got search engine results: 5 for [What are some specific applications of LLMs in optimizing the performance of Slack bots?]\n",
      "Searching DuckDuckGo for [In what ways can LLMs improve the user experience of Slack bots?]\n",
      "Got search engine results: 5 for [In what ways can LLMs improve the user experience of Slack bots?]\n",
      "Searching DuckDuckGo for [Examples of successful implementation of LLMs in Slack bots]\n",
      "Got search engine results: 5 for [Examples of successful implementation of LLMs in Slack bots]\n",
      "Got 4 search engine results for [SubjectMatterExpert]\n",
      "Got 4 search engine results for [SubjectMatterExpert]\n",
      "Got 4 search engine results for [SubjectMatterExpert]\n",
      "Generating answers for [SubjectMatterExpert]\n",
      "Swapping roles for SubjectMatterExpert\n",
      "Converted messages for SubjectMatterExpert while swapping roles: 2 messages\n",
      "Got 3 search engine queries for [SubjectMatterExpert]\n",
      "Searching DuckDuckGo for [How can Large Language Models (LLMs) enhance the conversational abilities of Slack bots?]\n",
      "Got search engine results: 5 for [How can Large Language Models (LLMs) enhance the conversational abilities of Slack bots?]\n",
      "Searching DuckDuckGo for [What are some advanced NLP methods that can be leveraged to improve Slack bots' performance?]\n",
      "Got search engine results: 5 for [What are some advanced NLP methods that can be leveraged to improve Slack bots' performance?]\n",
      "Searching DuckDuckGo for [In what ways can LLMs be effectively utilized to enhance natural language understanding and generation in Slack bots?]\n",
      "Got search engine results: 5 for [In what ways can LLMs be effectively utilized to enhance natural language understanding and generation in Slack bots?]\n",
      "Got 3 search engine results for [SubjectMatterExpert]\n",
      "Finished generating answer for [SubjectMatterExpert]\n",
      "Finished generating answer for [SubjectMatterExpert]\n",
      "Routing messages for [SubjectMatterExpert]\n",
      "Continue asking question for [SubjectMatterExpert] as this is not the last end of the conversation\n",
      "Routing messages for [SubjectMatterExpert]\n",
      "Continue asking question for [SubjectMatterExpert] as this is not the last end of the conversation\n",
      "Generating question for SashaRodriguez\n",
      "Generating question for LunaChang\n",
      "Swapping roles for SashaRodriguez\n",
      "Converted messages for SashaRodriguez while swapping roles: 3 messages\n",
      "Swapping roles for LunaChang\n",
      "Converted messages for LunaChang while swapping roles: 3 messages\n",
      "Finished generating answer for [SubjectMatterExpert]\n",
      "Routing messages for [SubjectMatterExpert]\n",
      "Continue asking question for [SubjectMatterExpert] as this is not the last end of the conversation\n",
      "Generating question for EthanPatel\n",
      "Swapping roles for EthanPatel\n",
      "Converted messages for EthanPatel while swapping roles: 3 messages\n",
      "Generated question for LunaChang\n",
      "Generating answers for [SubjectMatterExpert]\n",
      "Swapping roles for SubjectMatterExpert\n",
      "Converted messages for SubjectMatterExpert while swapping roles: 4 messages\n",
      "Generated question for EthanPatel\n",
      "Generating answers for [SubjectMatterExpert]\n",
      "Swapping roles for SubjectMatterExpert\n",
      "Converted messages for SubjectMatterExpert while swapping roles: 4 messages\n",
      "Got 2 search engine queries for [SubjectMatterExpert]\n",
      "Searching DuckDuckGo for [Benefits of using LLMs in Slack bot development]\n",
      "Got search engine results: 5 for [Benefits of using LLMs in Slack bot development]\n",
      "Searching DuckDuckGo for [Specific examples or case studies of LLM integration in Slack bots for improved natural language generation tasks]\n",
      "Got search engine results: 5 for [Specific examples or case studies of LLM integration in Slack bots for improved natural language generation tasks]\n",
      "Got 2 search engine results for [SubjectMatterExpert]\n",
      "Generated question for SashaRodriguez\n",
      "Got 2 search engine queries for [SubjectMatterExpert]\n",
      "Searching DuckDuckGo for [How does fine-tuning Large Language Models (LLMs) impact the performance of Slack bots?]\n",
      "Got search engine results: 5 for [How does fine-tuning Large Language Models (LLMs) impact the performance of Slack bots?]\n",
      "Searching DuckDuckGo for [How does fine-tuning LLMs help in optimizing responses and enhancing user interactions in Slack bots?]\n",
      "Got search engine results: 5 for [How does fine-tuning LLMs help in optimizing responses and enhancing user interactions in Slack bots?]\n",
      "Got 2 search engine results for [SubjectMatterExpert]\n",
      "Generating answers for [SubjectMatterExpert]\n",
      "Swapping roles for SubjectMatterExpert\n",
      "Converted messages for SubjectMatterExpert while swapping roles: 4 messages\n",
      "Finished generating answer for [SubjectMatterExpert]\n",
      "Finished generating answer for [SubjectMatterExpert]\n",
      "Routing messages for [SubjectMatterExpert]\n",
      "Continue asking question for [SubjectMatterExpert] as this is not the last end of the conversation\n",
      "Routing messages for [SubjectMatterExpert]\n",
      "Continue asking question for [SubjectMatterExpert] as this is not the last end of the conversation\n",
      "Generating question for AlexisChen\n",
      "Generating question for OliverKim\n",
      "Swapping roles for AlexisChen\n",
      "Converted messages for AlexisChen while swapping roles: 3 messages\n",
      "Swapping roles for OliverKim\n",
      "Converted messages for OliverKim while swapping roles: 3 messages\n",
      "Generated question for AlexisChen\n",
      "Generating answers for [SubjectMatterExpert]\n",
      "Swapping roles for SubjectMatterExpert\n",
      "Converted messages for SubjectMatterExpert while swapping roles: 4 messages\n",
      "Got 3 search engine queries for [SubjectMatterExpert]\n",
      "Searching DuckDuckGo for [Slack bots integrated with Large Language Models (LLMs) case studies]\n",
      "Got search engine results: 5 for [Slack bots integrated with Large Language Models (LLMs) case studies]\n",
      "Searching DuckDuckGo for [Impact of LLM integration on user engagement in Slack bots]\n",
      "Got search engine results: 5 for [Impact of LLM integration on user engagement in Slack bots]\n",
      "Searching DuckDuckGo for [Task completion rates improvement with LLM-integrated Slack bots examples]\n",
      "Got search engine results: 5 for [Task completion rates improvement with LLM-integrated Slack bots examples]\n",
      "Got 3 search engine results for [SubjectMatterExpert]\n",
      "Generated question for OliverKim\n",
      "Finished generating answer for [SubjectMatterExpert]\n",
      "Got 4 search engine queries for [SubjectMatterExpert]\n",
      "Searching DuckDuckGo for [What are the challenges of implementing Large Language Models (LLMs) in Slack bots?]\n",
      "Got search engine results: 5 for [What are the challenges of implementing Large Language Models (LLMs) in Slack bots?]\n",
      "Searching DuckDuckGo for [How can developers overcome the challenges of implementing LLMs in Slack bots?]\n",
      "Got search engine results: 5 for [How can developers overcome the challenges of implementing LLMs in Slack bots?]\n",
      "Searching DuckDuckGo for [What are the limitations of using LLMs in conversational AI, specifically in Slack bots?]\n",
      "Got search engine results: 5 for [What are the limitations of using LLMs in conversational AI, specifically in Slack bots?]\n",
      "Searching DuckDuckGo for [How can developers maximize the benefits of LLMs in Slack bots despite their limitations?]\n",
      "Got search engine results: 5 for [How can developers maximize the benefits of LLMs in Slack bots despite their limitations?]\n",
      "Got 4 search engine results for [SubjectMatterExpert]\n",
      "Finished generating answer for [SubjectMatterExpert]\n",
      "Generating answers for [SubjectMatterExpert]\n",
      "Swapping roles for SubjectMatterExpert\n",
      "Converted messages for SubjectMatterExpert while swapping roles: 4 messages\n",
      "Routing messages for [SubjectMatterExpert]\n",
      "Continue asking question for [SubjectMatterExpert] as this is not the last end of the conversation\n",
      "Generating question for EthanPatel\n",
      "Swapping roles for EthanPatel\n",
      "Converted messages for EthanPatel while swapping roles: 5 messages\n",
      "Routing messages for [SubjectMatterExpert]\n",
      "Continue asking question for [SubjectMatterExpert] as this is not the last end of the conversation\n",
      "Generating question for SashaRodriguez\n",
      "Swapping roles for SashaRodriguez\n",
      "Converted messages for SashaRodriguez while swapping roles: 5 messages\n",
      "Finished generating answer for [SubjectMatterExpert]\n",
      "Routing messages for [SubjectMatterExpert]\n",
      "Continue asking question for [SubjectMatterExpert] as this is not the last end of the conversation\n",
      "Generating question for LunaChang\n",
      "Swapping roles for LunaChang\n",
      "Converted messages for LunaChang while swapping roles: 5 messages\n",
      "Generated question for EthanPatel\n",
      "Generating answers for [SubjectMatterExpert]\n",
      "Swapping roles for SubjectMatterExpert\n",
      "Converted messages for SubjectMatterExpert while swapping roles: 6 messages\n",
      "Got 2 search engine queries for [SubjectMatterExpert]\n",
      "Searching DuckDuckGo for [challenges in implementing causality-guided debiasing techniques for LLM-powered Slack bots]\n",
      "Got search engine results: 5 for [challenges in implementing causality-guided debiasing techniques for LLM-powered Slack bots]\n",
      "Searching DuckDuckGo for [integration of causality-guided debiasing techniques into the development process of LLM-powered Slack bots]\n",
      "Got search engine results: 5 for [integration of causality-guided debiasing techniques into the development process of LLM-powered Slack bots]\n",
      "Got 2 search engine results for [SubjectMatterExpert]\n",
      "Generated question for SashaRodriguez\n",
      "Generated question for LunaChang\n",
      "Got 4 search engine queries for [SubjectMatterExpert]\n",
      "Searching DuckDuckGo for [How can LLMs enhance the functionality of Slack bots?]\n",
      "Got search engine results: 5 for [How can LLMs enhance the functionality of Slack bots?]\n",
      "Searching DuckDuckGo for [How can LLMs be applied to improve Slack bots?]\n",
      "Got search engine results: 5 for [How can LLMs be applied to improve Slack bots?]\n",
      "Searching DuckDuckGo for [How does fine-tuning LLMs impact the performance of Slack bots?]\n",
      "Got search engine results: 5 for [How does fine-tuning LLMs impact the performance of Slack bots?]\n",
      "Searching DuckDuckGo for [How does fine-tuning LLMs optimize responses and enhance user interactions in Slack bots?]\n",
      "Got search engine results: 5 for [How does fine-tuning LLMs optimize responses and enhance user interactions in Slack bots?]\n",
      "Got 4 search engine results for [SubjectMatterExpert]\n",
      "Finished generating answer for [SubjectMatterExpert]\n",
      "Generating answers for [SubjectMatterExpert]\n",
      "Swapping roles for SubjectMatterExpert\n",
      "Converted messages for SubjectMatterExpert while swapping roles: 6 messages\n",
      "Generating answers for [SubjectMatterExpert]\n",
      "Swapping roles for SubjectMatterExpert\n",
      "Converted messages for SubjectMatterExpert while swapping roles: 6 messages\n",
      "Routing messages for [SubjectMatterExpert]\n",
      "Continue asking question for [SubjectMatterExpert] as this is not the last end of the conversation\n",
      "Generating question for AlexisChen\n",
      "Swapping roles for AlexisChen\n",
      "Converted messages for AlexisChen while swapping roles: 5 messages\n",
      "Generated question for AlexisChen\n",
      "Generating answers for [SubjectMatterExpert]\n",
      "Swapping roles for SubjectMatterExpert\n",
      "Converted messages for SubjectMatterExpert while swapping roles: 6 messages\n",
      "Got 3 search engine queries for [SubjectMatterExpert]\n",
      "Searching DuckDuckGo for [Benefits of integrating Large Language Models (LLMs) in Slack bot development for natural language generation tasks]\n",
      "Got search engine results: 5 for [Benefits of integrating Large Language Models (LLMs) in Slack bot development for natural language generation tasks]\n",
      "Searching DuckDuckGo for [Specific examples or case studies demonstrating improvements in Slack bot performance with LLM integration]\n",
      "Got search engine results: 5 for [Specific examples or case studies demonstrating improvements in Slack bot performance with LLM integration]\n",
      "Searching DuckDuckGo for [Insights and recommendations on incorporating LLMs effectively in Slack bot design for optimal performance]\n",
      "Got search engine results: 5 for [Insights and recommendations on incorporating LLMs effectively in Slack bot design for optimal performance]\n",
      "Got 3 search engine results for [SubjectMatterExpert]\n",
      "Got 5 search engine queries for [SubjectMatterExpert]\n",
      "Finished generating answer for [SubjectMatterExpert]\n",
      "Finished generating answer for [SubjectMatterExpert]\n",
      "Got 2 search engine queries for [SubjectMatterExpert]\n",
      "Searching DuckDuckGo for [What are some best practices for developers looking to integrate Large Language Models (LLMs) into Slack bots effectively?]\n",
      "Got search engine results: 5 for [What are some best practices for developers looking to integrate Large Language Models (LLMs) into Slack bots effectively?]\n",
      "Searching DuckDuckGo for [How can developers ensure smooth integration of LLMs into Slack bots to enhance user interactions and overall performance?]\n",
      "Got search engine results: 5 for [How can developers ensure smooth integration of LLMs into Slack bots to enhance user interactions and overall performance?]\n",
      "Searching DuckDuckGo for [What are the key considerations that developers should keep in mind when incorporating LLMs into Slack bots?]\n",
      "Got search engine results: 5 for [What are the key considerations that developers should keep in mind when incorporating LLMs into Slack bots?]\n",
      "Searching DuckDuckGo for [Can you provide tips for developers on optimizing the performance of Slack bots by integrating Large Language Models (LLMs) efficiently?]\n",
      "Got search engine results: 5 for [Can you provide tips for developers on optimizing the performance of Slack bots by integrating Large Language Models (LLMs) efficiently?]\n",
      "Searching DuckDuckGo for [What are some common challenges developers may face when integrating LLMs into Slack bots, and how can they overcome them?]\n",
      "Got search engine results: 5 for [What are some common challenges developers may face when integrating LLMs into Slack bots, and how can they overcome them?]\n",
      "Searching DuckDuckGo for [Case studies of implementing LLMs in Slack bots]\n",
      "Got search engine results: 5 for [Case studies of implementing LLMs in Slack bots]\n",
      "Searching DuckDuckGo for [Practical examples of using LLMs to enhance Slack bot conversational abilities]\n",
      "Got search engine results: 5 for [Practical examples of using LLMs to enhance Slack bot conversational abilities]\n",
      "Got 5 search engine results for [SubjectMatterExpert]\n",
      "Got 2 search engine results for [SubjectMatterExpert]\n",
      "Routing messages for [SubjectMatterExpert]\n",
      "Continue asking question for [SubjectMatterExpert] as this is not the last end of the conversation\n",
      "Routing messages for [SubjectMatterExpert]\n",
      "Continue asking question for [SubjectMatterExpert] as this is not the last end of the conversation\n",
      "Generating question for OliverKim\n",
      "Generating question for EthanPatel\n",
      "Swapping roles for OliverKim\n",
      "Converted messages for OliverKim while swapping roles: 5 messages\n",
      "Swapping roles for EthanPatel\n",
      "Converted messages for EthanPatel while swapping roles: 7 messages\n",
      "Generated question for EthanPatel\n",
      "Generating answers for [SubjectMatterExpert]\n",
      "Swapping roles for SubjectMatterExpert\n",
      "Converted messages for SubjectMatterExpert while swapping roles: 8 messages\n",
      "Got 10 search engine queries for [SubjectMatterExpert]\n",
      "Searching DuckDuckGo for [How can LLMs improve the functionality of Slack bots?]\n",
      "Got search engine results: 5 for [How can LLMs improve the functionality of Slack bots?]\n",
      "Searching DuckDuckGo for [How can LLMs be applied to enhance Slack bots?]\n",
      "Got search engine results: 5 for [How can LLMs be applied to enhance Slack bots?]\n",
      "Searching DuckDuckGo for [How does fine-tuning LLMs impact the performance of Slack bots?]\n",
      "Got search engine results: 5 for [How does fine-tuning LLMs impact the performance of Slack bots?]\n",
      "Searching DuckDuckGo for [How does fine-tuning LLMs optimize responses and enhance user interactions on Slack?]\n",
      "Got search engine results: 5 for [How does fine-tuning LLMs optimize responses and enhance user interactions on Slack?]\n",
      "Searching DuckDuckGo for [How does fine-tuning LLMs help Slack bots adapt and provide accurate responses?]\n",
      "Got search engine results: 5 for [How does fine-tuning LLMs help Slack bots adapt and provide accurate responses?]\n",
      "Searching DuckDuckGo for [Can fine-tuning LLMs enhance the accuracy and adaptability of Slack bot responses?]\n",
      "Got search engine results: 5 for [Can fine-tuning LLMs enhance the accuracy and adaptability of Slack bot responses?]\n",
      "Searching DuckDuckGo for [Can I reach out for further insights on leveraging LLMs for optimizing Slack bot performance?]\n",
      "Got search engine results: 5 for [Can I reach out for further insights on leveraging LLMs for optimizing Slack bot performance?]\n",
      "Searching DuckDuckGo for [Can fine-tuning LLMs improve the ability of Slack bots to follow instructions accurately?]\n",
      "Got search engine results: 5 for [Can fine-tuning LLMs improve the ability of Slack bots to follow instructions accurately?]\n",
      "Searching DuckDuckGo for [What is the significance of fine-tuning LLMs for Slack bots?]\n",
      "Got search engine results: 5 for [What is the significance of fine-tuning LLMs for Slack bots?]\n",
      "Searching DuckDuckGo for [How does fine-tuning LLMs help Slack bots overcome challenges in following complex instructions?]\n",
      "Got search engine results: 5 for [How does fine-tuning LLMs help Slack bots overcome challenges in following complex instructions?]\n",
      "Got 10 search engine results for [SubjectMatterExpert]\n",
      "Finished generating answer for [SubjectMatterExpert]\n",
      "Finished generating answer for [SubjectMatterExpert]\n",
      "Finished generating answer for [SubjectMatterExpert]\n",
      "Generated question for OliverKim\n",
      "Routing messages for [SubjectMatterExpert]\n",
      "Continue asking question for [SubjectMatterExpert] as this is not the last end of the conversation\n",
      "Routing messages for [SubjectMatterExpert]\n",
      "Continue asking question for [SubjectMatterExpert] as this is not the last end of the conversation\n",
      "Routing messages for [SubjectMatterExpert]\n",
      "Continue asking question for [SubjectMatterExpert] as this is not the last end of the conversation\n",
      "Generating answers for [SubjectMatterExpert]\n",
      "Swapping roles for SubjectMatterExpert\n",
      "Converted messages for SubjectMatterExpert while swapping roles: 6 messages\n",
      "Generating question for SashaRodriguez\n",
      "Generating question for LunaChang\n",
      "Generating question for AlexisChen\n",
      "Swapping roles for SashaRodriguez\n",
      "Converted messages for SashaRodriguez while swapping roles: 7 messages\n",
      "Swapping roles for LunaChang\n",
      "Converted messages for LunaChang while swapping roles: 7 messages\n",
      "Swapping roles for AlexisChen\n",
      "Converted messages for AlexisChen while swapping roles: 7 messages\n",
      "Generated question for AlexisChen\n",
      "Generating answers for [SubjectMatterExpert]\n",
      "Swapping roles for SubjectMatterExpert\n",
      "Converted messages for SubjectMatterExpert while swapping roles: 8 messages\n",
      "Got 3 search engine queries for [SubjectMatterExpert]\n",
      "Searching DuckDuckGo for [Case studies of causality-guided debiasing techniques applied to LLM-powered systems]\n",
      "Got search engine results: 5 for [Case studies of causality-guided debiasing techniques applied to LLM-powered systems]\n",
      "Searching DuckDuckGo for [Real-world examples of causality-guided debiasing in LLM-powered systems]\n",
      "Got search engine results: 5 for [Real-world examples of causality-guided debiasing in LLM-powered systems]\n",
      "Searching DuckDuckGo for [Practical applications of causality-guided debiasing techniques in LLMs]\n",
      "Got search engine results: 5 for [Practical applications of causality-guided debiasing techniques in LLMs]\n",
      "Got 3 search engine results for [SubjectMatterExpert]\n",
      "Generated question for LunaChang\n",
      "Generated question for SashaRodriguez\n",
      "Got 4 search engine queries for [SubjectMatterExpert]\n",
      "Finished generating answer for [SubjectMatterExpert]\n",
      "Searching DuckDuckGo for [Building better slack bots using LLMs]\n",
      "Got search engine results: 5 for [Building better slack bots using LLMs]\n",
      "Searching DuckDuckGo for [How can LLMs be utilized to improve Slack bots' natural language understanding and generation?]\n",
      "Got search engine results: 5 for [How can LLMs be utilized to improve Slack bots' natural language understanding and generation?]\n",
      "Searching DuckDuckGo for [What challenges do developers face when implementing LLMs in Slack bots and how can they overcome them?]\n",
      "Got search engine results: 5 for [What challenges do developers face when implementing LLMs in Slack bots and how can they overcome them?]\n",
      "Searching DuckDuckGo for [Are there any case studies/examples of successfully implementing LLMs in Slack bots to enhance conversational abilities?]\n",
      "Got search engine results: 5 for [Are there any case studies/examples of successfully implementing LLMs in Slack bots to enhance conversational abilities?]\n",
      "Got 4 search engine results for [SubjectMatterExpert]\n",
      "Routing messages for [SubjectMatterExpert]\n",
      "Generating answers for [SubjectMatterExpert]\n",
      "Swapping roles for SubjectMatterExpert\n",
      "Converted messages for SubjectMatterExpert while swapping roles: 8 messages\n",
      "Generating answers for [SubjectMatterExpert]\n",
      "Swapping roles for SubjectMatterExpert\n",
      "Converted messages for SubjectMatterExpert while swapping roles: 8 messages\n",
      "Finished generating answer for [SubjectMatterExpert]\n",
      "Routing messages for [SubjectMatterExpert]\n",
      "Continue asking question for [SubjectMatterExpert] as this is not the last end of the conversation\n",
      "Generating question for OliverKim\n",
      "Swapping roles for OliverKim\n",
      "Converted messages for OliverKim while swapping roles: 7 messages\n",
      "Got 4 search engine queries for [SubjectMatterExpert]\n",
      "Searching DuckDuckGo for [Building better slack bots using LLMs]\n",
      "Got search engine results: 5 for [Building better slack bots using LLMs]\n",
      "Searching DuckDuckGo for [How LLMs can improve the conversational abilities of Slack bots]\n",
      "Got search engine results: 5 for [How LLMs can improve the conversational abilities of Slack bots]\n",
      "Searching DuckDuckGo for [Specific examples or case studies of Slack bots integrated with LLMs showing improvements in user engagement or task completion rates]\n",
      "Got search engine results: 5 for [Specific examples or case studies of Slack bots integrated with LLMs showing improvements in user engagement or task completion rates]\n",
      "Searching DuckDuckGo for [Advice or best practices for developers looking to integrate LLMs into their Slack bots effectively]\n",
      "Got search engine results: 5 for [Advice or best practices for developers looking to integrate LLMs into their Slack bots effectively]\n",
      "Got 4 search engine results for [SubjectMatterExpert]\n",
      "Generated question for OliverKim\n",
      "Got 3 search engine queries for [SubjectMatterExpert]\n",
      "Searching DuckDuckGo for [Benefits of using LLMs in enhancing Slack bot performance in natural language generation tasks]\n",
      "Got search engine results: 5 for [Benefits of using LLMs in enhancing Slack bot performance in natural language generation tasks]\n",
      "Searching DuckDuckGo for [Specific examples or case studies demonstrating improvements from LLM integration in Slack bot development]\n",
      "Got search engine results: 5 for [Specific examples or case studies demonstrating improvements from LLM integration in Slack bot development]\n",
      "Searching DuckDuckGo for [Insights and recommendations on effectively incorporating LLMs in Slack bot design for optimal performance]\n",
      "Got search engine results: 5 for [Insights and recommendations on effectively incorporating LLMs in Slack bot design for optimal performance]\n",
      "Got 3 search engine results for [SubjectMatterExpert]\n",
      "Finished generating answer for [SubjectMatterExpert]\n",
      "Generating answers for [SubjectMatterExpert]\n",
      "Swapping roles for SubjectMatterExpert\n",
      "Converted messages for SubjectMatterExpert while swapping roles: 8 messages\n",
      "Routing messages for [SubjectMatterExpert]\n",
      "Got 3 search engine queries for [SubjectMatterExpert]\n",
      "Searching DuckDuckGo for [Insights on how LLMs can introduce biases and how to mitigate them in Slack bots]\n",
      "Got search engine results: 5 for [Insights on how LLMs can introduce biases and how to mitigate them in Slack bots]\n",
      "Searching DuckDuckGo for [Challenges faced in implementing causality-guided debiasing techniques for LLM-powered Slack bots]\n",
      "Got search engine results: 5 for [Challenges faced in implementing causality-guided debiasing techniques for LLM-powered Slack bots]\n",
      "Searching DuckDuckGo for [Case studies or real-world examples of successful application of causality-guided debiasing techniques in LLM-powered systems]\n",
      "Got search engine results: 5 for [Case studies or real-world examples of successful application of causality-guided debiasing techniques in LLM-powered systems]\n",
      "Got 3 search engine results for [SubjectMatterExpert]\n",
      "Finished generating answer for [SubjectMatterExpert]\n",
      "Routing messages for [SubjectMatterExpert]\n",
      "Finished generating answer for [SubjectMatterExpert]\n",
      "Routing messages for [SubjectMatterExpert]\n",
      "Finished generating answer for [SubjectMatterExpert]\n",
      "Routing messages for [SubjectMatterExpert]\n",
      "conduct_interviews\n",
      "--  {'topic': 'Building better slack bots using LLMs', 'outline': Outline(page_title='Building Better Slack Bots Using Large Language Models (LLMs)', sections=[Section(section_title='Introduction', description='Overview of using Large Language Models (LLMs) to enhance Slack bots.', subsections=None), Se\n",
      "refine_outline\n",
      "--  {'topic': 'Building better slack bots using LLMs', 'outline': Outline(page_title='Building Better Slack Bots Using Large Language Models (LLMs)', sections=[Section(section_title='Introduction', description='Overview of using Large Language Models (LLMs) to enhance Slack bots.', subsections=None), Se\n",
      "index_references\n",
      "--  {'topic': 'Building better slack bots using LLMs', 'outline': Outline(page_title='Building Better Slack Bots Using Large Language Models (LLMs)', sections=[Section(section_title='Introduction', description='Overview of using Large Language Models (LLMs) to enhance Slack bots.', subsections=None), Se\n",
      "write_sections\n",
      "--  {'topic': 'Building better slack bots using LLMs', 'outline': Outline(page_title='Building Better Slack Bots Using Large Language Models (LLMs)', sections=[Section(section_title='Introduction', description='Overview of using Large Language Models (LLMs) to enhance Slack bots.', subsections=None), Se\n",
      "write_article\n",
      "--  {'topic': 'Building better slack bots using LLMs', 'outline': Outline(page_title='Building Better Slack Bots Using Large Language Models (LLMs)', sections=[Section(section_title='Introduction', description='Overview of using Large Language Models (LLMs) to enhance Slack bots.', subsections=None), Se\n",
      "__end__\n",
      "--  {'topic': 'Building better slack bots using LLMs', 'outline': Outline(page_title='Building Better Slack Bots Using Large Language Models (LLMs)', sections=[Section(section_title='Introduction', description='Overview of using Large Language Models (LLMs) to enhance Slack bots.', subsections=None), Se\n"
     ]
    }
   ],
   "source": [
    "async for step in storm.astream(\n",
    "    {\n",
    "        \"topic\": \"Building better slack bots using LLMs\",\n",
    "    }\n",
    "):\n",
    "    name = next(iter(step))\n",
    "    print(name)\n",
    "    print(\"-- \", str(step[name])[:300])\n",
    "    if END in step:\n",
    "        results = step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "1bef7021",
   "metadata": {},
   "outputs": [],
   "source": [
    "article = results[END][\"article\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b094067",
   "metadata": {},
   "source": [
    "## Render the Wiki\n",
    "\n",
    "Now we can render the final wiki page!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "f7750c83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Building Better Slack Bots Using LLMs\n",
       "\n",
       "### Impact of Million-Plus Token Context Window Language Models on RAG\n",
       "\n",
       "The introduction section provides an overview of the impact of million-plus token context window language models on the Retrieval-Augmented Generation (RAG) framework. These advanced language models, with the capability to process over a million tokens in their context window, have revolutionized natural language processing tasks. In the context of RAG, these models play a crucial role in enhancing information retrieval and generation processes by enabling a deeper understanding of multimodal inputs and optimizing the handling of longer inputs. The introduction sets the stage for exploring the technical aspects, enhancements in information retrieval and generation, advancements in extending context windows, applications, challenges, and considerations associated with integrating million-plus token context window language models with the RAG framework.\n",
       "\n",
       "### Understanding Large Language Models (LLMs)\n",
       "\n",
       "Large Language Models (LLMs) have revolutionized the field of natural language processing with their ability to understand, generate, and manipulate text on a massive scale. These models, such as Gemini 1.5, have significantly impacted the Retrieval-Augmented Generation (RAG) framework by enhancing information retrieval and generation processes.\n",
       "\n",
       "#### Overview of Large Language Models (LLMs)\n",
       "\n",
       "Large Language Models (LLMs) are advanced AI models that leverage massive amounts of training data to understand and generate text. They have the capacity to process millions of tokens in a single context window, leading to improved performance in various NLP tasks.\n",
       "\n",
       "#### Significance of LLMs in Conversational AI\n",
       "\n",
       "LLMs play a crucial role in advancing Conversational AI capabilities by enabling chatbots and virtual assistants to comprehend and generate human-like text. Through prompt engineering, LLM-based chatbots can be guided to exhibit desired behaviors in conversational settings.\n",
       "\n",
       "#### StreamingLLM Framework\n",
       "\n",
       "The StreamingLLM framework is an innovative approach that allows large language models to handle text sequences of infinite length without the need for fine-tuning. By preserving attention sinks and maintaining a near-normal attention score distribution, StreamingLLM ensures seamless processing of lengthy conversations.[3]\n",
       "\n",
       "### Benefits of Incorporating LLMs in Slack Bots\n",
       "\n",
       "Large Language Models (LLMs) have revolutionized the capabilities of Slack bots, enabling them to engage in more natural and context-aware conversations with users. By incorporating LLMs into Slack bots, several benefits arise:\n",
       "\n",
       "#### Enhanced Conversational Abilities\n",
       "\n",
       "LLMs empower Slack bots to understand nuances in user inputs, provide more relevant responses, and mimic human-like conversations, leading to improved user experience and interaction.\n",
       "\n",
       "#### Adaptability to Various User Inputs\n",
       "\n",
       "With LLMs, Slack bots can adapt to a wide range of user inputs, allowing them to handle diverse queries and requests effectively, enhancing the bot's versatility and utility.\n",
       "\n",
       "#### Improved Context Awareness\n",
       "\n",
       "LLMs enable Slack bots to maintain context across conversations, making them capable of remembering previous interactions and providing coherent responses, which enhances the overall conversational flow.[4]\n",
       "\n",
       "### Challenges and Considerations\n",
       "\n",
       "Implementing million-plus token context window language models in the RAG framework presents several challenges and considerations that need to be addressed for optimal performance and effectiveness.\n",
       "\n",
       "#### Model Complexity\n",
       "\n",
       "One of the primary challenges is the increased complexity of million-plus token models, requiring significant computational resources and memory capacity. This complexity can hinder the efficiency of training and inference processes.\n",
       "\n",
       "#### Fine-Tuning and Adaptation\n",
       "\n",
       "Fine-tuning million-plus token models for specific tasks within the RAG framework can be a non-trivial task. It requires substantial expertise and time to adapt the models effectively, potentially limiting their widespread application.\n",
       "\n",
       "#### Data Requirements\n",
       "\n",
       "Utilizing million-plus token models often demands large volumes of high-quality training data to achieve optimal performance. Acquiring, preprocessing, and managing such extensive datasets can be resource-intensive and challenging.\n",
       "\n",
       "#### Interpretability and Bias\n",
       "\n",
       "The interpretability of outputs from million-plus token models can be challenging, as the models' complexity may obscure the reasoning behind their generated responses. Moreover, these models may inherit biases present in the training data, necessitating careful mitigation strategies.\n",
       "\n",
       "#### Ethical and Legal Implications\n",
       "\n",
       "Deploying large language models in the RAG framework raises ethical concerns related to misinformation, privacy, and potential misuse. Addressing these implications requires robust governance frameworks and adherence to legal regulations.[5]\n",
       "\n",
       "### Best Practices for Building Slack Bots with LLMs\n",
       "\n",
       "Building Slack bots with Large Language Models (LLMs) requires attention to several best practices to ensure optimal performance and user experience. By following these guidelines, developers can create more effective and engaging bots that leverage the power of LLMs to enhance conversational interactions.\n",
       "\n",
       "#### 1. Data Preparation\n",
       "\n",
       "Ensure that the training data used for fine-tuning LLMs on Slack messages is relevant and representative of the conversations the bot is expected to engage in. Cleaning and preprocessing the data is essential to remove noise and irrelevant information.\n",
       "\n",
       "#### 2. Fine-Tuning Process\n",
       "\n",
       "Use HuggingFace's libraries for fine-tuning LLMs on Slack messages, following best practices to adapt pre-trained models effectively. Consider the specific use case and desired conversational style when fine-tuning the LLM for Slack bot deployment.\n",
       "\n",
       "#### 3. Prompt Engineering\n",
       "\n",
       "Implement prompt engineering techniques to guide the behavior of the LLM-powered Slack bot. Crafting appropriate prompts can influence the bot's responses and improve the quality of generated text in conversations.\n",
       "\n",
       "#### 4. Continuous Monitoring and Evaluation\n",
       "\n",
       "Regularly monitor the performance of the LLM-powered Slack bot in real-world interactions. Collect feedback from users and evaluate the bot's responses to identify areas for improvement and refine the conversational capabilities.[6]\n",
       "\n",
       "### Case Studies\n",
       "\n",
       "Various case studies have demonstrated the practical applications of integrating million-plus token context window language models with the RAG framework. These case studies highlight the effectiveness of utilizing large language models for enhancing information retrieval and generation processes.\n",
       "\n",
       "#### Medical Data Analysis\n",
       "\n",
       "In a case study focused on medical data analysis, researchers used million-plus token context window language models to process vast amounts of medical literature and patient records. By leveraging the extended context windows, the models improved the accuracy of generating summaries and answering complex medical questions within the RAG system.\n",
       "\n",
       "#### Financial News Summarization\n",
       "\n",
       "Another case study explored the use of million-plus token context window language models for summarizing financial news articles. By training the models on a diverse range of financial data and news sources, the system was able to generate concise and informative summaries that captured the key insights from lengthy articles, showcasing the efficiency of these models in information condensation.\n",
       "\n",
       "#### Legal Document Analysis\n",
       "\n",
       "In a legal document analysis case study, million-plus token context window language models were applied to extract relevant information from legal texts and assist in generating case summaries. The models' ability to consider a broader context window enabled more accurate retrieval of legal precedents and facilitated the creation of coherent summaries, demonstrating their utility in the legal domain.[7]\n",
       "\n",
       "### References\n",
       "\n",
       "[1] https://yellow.ai/blog/large-language-models/\n",
       "[2] https://www.analyticsvidhya.com/blog/2023/07/llms-in-conversational-ai/\n",
       "[3] https://bdtechtalks.com/2023/11/27/streamingllm/\n",
       "[4] https://opendatascience.com/fine-tuning-llms-on-slack-messages/\n",
       "[5] https://towardsai.net/p/machine-learning/how-to-create-your-own-llm-powered-slackbot-with-langchain-on-your-own-private-data\n",
       "[6] https://odsc.medium.com/fine-tuning-llms-on-slack-messages-f580d4996cc3\n",
       "[7] https://datasciencedojo.com/blog/llm-chatbot/"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "# We will down-header the sections to create less confusion in this notebook\n",
    "Markdown(article.replace(\"\\n#\", \"\\n##\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e24611",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
