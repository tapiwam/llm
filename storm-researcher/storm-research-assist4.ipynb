{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b912966-4dce-4685-a5b2-a39c5229a0f1",
   "metadata": {},
   "source": [
    "# Storm Research Assistant\n",
    "\n",
    "Reference\n",
    "https://github.com/langchain-ai/langgraph/blob/main/examples/storm/storm.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f32108c2-977f-450d-82cb-90aa21f09171",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert_load_from_file: gguf version     = 2\n",
      "bert_load_from_file: gguf alignment   = 32\n",
      "bert_load_from_file: gguf data offset = 695552\n",
      "bert_load_from_file: model name           = BERT\n",
      "bert_load_from_file: model architecture   = bert\n",
      "bert_load_from_file: model file type      = 1\n",
      "bert_load_from_file: bert tokenizer vocab = 30522\n"
     ]
    }
   ],
   "source": [
    "from storm import *\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "\n",
    "\n",
    "fast_llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "# long_context_llm = ChatOpenAI(model=\"gpt-4-turbo-preview\")\n",
    "long_context_llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\n",
    "\n",
    "# haiku model\n",
    "# haiku_model_name = \"claude-3-haiku-20240307\"\n",
    "# fast_llm = ChatAnthropic(model_name=haiku_model_name)\n",
    "# long_context_llm = ChatAnthropic(model_name=haiku_model_name)\n",
    "\n",
    "\n",
    "embeddings = get_gpt4all_embeddings()\n",
    "\n",
    "vectorstore_dir = \"./data/storm/vectorstore/\"\n",
    "vectorstore = Chroma(persist_directory=vectorstore_dir, embedding_function=embeddings)\n",
    "\n",
    "interview_config = InterviewConfig(long_llm=long_context_llm, \n",
    "                                   fast_llm=fast_llm, \n",
    "                                   max_conversations=3, \n",
    "                                   max_reference_length=10000,\n",
    "                                   tags_to_extract=[ \"p\", \"h1\", \"h2\", \"h3\"],\n",
    "                                   embeddings=embeddings,\n",
    "                                   vectorstore=vectorstore,\n",
    "                                   vectorstore_dir=vectorstore_dir,\n",
    "                                   runnable_config=RunnableConfig()\n",
    "                                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c8209e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_topic = \"Increase development productivity by using Docker compose and local docker labs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1908675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching DuckDuckGo for [Docker compose]\n",
      "Got search engine results: 5 for [Docker compose]\n",
      "- {'title': 'Introduction to Docker Compose | Baeldung on Ops', 'href': 'https://www.baeldung.com/ops/docker-compose', 'body': 'Learn how to use Docker Compose to manage multiple containers with a single YAML file. Discover the features and mechanisms of services, volumes, networks, and more.'}\n",
      "- {'title': 'How to Set up Docker Compose: Step-By-Step Tutorial', 'href': 'https://medium.com/@kaaiot/how-to-set-up-docker-compose-step-by-step-tutorial-9c339df67a2d', 'body': 'Docker Compose is like a helpful assistant for organizing and running complex applications with multiple containers. By using a straightforward YAML file, you can easily define all the services ...'}\n",
      "- {'title': 'How to Use Docker Compose V2 | Linode Docs', 'href': 'https://www.linode.com/docs/guides/how-to-use-docker-compose-v2/', 'body': 'To install and use Docker Compose V2, add the Docker repository to the local package manager. Then download and install Docker Engine, Docker CLI, and the Compose plug-in. Construct the docker-compose.yml file, adding the services/containers to use, along with any storage space, networking components, and configurations.'}\n",
      "- {'title': 'Difference Between Docker, Dockerfile, and Docker Compose', 'href': 'https://www.baeldung.com/ops/docker-dockerfile-docker-compose', 'body': 'Docker Compose is a tool for defining and running multi-container Docker applications. Using a YAML configuration file, Docker Compose allows us to configure multiple containers in one place. We can then start and stop all of those containers at once using a single command.'}\n",
      "- {'title': 'Docker - Compose - GeeksforGeeks', 'href': 'https://www.geeksforgeeks.org/docker-compose/', 'body': 'Learn how to use Docker Compose to run multi-container applications with a YAML file. See an example of creating a Python API and a PHP website with Docker Compose.'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'content': 'Learn how to use Docker Compose to manage multiple containers with a single YAML file. Discover the features and mechanisms of services, volumes, networks, and more.',\n",
       "  'url': 'https://www.baeldung.com/ops/docker-compose'},\n",
       " {'content': 'Docker Compose is like a helpful assistant for organizing and running complex applications with multiple containers. By using a straightforward YAML file, you can easily define all the services ...',\n",
       "  'url': 'https://medium.com/@kaaiot/how-to-set-up-docker-compose-step-by-step-tutorial-9c339df67a2d'},\n",
       " {'content': 'To install and use Docker Compose V2, add the Docker repository to the local package manager. Then download and install Docker Engine, Docker CLI, and the Compose plug-in. Construct the docker-compose.yml file, adding the services/containers to use, along with any storage space, networking components, and configurations.',\n",
       "  'url': 'https://www.linode.com/docs/guides/how-to-use-docker-compose-v2/'},\n",
       " {'content': 'Docker Compose is a tool for defining and running multi-container Docker applications. Using a YAML configuration file, Docker Compose allows us to configure multiple containers in one place. We can then start and stop all of those containers at once using a single command.',\n",
       "  'url': 'https://www.baeldung.com/ops/docker-dockerfile-docker-compose'},\n",
       " {'content': 'Learn how to use Docker Compose to run multi-container applications with a YAML file. See an example of creating a Python API and a PHP website with Docker Compose.',\n",
       "  'url': 'https://www.geeksforgeeks.org/docker-compose/'}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# await search_engine.ainvoke(\"Docker compose\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46e767af",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mx\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'x' is not defined"
     ]
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22bb213",
   "metadata": {},
   "source": [
    "## Test Single Interview using StormInterviewGraph1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22285165",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# interview_graph = StormInterviewGraph1(interview_config=interview_config)\n",
    "\n",
    "# context = \"increase development productivity by using Docker compose and local docker labs\"\n",
    "\n",
    "# tm1 = HumanMessage(content=\"What is the best way to increase development productivity by using Docker compose and local docker labs\", name=\"John Smith\")\n",
    "\n",
    "# test_state = InterviewState(\n",
    "#     context=context,\n",
    "#     interview_config=interview_config,\n",
    "#     editor=Editor(affiliation=\"Software Company X\", name=\"John Doe\", role=\"Lead Software Engineer\", description=\"Experienced software engineer.\"),\n",
    "#     messages=[],\n",
    "#     references={}\n",
    "# )\n",
    "\n",
    "# # Run interview\n",
    "\n",
    "# final_step = None\n",
    "# async for step in interview_graph.graph.astream(test_state.as_dict()):\n",
    "#     name = next(iter(step))\n",
    "#     print(name)\n",
    "#     print(f\"Processing step: {name}\")\n",
    "#     print(\"-- \", str(step[name][\"messages\"])[:300])\n",
    "#     if END in step:\n",
    "#         final_step = step\n",
    "        \n",
    "# final_state = next(iter(final_step.values()))\n",
    "# final_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b221bf",
   "metadata": {},
   "source": [
    "## Test Full Graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010d034f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# i = Interviews(topic=example_topic, interview_config=interview_config)\n",
    "\n",
    "# g = StormGraph(interview_config=interview_config, topic=example_topic)\n",
    "# g1 = await g.graph.ainvoke(i.as_dict())\n",
    "\n",
    "# g1x = Interviews.from_dict(g1)\n",
    "# g1x.outline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8216ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1fb75b15",
   "metadata": {},
   "source": [
    "## Reload from JSON\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c614b94b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interviews.from_dict: data is an instance of dict\n",
      "InterviewState.from_dict: data is an instance of dict\n",
      "InterviewState.from_dict: data is an instance of dict\n",
      "InterviewState.from_dict: data is an instance of dict\n",
      "InterviewState.from_dict: data is an instance of dict\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'section_title': 'Introduction to Docker Compose',\n",
       "  'content': 'Docker Compose is a tool that allows users to define and run multi-container Docker applications in an efficient and streamlined manner. It simplifies the process of managing complex applications by providing a way to define services, dependencies, and networks in a single file. With Docker Compose, developers can easily orchestrate the deployment of multiple containers, making it easier to replicate the production environment locally for development and testing purposes.',\n",
       "  'subsections': [{'subsection_title': 'What is Docker Compose?',\n",
       "    'content': 'Docker Compose is a tool designed to simplify the process of defining and running multi-container Docker applications. It allows developers to use a YAML file to configure the services, networks, and volumes required for their application. By defining these configurations in a single file, Docker Compose automates the creation and management of the containers, making it easier to deploy and scale applications.'},\n",
       "   {'subsection_title': 'Key Features of Docker Compose',\n",
       "    'content': 'Some key features of Docker Compose include the ability to define services with specific configurations, manage dependencies between containers, scale containers up or down based on requirements, and easily create and manage networks for communication between containers. These features make it a powerful tool for streamlining the development and deployment of multi-container applications.'}],\n",
       "  'citations': ['https://dev.to/theramoliya/docker-utilize-docker-compose-for-local-development-environments-17di',\n",
       "   'https://reintech.io/blog/leveraging-docker-compose-local-dev-environments']},\n",
       " {'section_title': 'Benefits of Using Docker Compose',\n",
       "  'content': 'Incorporating Docker Compose into the development workflow offers various advantages that enhance productivity and consistency. Docker Compose is a tool that simplifies the process of defining and running multi-container Docker applications efficiently. By leveraging Docker Compose, developers can streamline their development processes, automate container management tasks, and ensure consistency across different environments.',\n",
       "  'subsections': [{'subsection_title': 'Improved Development Workflow',\n",
       "    'content': 'Docker Compose significantly improves the development workflow by automating container management tasks. It allows developers to define multi-container applications in a single file, making it easier to spin up complex environments with a single command. Docker Compose also enables the isolation of dependencies in separate containers, reducing conflicts and ensuring that each service runs in its own container, leading to a more efficient development process. [#1] [#2]'},\n",
       "   {'subsection_title': 'Consistent Deployment Environments',\n",
       "    'content': \"One of the key benefits of using Docker Compose is the ability to maintain consistency across different development and production environments. Developers can define all the services and dependencies required for their applications in a Docker Compose file, ensuring that every team member uses the same development environment. This consistency minimizes the 'it works on my machine' issues and facilitates seamless collaboration within the team. Docker Compose also helps in reducing configuration errors, streamlining deployment processes, and enhancing overall productivity. [#1] [#2]\"}],\n",
       "  'citations': ['https://dev.to/theramoliya/docker-utilize-docker-compose-for-local-development-environments-17di',\n",
       "   'https://reintech.io/blog/leveraging-docker-compose-local-dev-environments']},\n",
       " {'section_title': 'Setting Up Local Docker Labs',\n",
       "  'content': 'Setting up local Docker labs is essential for facilitating testing, experimentation, and learning in a controlled environment. By creating a local Docker lab, developers can simulate complex application architectures, test new features, and troubleshoot without affecting production environments. Docker Compose plays a crucial role in setting up these labs by simplifying the management of multi-container applications.',\n",
       "  'subsections': [{'subsection_title': 'Creating a Local Docker Lab',\n",
       "    'content': 'To create a local Docker lab, follow these step-by-step instructions: 1. Define your application architecture in a docker-compose.yml file. 2. Specify the services, networks, and volumes required for your application. 3. Use Docker Compose commands to build, start, and manage your containers. 4. Test and iterate on your application within the local Docker lab environment to ensure functionality and performance.'},\n",
       "   {'subsection_title': 'Best Practices for Utilizing Local Docker Labs',\n",
       "    'content': 'Effective utilization of local Docker labs involves: 1. Ensuring consistency by providing the same development environment to all team members. 2. Isolating dependencies in separate containers to avoid conflicts. 3. Streamlining deployment processes to reduce configuration errors. 4. Enhancing collaboration within development teams through shared lab environments.'}],\n",
       "  'citations': []},\n",
       " {'section_title': 'Optimizing Development Productivity with Docker Tools',\n",
       "  'content': 'In addition to Docker Compose and setting up local Docker labs, there are additional Docker tools and practices that can further enhance development productivity and efficiency.',\n",
       "  'subsections': [{'subsection_title': 'Integration with CI/CD Pipelines',\n",
       "    'content': 'One way to optimize development productivity is by integrating Docker Compose and local Docker labs into continuous integration and continuous deployment (CI/CD) pipelines. By incorporating Docker tools into the CI/CD process, developers can automate testing and deployment workflows. This integration ensures that code changes are thoroughly tested in a controlled environment before being deployed to production. Docker Compose can be utilized to define the services required for testing and staging environments, while local Docker labs can simulate the production environment for accurate testing. [#1] [#2]'},\n",
       "   {'subsection_title': 'Performance Tuning and Monitoring',\n",
       "    'content': 'Another aspect of optimizing development productivity with Docker tools is performance tuning and monitoring. Docker provides tools and features to optimize container performance, such as setting resource constraints, managing container networking, and utilizing efficient storage solutions. Monitoring tools like Docker Stats and Docker Events can help developers track the performance of containerized applications, identify bottlenecks, and make informed decisions to improve productivity. By monitoring container metrics and performance data, developers can ensure that applications run efficiently and meet performance expectations. [#3] [#4]'}],\n",
       "  'citations': ['https://dev.to/theramoliya/docker-utilize-docker-compose-for-local-development-environments-17di',\n",
       "   'https://reintech.io/blog/leveraging-docker-compose-local-dev-environments',\n",
       "   'https://utopia-insights.dev/docker-compose-makes-your-dev-environment-as-fast-as-it-gets/',\n",
       "   'https://medium.com/simform-engineering/setting-up-a-local-development-environment-using-docker-compose-551efb4ec0ee']}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read Interviews from interviews.json\n",
    "with open(\"interviews.json\", \"r\") as f:\n",
    "    interviews = json.load(f)\n",
    "    i = Interviews.from_dict(interviews)\n",
    "    i.interview_config = interview_config\n",
    "    for c in i.conversations or []:\n",
    "        c.interview_config = interview_config\n",
    "\n",
    "# i\n",
    "i.wiki_sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41119bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# g1y = await node_generate_sections(i)\n",
    "# g1y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f770dcee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48fb5b9c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mx\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'x' is not defined"
     ]
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ae35f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_wikipedia_retriever(k: int = 2, content_chars_max: int = 5000) -> WikipediaRetriever:\n",
    "#     return WikipediaRetriever(load_all_available_meta=True, top_k_results=k, doc_content_chars_max=content_chars_max)\n",
    "\n",
    "# wr = get_wikipedia_retriever()\n",
    "# docs = wr.invoke(\"Fornite\")\n",
    "\n",
    "# print(f\"Number of docs: {len(docs)}\")\n",
    "# for doc in docs:\n",
    "#     print(f\"==============\\nLength: {len(doc.page_content)}\")\n",
    "#     pprint.pprint(doc.page_content[:300])\n",
    "    \n",
    "#     print(f\"Metdata: {doc.metadata.keys()}\")\n",
    "#     print(f\"Source: {doc.metadata['source']}\")\n",
    "#     print(f\"URL: {doc.metadata['page_url']}\")\n",
    "#     print(f\"Title: {doc.metadata['title']}\")\n",
    "#     print(f\"Categories: {doc.metadata['categories']}\")\n",
    "#     print(f\"Summary: {doc.metadata['summary']}\")\n",
    "#     print(f\"References: {doc.metadata['references']}\")\n",
    "#     print(f\"Related Titles: {doc.metadata['related_titles']}\")\n",
    "#     print(f\"Sections: {doc.metadata['sections']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8b76b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching DuckDuckGo for [Docker compose]\n",
      "Got search engine results: 5 for [Docker compose]\n",
      "Got search engine results: \n",
      "[{'title': 'Introduction to Docker Compose | Baeldung on Ops', 'href': 'https://www.baeldung.com/ops/docker-compose', 'body': \"Docker Compose is a tool that helps us overcome this problem and easily handle multiple containers at once. In this tutorial, we'll examine its main features and powerful mechanisms. 2. The YAML Configuration Explained. In short, Docker Compose works by applying many rules declared within a single docker-compose.yml configuration file.\"}, {'title': 'How to Set up Docker Compose: Step-By-Step Tutorial', 'href': 'https://medium.com/@kaaiot/how-to-set-up-docker-compose-step-by-step-tutorial-9c339df67a2d', 'body': 'Docker Compose is like a helpful assistant for organizing and running complex applications with multiple containers. By using a straightforward YAML file, you can easily define all the services ...'}, {'title': 'Docker Compose for Python Applications: A Comprehensive Guide', 'href': 'https://www.geeksforgeeks.org/docker-compose-for-python-applications/', 'body': 'Docker Compose is a tool for defining and managing multi-container Docker applications, it utilizes a YAML configuration file to indicate the services, network, and volumes expected for an application, allowing developers to define complex application architectures and manage them as a single unit.'}, {'title': 'How to Use Docker Compose V2 | Linode Docs', 'href': 'https://www.linode.com/docs/guides/how-to-use-docker-compose-v2/', 'body': 'To install and use Docker Compose V2, add the Docker repository to the local package manager. Then download and install Docker Engine, Docker CLI, and the Compose plug-in. Construct the docker-compose.yml file, adding the services/containers to use, along with any storage space, networking components, and configurations.'}, {'title': 'Difference Between Docker, Dockerfile, and Docker Compose', 'href': 'https://www.baeldung.com/ops/docker-dockerfile-docker-compose', 'body': 'Docker Compose is a tool for defining and running multi-container Docker applications. Using a YAML configuration file, Docker Compose allows us to configure multiple containers in one place. We can then start and stop all of those containers at once using a single command.'}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'content': \"Docker Compose is a tool that helps us overcome this problem and easily handle multiple containers at once. In this tutorial, we'll examine its main features and powerful mechanisms. 2. The YAML Configuration Explained. In short, Docker Compose works by applying many rules declared within a single docker-compose.yml configuration file.\",\n",
       "  'url': 'https://www.baeldung.com/ops/docker-compose'},\n",
       " {'content': 'Docker Compose is like a helpful assistant for organizing and running complex applications with multiple containers. By using a straightforward YAML file, you can easily define all the services ...',\n",
       "  'url': 'https://medium.com/@kaaiot/how-to-set-up-docker-compose-step-by-step-tutorial-9c339df67a2d'},\n",
       " {'content': 'Docker Compose is a tool for defining and managing multi-container Docker applications, it utilizes a YAML configuration file to indicate the services, network, and volumes expected for an application, allowing developers to define complex application architectures and manage them as a single unit.',\n",
       "  'url': 'https://www.geeksforgeeks.org/docker-compose-for-python-applications/'},\n",
       " {'content': 'To install and use Docker Compose V2, add the Docker repository to the local package manager. Then download and install Docker Engine, Docker CLI, and the Compose plug-in. Construct the docker-compose.yml file, adding the services/containers to use, along with any storage space, networking components, and configurations.',\n",
       "  'url': 'https://www.linode.com/docs/guides/how-to-use-docker-compose-v2/'},\n",
       " {'content': 'Docker Compose is a tool for defining and running multi-container Docker applications. Using a YAML configuration file, Docker Compose allows us to configure multiple containers in one place. We can then start and stop all of those containers at once using a single command.',\n",
       "  'url': 'https://www.baeldung.com/ops/docker-dockerfile-docker-compose'}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Generate Article\n",
    "\n",
    "# await search_engine.ainvoke(\"Docker compose\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e708d31a",
   "metadata": {},
   "source": [
    "#### Generate Sections\n",
    "\n",
    "Now you can generate the sections using the indexed docs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0afd728d",
   "metadata": {},
   "source": [
    "#### Generate final article\n",
    "\n",
    "Now we can rewrite the draft to appropriately group all the citations and maintain a consistent voice.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05089f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "writer_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are an expert Wikipedia author. Write the complete wiki article on {topic} using the following section drafts:\\n\\n\"\n",
    "            \"{draft}\\n\\nStrictly follow Wikipedia format guidelines.\",\n",
    "        ),\n",
    "        (\n",
    "            \"user\",\n",
    "            'Write the complete Wiki article using markdown format. Organize citations using footnotes like \"[1]\",\"\" avoiding duplicates in the footer. Include URLs in the footer.',\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "writer = writer_prompt | long_context_llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6e6e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for tok in writer.stream({\"topic\": example_topic, \"draft\": section.as_str}):\n",
    "    print(tok, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6ab734",
   "metadata": {},
   "source": [
    "## Final Flow\n",
    "\n",
    "Now it's time to string everything together. We will have 6 main stages in sequence:\n",
    ".\n",
    "\n",
    "1. Generate the initial outline + perspectives\n",
    "2. Batch converse with each perspective to expand the content for the article\n",
    "3. Refine the outline based on the conversations\n",
    "4. Index the reference docs from the conversations\n",
    "5. Write the individual sections of the article\n",
    "6. Write the final wiki\n",
    "\n",
    "The state tracks the outputs of each stage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e775ba8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResearchState(TypedDict):\n",
    "    topic: str\n",
    "    outline: Outline\n",
    "    editors: List[Editor]\n",
    "    interview_results: List[InterviewState]\n",
    "    # The final sections output\n",
    "    sections: List[WikiSection]\n",
    "    article: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1854d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "\n",
    "async def initialize_research(state: ResearchState):\n",
    "    topic = state[\"topic\"]\n",
    "    coros = (\n",
    "        generate_outline_direct.ainvoke({\"topic\": topic}),\n",
    "        survey_subjects.ainvoke(topic),\n",
    "    )\n",
    "    results = await asyncio.gather(*coros)\n",
    "    return {\n",
    "        **state,\n",
    "        \"outline\": results[0],\n",
    "        \"editors\": results[1].editors,\n",
    "    }\n",
    "\n",
    "\n",
    "async def conduct_interviews(state: ResearchState):\n",
    "    topic = state[\"topic\"]\n",
    "    initial_states = [\n",
    "        {\n",
    "            \"editor\": editor,\n",
    "            \"messages\": [\n",
    "                AIMessage(\n",
    "                    content=f\"So you said you were writing an article on {topic}?\",\n",
    "                    name=\"SubjectMatterExpert\",\n",
    "                )\n",
    "            ],\n",
    "        }\n",
    "        for editor in state[\"editors\"]\n",
    "    ]\n",
    "    # We call in to the sub-graph here to parallelize the interviews\n",
    "    interview_results = await interview_graph.abatch(initial_states)\n",
    "\n",
    "    return {\n",
    "        **state,\n",
    "        \"interview_results\": interview_results,\n",
    "    }\n",
    "\n",
    "\n",
    "def format_conversation(interview_state):\n",
    "    messages = interview_state[\"messages\"]\n",
    "    convo = \"\\n\".join(f\"{m.name}: {m.content}\" for m in messages)\n",
    "    return f'Conversation with {interview_state[\"editor\"].name}\\n\\n' + convo\n",
    "\n",
    "\n",
    "async def refine_outline(state: ResearchState):\n",
    "    convos = \"\\n\\n\".join(\n",
    "        [\n",
    "            format_conversation(interview_state)\n",
    "            for interview_state in state[\"interview_results\"]\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    updated_outline = await refine_outline_chain.ainvoke(\n",
    "        {\n",
    "            \"topic\": state[\"topic\"],\n",
    "            \"old_outline\": state[\"outline\"].as_str,\n",
    "            \"conversations\": convos,\n",
    "        }\n",
    "    )\n",
    "    return {**state, \"outline\": updated_outline}\n",
    "\n",
    "\n",
    "async def index_references(state: ResearchState):\n",
    "    all_docs = []\n",
    "    for interview_state in state[\"interview_results\"]:\n",
    "        reference_docs = [\n",
    "            Document(page_content=v, metadata={\"source\": k})\n",
    "            for k, v in interview_state[\"references\"].items()\n",
    "        ]\n",
    "        all_docs.extend(reference_docs)\n",
    "    await vectorstore.aadd_documents(all_docs)\n",
    "    return state\n",
    "\n",
    "\n",
    "async def write_sections(state: ResearchState):\n",
    "    outline = state[\"outline\"]\n",
    "    sections = await section_writer.abatch(\n",
    "        [\n",
    "            {\n",
    "                \"outline\": refined_outline.as_str,\n",
    "                \"section\": section.section_title,\n",
    "                \"topic\": state[\"topic\"],\n",
    "            }\n",
    "            for section in outline.sections\n",
    "        ]\n",
    "    )\n",
    "    return {\n",
    "        **state,\n",
    "        \"sections\": sections,\n",
    "    }\n",
    "\n",
    "\n",
    "async def write_article(state: ResearchState):\n",
    "    topic = state[\"topic\"]\n",
    "    sections = state[\"sections\"]\n",
    "    draft = \"\\n\\n\".join([section.as_str for section in sections])\n",
    "    article = await writer.ainvoke({\"topic\": topic, \"draft\": draft})\n",
    "    return {\n",
    "        **state,\n",
    "        \"article\": article,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87881e3",
   "metadata": {},
   "source": [
    "#### Create the graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3b4be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "builder_of_storm = StateGraph(ResearchState)\n",
    "\n",
    "nodes = [\n",
    "    (\"init_research\", initialize_research),\n",
    "    (\"conduct_interviews\", conduct_interviews),\n",
    "    (\"refine_outline\", refine_outline),\n",
    "    (\"index_references\", index_references),\n",
    "    (\"write_sections\", write_sections),\n",
    "    (\"write_article\", write_article),\n",
    "]\n",
    "for i in range(len(nodes)):\n",
    "    name, node = nodes[i]\n",
    "    builder_of_storm.add_node(name, node)\n",
    "    if i > 0:\n",
    "        builder_of_storm.add_edge(nodes[i - 1][0], name)\n",
    "\n",
    "builder_of_storm.set_entry_point(nodes[0][0])\n",
    "builder_of_storm.set_finish_point(nodes[-1][0])\n",
    "storm = builder_of_storm.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a815f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "async for step in storm.astream(\n",
    "    {\n",
    "        \"topic\": \"Building better slack bots using LLMs\",\n",
    "    }\n",
    "):\n",
    "    name = next(iter(step))\n",
    "    print(name)\n",
    "    print(\"-- \", str(step[name])[:300])\n",
    "    if END in step:\n",
    "        results = step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bef7021",
   "metadata": {},
   "outputs": [],
   "source": [
    "article = results[END][\"article\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b094067",
   "metadata": {},
   "source": [
    "## Render the Wiki\n",
    "\n",
    "Now we can render the final wiki page!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7750c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "# We will down-header the sections to create less confusion in this notebook\n",
    "Markdown(article.replace(\"\\n#\", \"\\n##\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e24611",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
