{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b912966-4dce-4685-a5b2-a39c5229a0f1",
   "metadata": {},
   "source": [
    "# Storm Research Assistant\n",
    "\n",
    "Reference\n",
    "https://github.com/langchain-ai/langgraph/blob/main/examples/storm/storm.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54832538-aa97-4e40-9713-eaae1e62852a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prereqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60732872-a587-4384-ad96-3ba16facbbe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %pip install -U langchain_community langchain_openai langgraph wikipedia  scikit-learn  langchain_fireworks\n",
    "# # We use one or the other search engine below\n",
    "# %pip install -U tavily-python\n",
    "# %pip install -U duckduckgo-search\n",
    "# # ! apt-get install graphviz graphviz-dev\n",
    "# %pip install pygraphviz\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4abf06bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "example_topic = \"Impact of million-plus token context window language models on RAG\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62647209-990c-4cc1-a1ac-22e46e7ecc31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from storm import *\n",
    "\n",
    "fast_llm, long_context_llm = get_openai_llms(regular_model=\"gpt-3.5-turbo\", long_context_model=\"gpt-3.5-turbo-0125\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94644a48",
   "metadata": {},
   "source": [
    "### Generate Initial Outline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b278d8c-9e34-42ab-9649-bc7b3570bb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "outline_system_prompt = prompts.outline_system_wiki_writer\n",
    "outline_human_prompt = prompts.outline_user_topic_formatinstructions\n",
    "direct_gen_outline_prompt = get_chat_prompt_from_prompt_templates([outline_system_prompt, outline_human_prompt])\n",
    "\n",
    "outline_parser = get_pydantic_parser(pydantic_object=Outline)\n",
    "generate_outline_direct = get_chain_with_outputparser(direct_gen_outline_prompt, fast_llm, outline_parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90ee8329-896b-4085-a1fa-fec0a15937ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "initial_outline = generate_outline_direct.invoke({\"topic\": example_topic})\n",
    "logger.info(initial_outline.as_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5706a1d3",
   "metadata": {},
   "source": [
    "### Expand Topics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27f63ad0-7e07-48ac-85a9-80a53b528c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "related_subjects_prompt = get_chat_prompt_from_prompt_templates([prompts.related_subjects_human_wiki_writer])\n",
    "related_topics_parser = get_pydantic_parser(RelatedSubjects)\n",
    "expand_chain = get_chain_with_outputparser(related_subjects_prompt, fast_llm, related_topics_parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f801936-f6f7-44a0-bc79-4f0132fba79b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RelatedSubjects(topics=['Relevance-Aware Generation (RAG) models', 'Natural language processing (NLP) models', 'BERT (Bidirectional Encoder Representations from Transformers)', 'GPT (Generative Pre-trained Transformer)', 'Transformer (machine learning model architecture)', 'Context window in language models', 'Impact of large language models on information retrieval'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "related_subjects = await expand_chain.ainvoke({\"topic\": example_topic})\n",
    "related_subjects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95cc42a4-45f1-470c-a3f6-20a5661d5b43",
   "metadata": {},
   "source": [
    "## Generate Perspectives\n",
    "\n",
    "From these related subjects, we can select representative Wiki editors as \"subject matter experts\" with distinct backgrounds and affiliations. These will help distribute the search process to encourage a more well-rounded final report.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d79ea824-c561-4949-bbd4-127281f3eeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "gen_perspectives_prompt = get_chat_prompt_from_prompt_templates([prompts.perspective_system_generator, prompts.outline_user_topic_formatinstructions])\n",
    "perspectives_parser = get_pydantic_parser(pydantic_object=Perspectives)\n",
    "\n",
    "gen_perspectives_chain = gen_perspectives_prompt.partial(format_instructions=perspectives_parser.get_format_instructions()) | fast_llm | perspectives_parser\n",
    "\n",
    "\n",
    "\n",
    "@as_runnable\n",
    "async def survey_subjects(topic: str)-> Perspectives:\n",
    "    logger.info(f\"Survey Subjects for Topic: {topic}\")\n",
    "    related_subjects = await expand_chain.ainvoke({\"topic\": topic})\n",
    "    retrieved_docs = await wikipedia_retriever.abatch(\n",
    "        related_subjects.topics, return_exceptions=True\n",
    "    )\n",
    "    all_docs = []\n",
    "    for docs in retrieved_docs:\n",
    "        if isinstance(docs, BaseException):\n",
    "            continue\n",
    "        all_docs.extend(docs)\n",
    "    logger.info(f\"Retrieved {len(all_docs)} docs for Topic: {topic}\")\n",
    "    \n",
    "    formatted = format_docs(all_docs)\n",
    "    return await gen_perspectives_chain.ainvoke({\"examples\": formatted, \"topic\": topic})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1fb5d9c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'editors': [{'affiliation': 'Academic Institution',\n",
       "   'name': 'Dr. Linguistics',\n",
       "   'role': 'Language Model Expert',\n",
       "   'description': 'Dr. Linguistics is a renowned expert in language models, focusing on the impact of million-plus token context window models on the RAG framework. Their research involves analyzing the effectiveness and limitations of these models in relation to the RAG paradigm.'},\n",
       "  {'affiliation': 'Tech Company',\n",
       "   'name': 'AI Developer',\n",
       "   'role': 'Software Engineer',\n",
       "   'description': 'AI Developer is a software engineer working at a tech company specializing in large language models. They are interested in practical implementations and optimizations of million-plus token context window models for RAG applications, aiming to enhance performance and efficiency.'},\n",
       "  {'affiliation': 'OpenAI',\n",
       "   'name': 'Research Scientist',\n",
       "   'role': 'AI Researcher',\n",
       "   'description': 'Research Scientist at OpenAI dedicated to exploring cutting-edge advancements in large language models. They investigate the implications of integrating million-plus token context window models with the RAG framework, striving to push the boundaries of AI capabilities.'},\n",
       "  {'affiliation': 'Media Company',\n",
       "   'name': 'Content Strategist',\n",
       "   'role': 'Communications Specialist',\n",
       "   'description': 'Content Strategist at a media company with a focus on AI technologies. They are interested in how million-plus token context window models impact content creation and information dissemination within the context of RAG, aiming to leverage these models for innovative storytelling.'}]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perspectives = await survey_subjects.ainvoke(example_topic)\n",
    "perspectives.dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35abdf40-80dc-434c-8116-d6b9cbde5572",
   "metadata": {},
   "source": [
    "## Expert Dialog\n",
    "\n",
    "Each wikipedia writer is primed to role-play using the perspectives presented above. It will ask a series of questions of a second \"domain expert\" with access to a search engine. This generate content to generate a refined outline as well as an updated index of reference documents.\n",
    "\n",
    "### Interview State\n",
    "\n",
    "The conversation is cyclic, so we will construct it within its own graph. The State will contain messages, the reference docs, and the editor (with its own \"persona\") to make it easy to parallelize these conversations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0639d2-e8a6-43d4-8e9a-eb5f4051578c",
   "metadata": {},
   "source": [
    "# Dialog Roles\n",
    "\n",
    "The graph will have two participants: the wikipedia editor (generate_question), who asks questions based on its assigned role, and a domain expert (`gen_answer_chain), who uses a search engine to answer the questions as accurately as possible.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d644c46c-4d23-49e1-9093-39b4f6c8c157",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_qn_prompt = get_chat_prompt_from_prompt_templates([prompts.gen_question_system_generator, prompts.generate_messages_placeholder()])\n",
    "\n",
    "\n",
    "def swap_roles(state: InterviewState, name: str) -> InterviewState:\n",
    "\n",
    "    # Normalize name\n",
    "    name = cleanup_name(name)\n",
    "\n",
    "    logger.info(f'Swapping roles for {name}')\n",
    "\n",
    "    converted = []\n",
    "    for message in state[\"messages\"]:\n",
    "        if isinstance(message, AIMessage) and message.name != name:\n",
    "            message = HumanMessage(**message.dict(exclude={\"type\"}))\n",
    "        converted.append(message)\n",
    "    \n",
    "    state['messages'] = converted\n",
    "    \n",
    "    logger.info(f'Converted messages for {name} while swapping roles: {len(converted)} messages')\n",
    "    return state\n",
    "\n",
    "\n",
    "@as_runnable\n",
    "async def generate_question(state: InterviewState) -> InterviewState:\n",
    "    editor: Editor = state[\"editor\"]\n",
    "\n",
    "    name = cleanup_name(editor.name)\n",
    "\n",
    "\n",
    "    logger.info(f'Generating question for {name}')\n",
    "\n",
    "    gn_chain = (\n",
    "        RunnableLambda(swap_roles).bind(name=name)\n",
    "        | gen_qn_prompt.partial(persona=editor.persona)\n",
    "        | fast_llm\n",
    "        | RunnableLambda(tag_with_name).bind(name=name)\n",
    "    )\n",
    "    result:AIMessage = await gn_chain.ainvoke(state)\n",
    "    state[\"messages\"] = ([result])\n",
    "\n",
    "    logger.info(f'Generated question for {name}')\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "29ac2384-a123-467f-bbc6-828be9dc04fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Can you explain how million-plus token context window models impact the RAG framework in your research?', response_metadata={'token_usage': {'completion_tokens': 19, 'prompt_tokens': 224, 'total_tokens': 243}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_b28b39ffa8', 'finish_reason': 'stop', 'logprobs': None}, name='DrLinguistics')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intial_messages = [prompts.initial_question]\n",
    "initial_state: InterviewState = {\n",
    "    \"editor\": perspectives.editors[0],\n",
    "    \"messages\": intial_messages,\n",
    "    \"references\": None\n",
    "}\n",
    "\n",
    "question = await generate_question.ainvoke(initial_state)\n",
    "\n",
    "question[\"messages\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95e50f7-4e2c-4307-853d-2fc8f0b3dd82",
   "metadata": {},
   "source": [
    "### Answer questions\n",
    "\n",
    "The `gen_answer_chain` first generates queries (query expansion) to answer the editor's question, then responds with citations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a7fa969e-920f-487e-b124-6495777a1466",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "gen_queries_prompt = get_chat_prompt_from_prompt_templates([prompts.gen_queries_system_generator, prompts.generate_messages_placeholder()])\n",
    "queries_parser = get_pydantic_parser(Queries)\n",
    "\n",
    "gen_queries_chain = get_chain_with_outputparser(gen_queries_prompt, fast_llm, queries_parser)\n",
    "# gen_queries_prompt.partial(format_instructions=queries_parser.get_format_instructions()) | fast_llm | queries_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b25baa82-da8e-41ec-b4e0-8a24c7cf737d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Queries(queries=['How do million-plus token context window models impact the RAG framework in research?'])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "queries = await gen_queries_chain.ainvoke(\n",
    "    {\"messages\": [HumanMessage(content=question[\"messages\"][0].content)]}\n",
    ")\n",
    "\n",
    "queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "22273c9a-a505-40c3-bd6b-70f74393a61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_answer_prompt = get_chat_prompt_from_prompt_templates([prompts.generate_answer_system_generator, prompts.generate_messages_placeholder()])\n",
    "ac_parser = get_pydantic_parser(pydantic_object=AnswerWithCitations)\n",
    "\n",
    "gen_answer_chain = get_chain_with_outputparser(gen_answer_prompt, fast_llm, ac_parser)\\\n",
    "    .with_config(run_name=\"GenerateAnswer\")\\\n",
    "    # .with_structured_output(AnswerWithCitations, include_raw=True)                                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ca74edcc-a272-4ec1-92fc-756bf0690e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "async def gen_answer(\n",
    "    state: InterviewState,\n",
    "    config: Optional[RunnableConfig] = None,\n",
    "    name: str = \"SubjectMatterExpert\",\n",
    "    max_str_len: int = 15000,\n",
    "):\n",
    "    name = cleanup_name(name)\n",
    "\n",
    "    logger.info(f'START - Generate answers for [{name}]')\n",
    "\n",
    "    swapped_state = swap_roles(state, name)  # Convert all other AI messages\n",
    "    \n",
    "    # Generate search engine queries\n",
    "    queries:Queries = await gen_queries_chain.ainvoke(swapped_state)\n",
    "\n",
    "    logger.info(f\"Got {len(queries.queries)} search engine queries for [{name}] -\\n\\t {queries.queries}\")\n",
    "\n",
    "    # Run search engine\n",
    "    query_results = await search_engine.abatch(\n",
    "        queries.queries, config, return_exceptions=True\n",
    "    )\n",
    "    successful_results = [\n",
    "        res for res in query_results if not isinstance(res, Exception)\n",
    "    ]\n",
    "\n",
    "    all_query_results = {\n",
    "        res[\"url\"]: res[\"content\"] for results in successful_results for res in results\n",
    "    }\n",
    "    \n",
    "    logger.info(f\"Got {len(successful_results)} search engine results for [{name}] - \\n\\t {all_query_results}\")\n",
    "\n",
    "    # We could be more precise about handling max token length if we wanted to here\n",
    "    dumped_successful_results = json.dumps(all_query_results)[:max_str_len]\n",
    "    \n",
    "    logger.info(f\"Dumped {len(dumped_successful_results)} characters for [{name}] - \\n\\t {dumped_successful_results}\")\n",
    "    \n",
    "    # Append Questions from Wikipedia and Answers from the search engine\n",
    "    ai_message_for_queries: AIMessage = get_ai_message(json.dumps(queries.as_dict()))\n",
    "    \n",
    "    tool_results_message = generate_human_message(dumped_successful_results)\n",
    "    \n",
    "    logger.debug(f\"Got {ai_message_for_queries} for [{name}]\")\n",
    "    \n",
    "    # tool_call = queries[\"raw\"].additional_kwargs[\"tool_calls\"][0]\n",
    "    # tool_id = tool_call[\"id\"]\n",
    "\n",
    "    # tool_message = ToolMessage(tool_call_id=tool_id, content=dumped)\n",
    "    \n",
    "\n",
    "    swapped_state[\"messages\"].extend([ai_message_for_queries, tool_results_message])\n",
    "    \n",
    "    # Only update the shared state with the final answer to avoid\n",
    "    # polluting the dialogue history with intermediate messages\n",
    "    try:\n",
    "        generated: AnswerWithCitations = await gen_answer_chain.ainvoke(swapped_state)\n",
    "        \n",
    "        logger.info(f\"Genreted final answer {generated} for [{name}] - \\n\\t {generated.as_str}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error generating answer for [{name}] - {e}\")\n",
    "        generated = AnswerWithCitations(answer=\"\", cited_urls=[])\n",
    "    \n",
    "    cited_urls = set(generated.cited_urls)\n",
    "    \n",
    "    # Save the retrieved information to a the shared state for future reference\n",
    "    cited_references = {k: v for k, v in all_query_results.items() if k in cited_urls}\n",
    "    \n",
    "    formatted_message = AIMessage(name=name, content=generated.as_str)\n",
    "    # Add message to shared state\n",
    "    # state[\"messages\"].append(formatted_message)\n",
    "    state[\"messages\"] = add_messages(state[\"messages\"], [formatted_message])\n",
    "    \n",
    "    # Update references with cited references\n",
    "    state[\"references\"] = update_references(state[\"references\"], cited_references)\n",
    "\n",
    "    logger.info(f'END - generate answer for [{name}]')\n",
    "    \n",
    "    return state\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "934aafd6-7f0d-4a1b-8a52-45b89d8b3742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching DuckDuckGo for [Writing an article on {example_topic}]\n",
      "Got search engine results: 5 for [Writing an article on {example_topic}]\n",
      "Searching DuckDuckGo for [Impact of million-plus token context window models on the RAG framework in research]\n",
      "Got search engine results: 5 for [Impact of million-plus token context window models on the RAG framework in research]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The introduction of large-context models with million-plus token context windows, such as Gemini 1.5 and Claude, has sparked discussions in the AI community about their impact on the Retrieval-Augmented Generation (RAG) framework. These models claim the ability to effectively process a 1 million token context window, which is a significant increase compared to previous models like GPT-4 that supported 128,000 tokens. Some experts predict potential challenges for RAG due to these new large-context models, as they enable handling much larger amounts of information in a single pass, potentially reducing the need for retrieval mechanisms in RAG. This breakthrough in context window size could revolutionize how AI processes and understands vast amounts of data, leading to discussions about the future relevance of frameworks like RAG in the face of such advancements.\\n\\nCitations:\\n\\n[1]: https://medium.com/enterprise-rag/why-gemini-1-5-and-other-large-context-models-are-bullish-for-rag-ce3218930bb4\\n[2]: https://productmann.medium.com/the-implications-of-mega-context-models-by-gemini-and-claude-da42f770018b\\n[3]: https://medium.com/@ritiksharma009999/rag-vs-long-context-llms-a-comparative-analysis-eeedde99dc76\\n[4]: https://dev.to/dawiddahl/the-death-of-rag-what-a-10m-token-breakthrough-means-for-developers-3p24\\n[5]: https://towardsdatascience.com/de-coded-understanding-context-windows-for-transformer-models-cd1baca6427e'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intial_messages = [prompts.initial_question, generate_human_message(question[\"messages\"][0].content)]\n",
    "\n",
    "initial_state: InterviewState = {\n",
    "    \"editor\": perspectives.editors[0],\n",
    "    \"messages\": intial_messages,\n",
    "    \"references\": {}\n",
    "}\n",
    "\n",
    "example_answer = await gen_answer(initial_state)\n",
    "example_answer[\"messages\"][-1].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d779cb51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='So you said you were writing an article on {example_topic}?'),\n",
       " HumanMessage(content='Can you explain how million-plus token context window models impact the RAG framework in your research?'),\n",
       " AIMessage(content='{\"queries\": [\"Writing an article on {example_topic}\", \"Impact of million-plus token context window models on the RAG framework in research\"]}', name='AI'),\n",
       " HumanMessage(content='{\"https://seowind.io/how-to-write-an-article/\": \"Step 2: Select a Topic and an Attractive Heading. Having understood your audience, select a relevant topic based on their interests and questions. Be sure it\\'s one you can competently discuss. When deciding how to start writing an article, ensure it begins with a captivating title.\", \"https://www.semrush.com/blog/article-writing/\": \"An article is a piece of writing that provides information, presents ideas, or discusses a topic in a structured manner. You\\'ll find articles in newspapers, magazines, blogs, websites, and other publications. In fact, you\\'re reading one right now. In this step-by-step guide, we\\'ll focus on how to write an article for content marketing ...\", \"https://www.nytimes.com/2024/01/09/learning/a-guide-for-writing-a-how-to-article.html\": \"Steps 3, 4 and 5 in this lesson will be especially helpful. Created for a contest we ran in 2022, the guide can walk you through preparing and practicing for an interview; keeping the conversation ...\", \"https://leverageedu.com/blog/article-writing/\": \"Step 1: Find your target audience. Step 2: Select a topic and an attractive heading. Step 3: Research is the Key. Step 4: Write and Proofread. Step 5: Add Images and Infographics. Sample of Article Writing. Article Writing on Covid-19 for Students. Article on My Vision of India in the Future. Article Writing Topics.\", \"https://www.careerpower.in/school/english/article-writing\": \"Ans. There are 5 types of articles- Expository Articles, Argumentative Articles, Narrative Articles, Descriptive Articles, and Persuasive Articles. An article is a piece of writing which explicits ideas, thoughts, facts, suggestions, or recommendations based on a particular topic. To know more about the article writing format go through this page.\", \"https://medium.com/enterprise-rag/why-gemini-1-5-and-other-large-context-models-are-bullish-for-rag-ce3218930bb4\": \"The introduction of Gemini 1.5, boasting a 1 million token context window, has sparked discussions in the AI community, with some predicting a negative impact on Retrieval-Augmented Generation (RAG).\", \"https://productmann.medium.com/the-implications-of-mega-context-models-by-gemini-and-claude-da42f770018b\": \"1 day ago. March 5, 2024. It\\'s been an exciting few weeks with the introduction of new models from Google\\'s Gemini on Feb 15th and, now on March 4th, Anthropic\\'s Claude \\\\u2014 both now claiming the ability to effectively process a 1M token context window. After having road rash from experiencing GPT4\\'s all too often \\\\\"confusion ...\", \"https://medium.com/@ritiksharma009999/rag-vs-long-context-llms-a-comparative-analysis-eeedde99dc76\": \"It boasts an unprecedentedly large context window, capable of handling up to 10 million tokens. This is a substantial increase compared to the 128,000 tokens supported by GPT-4.\", \"https://dev.to/dawiddahl/the-death-of-rag-what-a-10m-token-breakthrough-means-for-developers-3p24\": \"In Summary. Google\\'s new break-through announcement could flip the script for developers by allowing AI to digest our entire codebases at once, thanks to its potential 10 million token capacity. This leap forward should make us rethink the need for RAG, as direct, comprehensive code understanding by AI becomes a reality.\", \"https://towardsdatascience.com/de-coded-understanding-context-windows-for-transformer-models-cd1baca6427e\": \"The context window is the maximum sequence length that a transformer can process at a time. With the rise of proprietary LLMs that limit the number of tokens and therefore the prompt size \\\\u2014 as well as the growing interest in techniques such as Retrieval Augmented Generation (RAG)\\\\u2014 understanding the key ideas around context windows and their implications is becoming increasingly important ...\"}'),\n",
       " AIMessage(content='The introduction of large-context models with million-plus token context windows, such as Gemini 1.5 and Claude, has sparked discussions in the AI community about their impact on the Retrieval-Augmented Generation (RAG) framework. These models claim the ability to effectively process a 1 million token context window, which is a significant increase compared to previous models like GPT-4 that supported 128,000 tokens. Some experts predict potential challenges for RAG due to these new large-context models, as they enable handling much larger amounts of information in a single pass, potentially reducing the need for retrieval mechanisms in RAG. This breakthrough in context window size could revolutionize how AI processes and understands vast amounts of data, leading to discussions about the future relevance of frameworks like RAG in the face of such advancements.\\n\\nCitations:\\n\\n[1]: https://medium.com/enterprise-rag/why-gemini-1-5-and-other-large-context-models-are-bullish-for-rag-ce3218930bb4\\n[2]: https://productmann.medium.com/the-implications-of-mega-context-models-by-gemini-and-claude-da42f770018b\\n[3]: https://medium.com/@ritiksharma009999/rag-vs-long-context-llms-a-comparative-analysis-eeedde99dc76\\n[4]: https://dev.to/dawiddahl/the-death-of-rag-what-a-10m-token-breakthrough-means-for-developers-3p24\\n[5]: https://towardsdatascience.com/de-coded-understanding-context-windows-for-transformer-models-cd1baca6427e', name='SubjectMatterExpert')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_answer[\"messages\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998671bf-958d-44c0-8421-523a71bea01a",
   "metadata": {},
   "source": [
    "# Construct the Interview Graph\n",
    "\n",
    "Now that we've defined the editor and domain expert, we can compose them in a graph.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c80347f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_num_turns = 7\n",
    "\n",
    "def route_messages(state: InterviewState, name: str = \"SubjectMatterExpert\"):\n",
    "\n",
    "    name = cleanup_name(name)\n",
    "\n",
    "    logger.info(f'Routing messages for [{name}]')\n",
    "\n",
    "    messages = state[\"messages\"]\n",
    "    num_responses = len(\n",
    "        [m for m in messages if isinstance(m, AIMessage) and m.name == name]\n",
    "    )\n",
    "\n",
    "    if num_responses >= MAX_INTERVIEW_QUESTIONS:\n",
    "        return END\n",
    "    \n",
    "    last_question = messages[-2]\n",
    "    if last_question.content.endswith(\"Thank you so much for your help!\"):\n",
    "        return END\n",
    "    \n",
    "    logger.info(f'Continue asking question for [{name}] as this is not the last end of the conversation')\n",
    "    return \"ask_question\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4800f958-00e0-4913-a246-c34dc3f0a00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "builder = StateGraph(InterviewState)\n",
    "\n",
    "builder.add_node(\"ask_question\", generate_question)\n",
    "builder.add_node(\"answer_question\", gen_answer)\n",
    "builder.add_conditional_edges(\"answer_question\", route_messages)\n",
    "builder.add_edge(\"ask_question\", \"answer_question\")\n",
    "\n",
    "builder.set_entry_point(\"ask_question\")\n",
    "interview_graph = builder.compile().with_config(run_name=\"Conduct Interviews\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c405fc-5b1c-44f5-b860-a10fe0d6616d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "# comment out if you have not installed pygraphviz\n",
    "# Image(interview_graph.get_graph().draw_png())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "96d47607-6c0b-493a-aebc-48356d0e0302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching DuckDuckGo for [Advantages of using million-plus token context window language models within the RAG framework]\n",
      "Got search engine results: 5 for [Advantages of using million-plus token context window language models within the RAG framework]\n",
      "Searching DuckDuckGo for [Disadvantages of using million-plus token context window language models within the RAG framework]\n",
      "Got search engine results: 5 for [Disadvantages of using million-plus token context window language models within the RAG framework]\n",
      "Searching DuckDuckGo for [Impact of million-plus token context window language models on the RAG framework]\n",
      "Got search engine results: 5 for [Impact of million-plus token context window language models on the RAG framework]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-30 22:57:40,492 [MainThread  ] [ERROR]  Error generating answer for [SubjectMatterExpert] - Invalid json output: The impact of million-plus token context window language models on the RAG framework has sparked discussions in the AI community. While some predict a negative impact on Retrieval-Augmented Generation (RAG) due to the introduction of models like Gemini 1.5 and Anthropic's Claude boasting a 1 million token context window, there are contrasting views on the benefits of retrieval for long context Language Learning Models (LLMs). The cost implications of using RAG with different context window sizes have also been analyzed, with findings indicating varying costs associated with different context window sizes, such as $0.0004/1k tokens for a 128k context window. Additionally, there have been advancements in context window sizes, with models capable of handling up to 10 million tokens, a significant increase compared to previous models like GPT-4 that supported 128,000 tokens.\n",
      "\n",
      "Citations:\n",
      "\n",
      "[1]: https://medium.com/enterprise-rag/why-gemini-1-5-and-other-large-context-models-are-bullish-for-rag-ce3218930bb4\n",
      "[2]: https://productmann.medium.com/the-implications-of-mega-context-models-by-gemini-and-claude-da42f770018b\n",
      "[3]: https://www.llamaindex.ai/blog/nvidia-research-rag-with-long-context-llms-7d94d40090c4\n",
      "[4]: https://ai88.substack.com/p/rag-vs-context-window-in-gpt4-accuracy-cost\n",
      "[5]: https://medium.com/@ritiksharma009999/rag-vs-long-context-llms-a-comparative-analysis-eeedde99dc76\n"
     ]
    }
   ],
   "source": [
    "\n",
    "final_step = None\n",
    "\n",
    "initial_state = {\n",
    "    \"editor\": perspectives.editors[0],\n",
    "    \"messages\": [\n",
    "        AIMessage(\n",
    "            content=f\"So you said you were writing an article on {example_topic}?\",\n",
    "            name=\"SubjectMatterExpert\",\n",
    "        )\n",
    "    ],\n",
    "}\n",
    "async for step in interview_graph.astream(initial_state):\n",
    "    name = next(iter(step))\n",
    "    logger.info(f\"Processing step: {name}\")\n",
    "    logger.debug(\"-- \", str(step[name][\"messages\"])[:300])\n",
    "    if END in step:\n",
    "        final_step = step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8c7a97ec-09a1-4873-b559-275526971a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_state = next(iter(final_step.values()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f22e50cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [AIMessage(content='So you said you were writing an article on Impact of million-plus token context window language models on RAG?', name='SubjectMatterExpert'),\n",
       "  AIMessage(content='Can you explain the specific advantages and disadvantages of using million-plus token context window language models within the RAG framework?', response_metadata={'token_usage': {'completion_tokens': 23, 'prompt_tokens': 238, 'total_tokens': 261}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_3bc1b5746c', 'finish_reason': 'stop', 'logprobs': None}, name='DrLinguistics'),\n",
       "  AIMessage(content='So you said you were writing an article on Impact of million-plus token context window language models on RAG?', name='SubjectMatterExpert'),\n",
       "  HumanMessage(content='Can you explain the specific advantages and disadvantages of using million-plus token context window language models within the RAG framework?', response_metadata={'token_usage': {'completion_tokens': 23, 'prompt_tokens': 238, 'total_tokens': 261}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_3bc1b5746c', 'finish_reason': 'stop', 'logprobs': None}, name='DrLinguistics'),\n",
       "  AIMessage(content='{\"queries\": [\"Advantages of using million-plus token context window language models within the RAG framework\", \"Disadvantages of using million-plus token context window language models within the RAG framework\"]}', name='AI'),\n",
       "  HumanMessage(content='{\"https://medium.com/enterprise-rag/why-gemini-1-5-and-other-large-context-models-are-bullish-for-rag-ce3218930bb4\": \"The introduction of Gemini 1.5, boasting a 1 million token context window, has sparked discussions in the AI community, with some predicting a negative impact on Retrieval-Augmented Generation (RAG).\", \"https://www.linkedin.com/pulse/leveraging-larger-context-windows-rag-benefits-cost-pawel-sobczak-epvif\": \"Large Language Models (LLMs) are constantly evolving, increasing for example size of context window. Retrieval-Augmented Generation (RAG) is a powerful technique that enhances LLM performance by ...\", \"https://www.linkedin.com/pulse/facing-extinction-rag-losing-ground-enhanced-context-windows-danter-w9i9f\": \"Context Window. RAG: Addressing the limitations of fixed context windows, RAG combines the power of language models with information retrieval techniques. By converting text to vector embeddings ...\", \"https://www.allabtai.com/rag-vs-context-window/\": \"Comparative Analysis. To grasp the practical differences, consider processing speed and cost. While RAG provides a cost-effective solution by fetching only relevant tokens, context windows, especially with advancements in hardware, promise rapid processing even for large datasets. However, the cost can vary significantly based on the amount of ...\", \"https://blog.google/technology/ai/long-context-window-ai-models/\": \"That\\'s where long context windows can help. Previously, Gemini could process up to 32,000 tokens at once, but 1.5 Pro \\\\u2014 the first 1.5 model we\\'re releasing for early testing \\\\u2014 has a context window of up to 1 million tokens \\\\u2014 the longest context window of any large-scale foundation model to date. In fact, we\\'ve even successfully ...\", \"https://ai.plainenglish.io/context-window-size-and-language-model-performance-balancing-act-2ae2964e3ec1\": \"Instead, they base their responses on the latest information within their context window, rather than the original request. In contrast, with a larger context, Language Learning Models (LLMs) exhibit the ability to \\'remember\\' more effectively. This enables them to provide answers that are better aligned with the ongoing conversation or task ...\", \"https://medium.com/@jm_51428/long-context-window-models-vs-rag-a73c35a763f2\": \"Long context window models vs. RAG. Jm. \\\\u00b7. Follow. 7 min read. \\\\u00b7. Feb 5, 2024. What is the best way to give a large language model the context it needs to answer a specific query or accomplish a ...\"}'),\n",
       "  AIMessage(content=\"Using million-plus token context window language models within the RAG framework can have both advantages and disadvantages. Advantages include the potential for enhanced performance of Language Learning Models (LLMs) by allowing them to 'remember' more effectively and provide better-aligned answers. Additionally, larger context windows can help in processing longer sequences of text efficiently. On the other hand, some concerns have been raised about the potential negative impact of million-plus token context windows on Retrieval-Augmented Generation (RAG). There are discussions in the AI community regarding the balance between the benefits and costs of leveraging larger context windows within the RAG framework.\\n\\nCitations:\\n\\n[1]: https://ai.plainenglish.io/context-window-size-and-language-model-performance-balancing-act-2ae2964e3ec1\\n[2]: https://medium.com/enterprise-rag/why-gemini-1-5-and-other-large-context-models-are-bullish-for-rag-ce3218930bb4\\n[3]: https://www.linkedin.com/pulse/leveraging-larger-context-windows-rag-benefits-cost-pawel-sobczak-epvif\\n[4]: https://www.allabtai.com/rag-vs-context-window/\\n[5]: https://blog.google/technology/ai/long-context-window-ai-models/\\n[6]: https://medium.com/@jm_51428/long-context-window-models-vs-rag-a73c35a763f2\", name='SubjectMatterExpert'),\n",
       "  AIMessage(content='Thank you for providing that information! If you have any more insights or details regarding the impact of million-plus token context window language models on the RAG framework, please share them.', response_metadata={'token_usage': {'completion_tokens': 36, 'prompt_tokens': 1287, 'total_tokens': 1323}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_3bc1b5746c', 'finish_reason': 'stop', 'logprobs': None}, name='DrLinguistics'),\n",
       "  AIMessage(content='So you said you were writing an article on Impact of million-plus token context window language models on RAG?', name='SubjectMatterExpert'),\n",
       "  HumanMessage(content='Can you explain the specific advantages and disadvantages of using million-plus token context window language models within the RAG framework?', response_metadata={'token_usage': {'completion_tokens': 23, 'prompt_tokens': 238, 'total_tokens': 261}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_3bc1b5746c', 'finish_reason': 'stop', 'logprobs': None}, name='DrLinguistics'),\n",
       "  AIMessage(content='So you said you were writing an article on Impact of million-plus token context window language models on RAG?', name='SubjectMatterExpert'),\n",
       "  HumanMessage(content='Can you explain the specific advantages and disadvantages of using million-plus token context window language models within the RAG framework?', response_metadata={'token_usage': {'completion_tokens': 23, 'prompt_tokens': 238, 'total_tokens': 261}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_3bc1b5746c', 'finish_reason': 'stop', 'logprobs': None}, name='DrLinguistics'),\n",
       "  HumanMessage(content='{\"queries\": [\"Advantages of using million-plus token context window language models within the RAG framework\", \"Disadvantages of using million-plus token context window language models within the RAG framework\"]}', name='AI'),\n",
       "  HumanMessage(content='{\"https://medium.com/enterprise-rag/why-gemini-1-5-and-other-large-context-models-are-bullish-for-rag-ce3218930bb4\": \"The introduction of Gemini 1.5, boasting a 1 million token context window, has sparked discussions in the AI community, with some predicting a negative impact on Retrieval-Augmented Generation (RAG).\", \"https://www.linkedin.com/pulse/leveraging-larger-context-windows-rag-benefits-cost-pawel-sobczak-epvif\": \"Large Language Models (LLMs) are constantly evolving, increasing for example size of context window. Retrieval-Augmented Generation (RAG) is a powerful technique that enhances LLM performance by ...\", \"https://www.linkedin.com/pulse/facing-extinction-rag-losing-ground-enhanced-context-windows-danter-w9i9f\": \"Context Window. RAG: Addressing the limitations of fixed context windows, RAG combines the power of language models with information retrieval techniques. By converting text to vector embeddings ...\", \"https://www.allabtai.com/rag-vs-context-window/\": \"Comparative Analysis. To grasp the practical differences, consider processing speed and cost. While RAG provides a cost-effective solution by fetching only relevant tokens, context windows, especially with advancements in hardware, promise rapid processing even for large datasets. However, the cost can vary significantly based on the amount of ...\", \"https://blog.google/technology/ai/long-context-window-ai-models/\": \"That\\'s where long context windows can help. Previously, Gemini could process up to 32,000 tokens at once, but 1.5 Pro \\\\u2014 the first 1.5 model we\\'re releasing for early testing \\\\u2014 has a context window of up to 1 million tokens \\\\u2014 the longest context window of any large-scale foundation model to date. In fact, we\\'ve even successfully ...\", \"https://ai.plainenglish.io/context-window-size-and-language-model-performance-balancing-act-2ae2964e3ec1\": \"Instead, they base their responses on the latest information within their context window, rather than the original request. In contrast, with a larger context, Language Learning Models (LLMs) exhibit the ability to \\'remember\\' more effectively. This enables them to provide answers that are better aligned with the ongoing conversation or task ...\", \"https://medium.com/@jm_51428/long-context-window-models-vs-rag-a73c35a763f2\": \"Long context window models vs. RAG. Jm. \\\\u00b7. Follow. 7 min read. \\\\u00b7. Feb 5, 2024. What is the best way to give a large language model the context it needs to answer a specific query or accomplish a ...\"}'),\n",
       "  AIMessage(content=\"Using million-plus token context window language models within the RAG framework can have both advantages and disadvantages. Advantages include the potential for enhanced performance of Language Learning Models (LLMs) by allowing them to 'remember' more effectively and provide better-aligned answers. Additionally, larger context windows can help in processing longer sequences of text efficiently. On the other hand, some concerns have been raised about the potential negative impact of million-plus token context windows on Retrieval-Augmented Generation (RAG). There are discussions in the AI community regarding the balance between the benefits and costs of leveraging larger context windows within the RAG framework.\\n\\nCitations:\\n\\n[1]: https://ai.plainenglish.io/context-window-size-and-language-model-performance-balancing-act-2ae2964e3ec1\\n[2]: https://medium.com/enterprise-rag/why-gemini-1-5-and-other-large-context-models-are-bullish-for-rag-ce3218930bb4\\n[3]: https://www.linkedin.com/pulse/leveraging-larger-context-windows-rag-benefits-cost-pawel-sobczak-epvif\\n[4]: https://www.allabtai.com/rag-vs-context-window/\\n[5]: https://blog.google/technology/ai/long-context-window-ai-models/\\n[6]: https://medium.com/@jm_51428/long-context-window-models-vs-rag-a73c35a763f2\", name='SubjectMatterExpert'),\n",
       "  HumanMessage(content='Thank you for providing that information! If you have any more insights or details regarding the impact of million-plus token context window language models on the RAG framework, please share them.', response_metadata={'token_usage': {'completion_tokens': 36, 'prompt_tokens': 1287, 'total_tokens': 1323}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_3bc1b5746c', 'finish_reason': 'stop', 'logprobs': None}, name='DrLinguistics'),\n",
       "  AIMessage(content='{\"queries\": [\"Impact of million-plus token context window language models on the RAG framework\"]}', name='AI'),\n",
       "  HumanMessage(content='{\"https://medium.com/enterprise-rag/why-gemini-1-5-and-other-large-context-models-are-bullish-for-rag-ce3218930bb4\": \"The introduction of Gemini 1.5, boasting a 1 million token context window, has sparked discussions in the AI community, with some predicting a negative impact on Retrieval-Augmented Generation (RAG).\", \"https://productmann.medium.com/the-implications-of-mega-context-models-by-gemini-and-claude-da42f770018b\": \"1 day ago. March 5, 2024. It\\'s been an exciting few weeks with the introduction of new models from Google\\'s Gemini on Feb 15th and, now on March 4th, Anthropic\\'s Claude \\\\u2014 both now claiming the ability to effectively process a 1M token context window. After having road rash from experiencing GPT4\\'s all too often \\\\\"confusion ...\", \"https://www.llamaindex.ai/blog/nvidia-research-rag-with-long-context-llms-7d94d40090c4\": \"Their work explored the impact of retrieval on long context LLMs, evaluating models like GPT-3.5-Turbo-16k and Llama2-7B-chat-4k. However, their findings diverge from NVIDIA\\'s in crucial ways. Bai et al. discerned that retrieval was beneficial only for the Llama2-7B-chat-4k with a 4K context window, but not for extended context models ...\", \"https://ai88.substack.com/p/rag-vs-context-window-in-gpt4-accuracy-cost\": \"But again, RAG also incurs a ~fixed LLM agent loop cost. For 128k context window, the average total cost came to roughly $0.0004/ 1k-tokens, or 4% of the GPT-4-Turbo cost. The LlamaIndex cost was slightly cheaper, but comparable at $0.00028/1k tokens (due to a less sophisticated agentic loop).\", \"https://medium.com/@ritiksharma009999/rag-vs-long-context-llms-a-comparative-analysis-eeedde99dc76\": \"It boasts an unprecedentedly large context window, capable of handling up to 10 million tokens. This is a substantial increase compared to the 128,000 tokens supported by GPT-4.\"}'),\n",
       "  AIMessage(content='\\n\\nCitations:\\n\\n', name='SubjectMatterExpert')],\n",
       " 'references': {'https://medium.com/enterprise-rag/why-gemini-1-5-and-other-large-context-models-are-bullish-for-rag-ce3218930bb4': 'The introduction of Gemini 1.5, boasting a 1 million token context window, has sparked discussions in the AI community, with some predicting a negative impact on Retrieval-Augmented Generation (RAG).',\n",
       "  'https://www.linkedin.com/pulse/leveraging-larger-context-windows-rag-benefits-cost-pawel-sobczak-epvif': 'Large Language Models (LLMs) are constantly evolving, increasing for example size of context window. Retrieval-Augmented Generation (RAG) is a powerful technique that enhances LLM performance by ...',\n",
       "  'https://www.allabtai.com/rag-vs-context-window/': 'Comparative Analysis. To grasp the practical differences, consider processing speed and cost. While RAG provides a cost-effective solution by fetching only relevant tokens, context windows, especially with advancements in hardware, promise rapid processing even for large datasets. However, the cost can vary significantly based on the amount of ...',\n",
       "  'https://blog.google/technology/ai/long-context-window-ai-models/': \"That's where long context windows can help. Previously, Gemini could process up to 32,000 tokens at once, but 1.5 Pro — the first 1.5 model we're releasing for early testing — has a context window of up to 1 million tokens — the longest context window of any large-scale foundation model to date. In fact, we've even successfully ...\",\n",
       "  'https://ai.plainenglish.io/context-window-size-and-language-model-performance-balancing-act-2ae2964e3ec1': \"Instead, they base their responses on the latest information within their context window, rather than the original request. In contrast, with a larger context, Language Learning Models (LLMs) exhibit the ability to 'remember' more effectively. This enables them to provide answers that are better aligned with the ongoing conversation or task ...\",\n",
       "  'https://medium.com/@jm_51428/long-context-window-models-vs-rag-a73c35a763f2': 'Long context window models vs. RAG. Jm. ·. Follow. 7 min read. ·. Feb 5, 2024. What is the best way to give a large language model the context it needs to answer a specific query or accomplish a ...'},\n",
       " 'editor': Editor(affiliation='Academic Institution', name='Dr. Linguistics', role='Language Model Expert', description='Dr. Linguistics is a renowned expert in language models, focusing on the impact of million-plus token context window models on the RAG framework. Their research involves analyzing the effectiveness and limitations of these models in relation to the RAG paradigm.')}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e14dcb",
   "metadata": {},
   "source": [
    "## Refine Outline\n",
    "\n",
    "At this point in STORM, we've conducted a large amount of research from different perspectives. It's time to refine the original outline based on these investigations. Below, create a chain using the LLM with a long context window to update the original outline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c284eb72-3856-406d-8582-5a1c92fd292e",
   "metadata": {},
   "outputs": [],
   "source": [
    "refine_outline_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"You are a Wikipedia writer. You have gathered information from experts and search engines. Now, you are refining the outline of the Wikipedia page. \\\n",
    "You need to make sure that the outline is comprehensive and specific. \\\n",
    "Topic you are writing about: {topic} \n",
    "\n",
    "Old outline:\n",
    "\n",
    "{old_outline}\n",
    "\"\"\",\n",
    "        ),\n",
    "        (\n",
    "            \"user\",\n",
    "            \"Refine the outline based on your conversations with subject-matter experts:\\n\\nConversations:\\n\\n{conversations}\\n\\n{format_instructions}\\n\\nWrite the refined Wikipedia outline:\",\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# Using turbo preview since the context can get quite long\n",
    "refine_outline_chain = refine_outline_prompt.partial(format_instructions=outline_parser.get_format_instructions()) | long_context_llm | outline_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19e58c5-086f-49ba-b921-791669d04b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "refined_outline = refine_outline_chain.invoke(\n",
    "    {\n",
    "        \"topic\": example_topic,\n",
    "        \"old_outline\": initial_outline.as_str,\n",
    "        \"conversations\": \"\\n\\n\".join(\n",
    "            f\"### {m.name}\\n\\n{m.content}\" for m in final_state[\"messages\"]\n",
    "        ),\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c397dc5-e614-4a7f-9f78-67dffc9b8c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(refined_outline.as_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8b76b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generate Article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563b2a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "from langchain_community.vectorstores import SKLearnVectorStore\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "reference_docs = [\n",
    "    Document(page_content=v, metadata={\"source\": k})\n",
    "    for k, v in final_state[\"references\"].items()\n",
    "]\n",
    "\n",
    "logger.info(f\"Number of references: {len(reference_docs)}\")\n",
    "\n",
    "# This really doesn't need to be a vectorstore for this size of data.\n",
    "# It could just be a numpy matrix. Or you could store documents\n",
    "# across requests if you want.\n",
    "vectorstore = SKLearnVectorStore.from_documents(\n",
    "    reference_docs,\n",
    "    embedding=embeddings,\n",
    ")\n",
    "retriever = vectorstore.as_retriever(k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever.invoke(\"What's a long context LLM anyway?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e708d31a",
   "metadata": {},
   "source": [
    "#### Generate Sections\n",
    "\n",
    "Now you can generate the sections using the indexed docs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524f4c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SubSection(BaseModel):\n",
    "    subsection_title: str = Field(..., title=\"Title of the subsection\")\n",
    "    content: str = Field(\n",
    "        ...,\n",
    "        title=\"Full content of the subsection. Include [#] citations to the cited sources where relevant.\",\n",
    "    )\n",
    "\n",
    "    @property\n",
    "    def as_str(self) -> str:\n",
    "        return f\"### {self.subsection_title}\\n\\n{self.content}\".strip()\n",
    "\n",
    "\n",
    "class WikiSection(BaseModel):\n",
    "    section_title: str = Field(..., title=\"Title of the section\")\n",
    "    content: str = Field(..., title=\"Full content of the section\")\n",
    "    subsections: Optional[List[Subsection]] = Field(\n",
    "        default=None,\n",
    "        title=\"Titles and descriptions for each subsection of the Wikipedia page.\",\n",
    "    )\n",
    "    citations: List[str] = Field(default_factory=list)\n",
    "\n",
    "    @property\n",
    "    def as_str(self) -> str:\n",
    "        subsections = \"\\n\\n\".join(\n",
    "            subsection.as_str for subsection in self.subsections or []\n",
    "        )\n",
    "        citations = \"\\n\".join([f\" [{i}] {cit}\" for i, cit in enumerate(self.citations)])\n",
    "        return (\n",
    "            f\"## {self.section_title}\\n\\n{self.content}\\n\\n{subsections}\".strip()\n",
    "            + f\"\\n\\n{citations}\".strip()\n",
    "        )\n",
    "\n",
    "\n",
    "section_writer_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are an expert Wikipedia writer. Complete your assigned WikiSection from the following outline:\\n\\n\"\n",
    "            \"{outline}\\n\\nCite your sources, using the following references:\\n\\n<Documents>\\n{docs}\\n<Documents>\",\n",
    "        ),\n",
    "        (\"user\", \"Write the full WikiSection for the {section} section.\\n{format_instructions}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "async def retrieve(inputs: dict):\n",
    "    docs = await retriever.ainvoke(inputs[\"topic\"] + \": \" + inputs[\"section\"])\n",
    "    formatted = \"\\n\".join(\n",
    "        [\n",
    "            f'<Document href=\"{doc.metadata[\"source\"]}\"/>\\n{doc.page_content}\\n</Document>'\n",
    "            for doc in docs\n",
    "        ]\n",
    "    )\n",
    "    return {\"docs\": formatted, **inputs}\n",
    "\n",
    "wiki_parser = PydanticOutputParser(pydantic_object=WikiSection)\n",
    "\n",
    "section_writer = (\n",
    "    retrieve\n",
    "    | section_writer_prompt.partial(format_instructions=wiki_parser.get_format_instructions())\n",
    "    | long_context_llm\n",
    "    | wiki_parser\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03723e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "section = await section_writer.ainvoke(\n",
    "    {\n",
    "        \"outline\": refined_outline.as_str,\n",
    "        \"section\": refined_outline.sections[1].section_title,\n",
    "        \"topic\": example_topic,\n",
    "    }\n",
    ")\n",
    "print(section.as_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0afd728d",
   "metadata": {},
   "source": [
    "#### Generate final article\n",
    "\n",
    "Now we can rewrite the draft to appropriately group all the citations and maintain a consistent voice.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05089f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "writer_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are an expert Wikipedia author. Write the complete wiki article on {topic} using the following section drafts:\\n\\n\"\n",
    "            \"{draft}\\n\\nStrictly follow Wikipedia format guidelines.\",\n",
    "        ),\n",
    "        (\n",
    "            \"user\",\n",
    "            'Write the complete Wiki article using markdown format. Organize citations using footnotes like \"[1]\",\"\" avoiding duplicates in the footer. Include URLs in the footer.',\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "writer = writer_prompt | long_context_llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6e6e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for tok in writer.stream({\"topic\": example_topic, \"draft\": section.as_str}):\n",
    "    print(tok, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6ab734",
   "metadata": {},
   "source": [
    "## Final Flow\n",
    "\n",
    "Now it's time to string everything together. We will have 6 main stages in sequence:\n",
    ".\n",
    "\n",
    "1. Generate the initial outline + perspectives\n",
    "2. Batch converse with each perspective to expand the content for the article\n",
    "3. Refine the outline based on the conversations\n",
    "4. Index the reference docs from the conversations\n",
    "5. Write the individual sections of the article\n",
    "6. Write the final wiki\n",
    "\n",
    "The state tracks the outputs of each stage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e775ba8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResearchState(TypedDict):\n",
    "    topic: str\n",
    "    outline: Outline\n",
    "    editors: List[Editor]\n",
    "    interview_results: List[InterviewState]\n",
    "    # The final sections output\n",
    "    sections: List[WikiSection]\n",
    "    article: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1854d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "\n",
    "async def initialize_research(state: ResearchState):\n",
    "    topic = state[\"topic\"]\n",
    "    coros = (\n",
    "        generate_outline_direct.ainvoke({\"topic\": topic}),\n",
    "        survey_subjects.ainvoke(topic),\n",
    "    )\n",
    "    results = await asyncio.gather(*coros)\n",
    "    return {\n",
    "        **state,\n",
    "        \"outline\": results[0],\n",
    "        \"editors\": results[1].editors,\n",
    "    }\n",
    "\n",
    "\n",
    "async def conduct_interviews(state: ResearchState):\n",
    "    topic = state[\"topic\"]\n",
    "    initial_states = [\n",
    "        {\n",
    "            \"editor\": editor,\n",
    "            \"messages\": [\n",
    "                AIMessage(\n",
    "                    content=f\"So you said you were writing an article on {topic}?\",\n",
    "                    name=\"SubjectMatterExpert\",\n",
    "                )\n",
    "            ],\n",
    "        }\n",
    "        for editor in state[\"editors\"]\n",
    "    ]\n",
    "    # We call in to the sub-graph here to parallelize the interviews\n",
    "    interview_results = await interview_graph.abatch(initial_states)\n",
    "\n",
    "    return {\n",
    "        **state,\n",
    "        \"interview_results\": interview_results,\n",
    "    }\n",
    "\n",
    "\n",
    "def format_conversation(interview_state):\n",
    "    messages = interview_state[\"messages\"]\n",
    "    convo = \"\\n\".join(f\"{m.name}: {m.content}\" for m in messages)\n",
    "    return f'Conversation with {interview_state[\"editor\"].name}\\n\\n' + convo\n",
    "\n",
    "\n",
    "async def refine_outline(state: ResearchState):\n",
    "    convos = \"\\n\\n\".join(\n",
    "        [\n",
    "            format_conversation(interview_state)\n",
    "            for interview_state in state[\"interview_results\"]\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    updated_outline = await refine_outline_chain.ainvoke(\n",
    "        {\n",
    "            \"topic\": state[\"topic\"],\n",
    "            \"old_outline\": state[\"outline\"].as_str,\n",
    "            \"conversations\": convos,\n",
    "        }\n",
    "    )\n",
    "    return {**state, \"outline\": updated_outline}\n",
    "\n",
    "\n",
    "async def index_references(state: ResearchState):\n",
    "    all_docs = []\n",
    "    for interview_state in state[\"interview_results\"]:\n",
    "        reference_docs = [\n",
    "            Document(page_content=v, metadata={\"source\": k})\n",
    "            for k, v in interview_state[\"references\"].items()\n",
    "        ]\n",
    "        all_docs.extend(reference_docs)\n",
    "    await vectorstore.aadd_documents(all_docs)\n",
    "    return state\n",
    "\n",
    "\n",
    "async def write_sections(state: ResearchState):\n",
    "    outline = state[\"outline\"]\n",
    "    sections = await section_writer.abatch(\n",
    "        [\n",
    "            {\n",
    "                \"outline\": refined_outline.as_str,\n",
    "                \"section\": section.section_title,\n",
    "                \"topic\": state[\"topic\"],\n",
    "            }\n",
    "            for section in outline.sections\n",
    "        ]\n",
    "    )\n",
    "    return {\n",
    "        **state,\n",
    "        \"sections\": sections,\n",
    "    }\n",
    "\n",
    "\n",
    "async def write_article(state: ResearchState):\n",
    "    topic = state[\"topic\"]\n",
    "    sections = state[\"sections\"]\n",
    "    draft = \"\\n\\n\".join([section.as_str for section in sections])\n",
    "    article = await writer.ainvoke({\"topic\": topic, \"draft\": draft})\n",
    "    return {\n",
    "        **state,\n",
    "        \"article\": article,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87881e3",
   "metadata": {},
   "source": [
    "#### Create the graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3b4be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "builder_of_storm = StateGraph(ResearchState)\n",
    "\n",
    "nodes = [\n",
    "    (\"init_research\", initialize_research),\n",
    "    (\"conduct_interviews\", conduct_interviews),\n",
    "    (\"refine_outline\", refine_outline),\n",
    "    (\"index_references\", index_references),\n",
    "    (\"write_sections\", write_sections),\n",
    "    (\"write_article\", write_article),\n",
    "]\n",
    "for i in range(len(nodes)):\n",
    "    name, node = nodes[i]\n",
    "    builder_of_storm.add_node(name, node)\n",
    "    if i > 0:\n",
    "        builder_of_storm.add_edge(nodes[i - 1][0], name)\n",
    "\n",
    "builder_of_storm.set_entry_point(nodes[0][0])\n",
    "builder_of_storm.set_finish_point(nodes[-1][0])\n",
    "storm = builder_of_storm.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a815f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "async for step in storm.astream(\n",
    "    {\n",
    "        \"topic\": \"Building better slack bots using LLMs\",\n",
    "    }\n",
    "):\n",
    "    name = next(iter(step))\n",
    "    print(name)\n",
    "    logger.info(\"-- \", str(step[name])[:300])\n",
    "    if END in step:\n",
    "        results = step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bef7021",
   "metadata": {},
   "outputs": [],
   "source": [
    "article = results[END][\"article\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b094067",
   "metadata": {},
   "source": [
    "## Render the Wiki\n",
    "\n",
    "Now we can render the final wiki page!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7750c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "# We will down-header the sections to create less confusion in this notebook\n",
    "Markdown(article.replace(\"\\n#\", \"\\n##\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e24611",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
