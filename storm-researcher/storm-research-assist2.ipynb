{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b912966-4dce-4685-a5b2-a39c5229a0f1",
   "metadata": {},
   "source": [
    "# Storm Research Assistant\n",
    "\n",
    "Reference\n",
    "https://github.com/langchain-ai/langgraph/blob/main/examples/storm/storm.ipynb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54832538-aa97-4e40-9713-eaae1e62852a",
   "metadata": {},
   "source": [
    "## Prereqs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60732872-a587-4384-ad96-3ba16facbbe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %pip install -U langchain_community langchain_openai langgraph wikipedia  scikit-learn  langchain_fireworks langchain_anthropic\n",
    "# # We use one or the other search engine below\n",
    "# %pip install -U tavily-python, playwright\n",
    "# %pip install -U duckduckgo-search\n",
    "# # ! apt-get install graphviz graphviz-dev\n",
    "# %pip install pygraphviz\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62647209-990c-4cc1-a1ac-22e46e7ecc31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert_load_from_file: gguf version     = 2\n",
      "bert_load_from_file: gguf alignment   = 32\n",
      "bert_load_from_file: gguf data offset = 695552\n",
      "bert_load_from_file: model name           = BERT\n",
      "bert_load_from_file: model architecture   = bert\n",
      "bert_load_from_file: model file type      = 1\n",
      "bert_load_from_file: bert tokenizer vocab = 30522\n"
     ]
    }
   ],
   "source": [
    "from storm import *\n",
    "\n",
    "# LLMS and Embeddings are provided at the top level\n",
    "\n",
    "fast_llm, _ = get_openai_llms(regular_model=\"gpt-3.5-turbo\", long_context_model=\"gpt-3.5-turbo-0125\")\n",
    "_, long_context_llm = get_anthropic_llms()\n",
    "\n",
    "# ollama_model = 'mistral:7b-instruct-q4_K_M'\n",
    "# fast_llm, long_context_llm = get_ollama_llms(regular_model=ollama_model, long_context_model=ollama_model)\n",
    "\n",
    "\n",
    "embeddings = get_gpt4all_embeddings()\n",
    "\n",
    "_ = None\n",
    "\n",
    "\n",
    "\n",
    "example_topic = \"Impact of THE Red Cross Church IN Zimbabwe early history\"\n",
    "\n",
    "interview_graph = StormInterviewGraph(fast_llm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94644a48",
   "metadata": {},
   "source": [
    "### Generate Initial Outline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b278d8c-9e34-42ab-9649-bc7b3570bb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chains and interview graph\n",
    "\n",
    "# generate_outline_direct = get_chain_outline(fast_llm)\n",
    "# expand_chain = get_chain_expand_related_topics(fast_llm)\n",
    "# gen_perspectives_chain = get_chain_perspective_generator(fast_llm)\n",
    "# gen_queries_chain = get_chain_queries(fast_llm)\n",
    "# gen_answer_chain = get_chain_answer(fast_llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5706a1d3",
   "metadata": {},
   "source": [
    "### Test Chains\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ee8329-896b-4085-a1fa-fec0a15937ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Generate Outline\n",
    "\n",
    "initial_outline = interview_graph.outline.invoke({\"topic\": example_topic})\n",
    "logger.info(initial_outline.as_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f801936-f6f7-44a0-bc79-4f0132fba79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Expand related topics\n",
    "\n",
    "related_subjects = await interview_graph.related_topics.ainvoke({\"topic\": example_topic})\n",
    "related_subjects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95cc42a4-45f1-470c-a3f6-20a5661d5b43",
   "metadata": {},
   "source": [
    "#### Generate Perspectives\n",
    "\n",
    "From these related subjects, we can select representative Wiki editors as \"subject matter experts\" with distinct backgrounds and affiliations. These will help distribute the search process to encourage a more well-rounded final report.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1fb5d9c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-31 23:57:34,240 [MainThread  ] [INFO ]  Survey Subjects for Topic: Impact of THE Red Cross Church IN Zimbabwe early history\n",
      "2024-03-31 23:57:37,947 [MainThread  ] [INFO ]  Retrieved 6 docs for Topic: Impact of THE Red Cross Church IN Zimbabwe early history\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Perspectives(editors=[Editor(affiliation='History of Zimbabwe Research Institute', name='Dr. Nkosi Moyo', role='Historian', description='Dr. Moyo is a renowned historian specializing in the pre-colonial and colonial history of Zimbabwe. With a focus on the Kingdom of Zimbabwe and the colonial era, Dr. Moyo will provide insights into the historical developments of the region.'), Editor(affiliation='International Red Cross and Red Crescent Movement Association', name='Sarah Patel', role='Humanitarian Advocate', description='Sarah is a dedicated humanitarian advocate with a strong commitment to protecting human rights and alleviating suffering. She will contribute perspectives on humanitarian efforts and the role of organizations in Zimbabwe.'), Editor(affiliation='Religious Harmony Council of Zimbabwe', name='Bishop Tendai Chikomo', role='Religious Leader', description='Bishop Chikomo is a respected religious leader promoting interfaith dialogue and understanding in Zimbabwe. He will focus on the religious landscape of Zimbabwe, including the diverse beliefs and practices in the country.'), Editor(affiliation='Zimbabwe Tourism Board', name='Ms. Sibongile Ndlovu', role='Tourism Expert', description=\"Ms. Ndlovu is an experienced tourism expert with a deep knowledge of Zimbabwe's cultural and natural attractions. She will provide insights into the tourism potential and unique offerings of Zimbabwe.\"), Editor(affiliation='The White Fathers Missionaries', name='Father Jean-Luc Dubois', role='Missionary', description='Father Dubois is a dedicated missionary with a long history of working in Africa, including Zimbabwe. He will share perspectives on the missionary activities and educational initiatives in the country.')])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. Generate perspectives\n",
    "perspectives = await interview_graph.survey_subjects.invoke(example_topic)\n",
    "perspectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb97bf64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-31 23:58:28,050 [MainThread  ] [INFO ]  Generating question for DrNkosiMoyo\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Swapping roles for DrNkosiMoyo\n",
      "Converted messages for DrNkosiMoyo while swapping roles: 1 messages\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "StormInterviewGraph.__init__.<locals>.<lambda>() got an unexpected keyword argument 'name'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 4. Generate questions\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m question \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m interview_graph\u001b[38;5;241m.\u001b[39mgenerate_question\u001b[38;5;241m.\u001b[39minvoke(initial_state)\n\u001b[1;32m      5\u001b[0m question[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/projects/llm/storm-researcher/storm/__init__.py:170\u001b[0m, in \u001b[0;36mStormInterviewGraph.agenerate_question\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m    162\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGenerating question for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    164\u001b[0m gn_chain \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    165\u001b[0m     RunnableLambda(swap_roles)\u001b[38;5;241m.\u001b[39mbind(name\u001b[38;5;241m=\u001b[39mname)\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;241m|\u001b[39m gen_qn_prompt\u001b[38;5;241m.\u001b[39mpartial(persona\u001b[38;5;241m=\u001b[39meditor\u001b[38;5;241m.\u001b[39mpersona)\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfast_llm\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtag_with_name\u001b[38;5;241m.\u001b[39mbind(name\u001b[38;5;241m=\u001b[39mname)\n\u001b[1;32m    169\u001b[0m )\n\u001b[0;32m--> 170\u001b[0m result:AIMessage \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m gn_chain\u001b[38;5;241m.\u001b[39mainvoke(state)\n\u001b[1;32m    171\u001b[0m state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m ([result])\n\u001b[1;32m    173\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGenerated question for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/langchain_core/runnables/base.py:2483\u001b[0m, in \u001b[0;36mRunnableSequence.ainvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   2481\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   2482\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps):\n\u001b[0;32m-> 2483\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m step\u001b[38;5;241m.\u001b[39mainvoke(\n\u001b[1;32m   2484\u001b[0m             \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m   2485\u001b[0m             \u001b[38;5;66;03m# mark each step as a child run\u001b[39;00m\n\u001b[1;32m   2486\u001b[0m             patch_config(\n\u001b[1;32m   2487\u001b[0m                 config, callbacks\u001b[38;5;241m=\u001b[39mrun_manager\u001b[38;5;241m.\u001b[39mget_child(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseq:step:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2488\u001b[0m             ),\n\u001b[1;32m   2489\u001b[0m         )\n\u001b[1;32m   2490\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[1;32m   2491\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/langchain_core/runnables/base.py:4470\u001b[0m, in \u001b[0;36mRunnableBindingBase.ainvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   4464\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mainvoke\u001b[39m(\n\u001b[1;32m   4465\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4466\u001b[0m     \u001b[38;5;28minput\u001b[39m: Input,\n\u001b[1;32m   4467\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   4468\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[1;32m   4469\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Output:\n\u001b[0;32m-> 4470\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbound\u001b[38;5;241m.\u001b[39mainvoke(\n\u001b[1;32m   4471\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m   4472\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_merge_configs(config),\n\u001b[1;32m   4473\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs},\n\u001b[1;32m   4474\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/langchain_core/runnables/base.py:3928\u001b[0m, in \u001b[0;36mRunnableLambda.ainvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   3926\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Invoke this runnable asynchronously.\"\"\"\u001b[39;00m\n\u001b[1;32m   3927\u001b[0m the_func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mafunc \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mafunc\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc\n\u001b[0;32m-> 3928\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_acall_with_config(\n\u001b[1;32m   3929\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ainvoke,\n\u001b[1;32m   3930\u001b[0m     \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m   3931\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_config(config, the_func),\n\u001b[1;32m   3932\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3933\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/langchain_core/runnables/base.py:1677\u001b[0m, in \u001b[0;36mRunnable._acall_with_config\u001b[0;34m(self, func, input, config, run_type, **kwargs)\u001b[0m\n\u001b[1;32m   1675\u001b[0m         output: Output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mcreate_task(coro, context\u001b[38;5;241m=\u001b[39mcontext)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m   1676\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1677\u001b[0m         output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m coro\n\u001b[1;32m   1678\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1679\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/langchain_core/runnables/base.py:3875\u001b[0m, in \u001b[0;36mRunnableLambda._ainvoke\u001b[0;34m(self, input, run_manager, config, **kwargs)\u001b[0m\n\u001b[1;32m   3873\u001b[0m                 output \u001b[38;5;241m=\u001b[39m chunk\n\u001b[1;32m   3874\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3875\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m acall_func_with_variable_args(\n\u001b[1;32m   3876\u001b[0m         cast(Callable, afunc), \u001b[38;5;28minput\u001b[39m, config, run_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m   3877\u001b[0m     )\n\u001b[1;32m   3878\u001b[0m \u001b[38;5;66;03m# If the output is a runnable, invoke it\u001b[39;00m\n\u001b[1;32m   3879\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, Runnable):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/langchain_core/runnables/base.py:3851\u001b[0m, in \u001b[0;36mRunnableLambda._ainvoke.<locals>.f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   3849\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m   3850\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mf\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):  \u001b[38;5;66;03m# type: ignore[no-untyped-def]\u001b[39;00m\n\u001b[0;32m-> 3851\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m run_in_executor(config, func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/langchain_core/runnables/config.py:514\u001b[0m, in \u001b[0;36mrun_in_executor\u001b[0;34m(executor_or_config, func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    501\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Run a function in an executor.\u001b[39;00m\n\u001b[1;32m    502\u001b[0m \n\u001b[1;32m    503\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    510\u001b[0m \u001b[38;5;124;03m    Output: The output of the function.\u001b[39;00m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m executor_or_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(executor_or_config, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;66;03m# Use default executor with context copied from current context\u001b[39;00m\n\u001b[0;32m--> 514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mget_running_loop()\u001b[38;5;241m.\u001b[39mrun_in_executor(\n\u001b[1;32m    515\u001b[0m         \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    516\u001b[0m         cast(Callable[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, T], partial(copy_context()\u001b[38;5;241m.\u001b[39mrun, func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)),\n\u001b[1;32m    517\u001b[0m     )\n\u001b[1;32m    519\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mget_running_loop()\u001b[38;5;241m.\u001b[39mrun_in_executor(\n\u001b[1;32m    520\u001b[0m     executor_or_config, partial(func, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs), \u001b[38;5;241m*\u001b[39margs\n\u001b[1;32m    521\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/concurrent/futures/thread.py:58\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuture\u001b[38;5;241m.\u001b[39mset_exception(exc)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/langchain_core/runnables/base.py:3845\u001b[0m, in \u001b[0;36mRunnableLambda._ainvoke.<locals>.func\u001b[0;34m(input, run_manager, config, **kwargs)\u001b[0m\n\u001b[1;32m   3839\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfunc\u001b[39m(\n\u001b[1;32m   3840\u001b[0m     \u001b[38;5;28minput\u001b[39m: Input,\n\u001b[1;32m   3841\u001b[0m     run_manager: AsyncCallbackManagerForChainRun,\n\u001b[1;32m   3842\u001b[0m     config: RunnableConfig,\n\u001b[1;32m   3843\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m   3844\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Output:\n\u001b[0;32m-> 3845\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall_func_with_variable_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3846\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_sync\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m   3847\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/langchain_core/runnables/config.py:347\u001b[0m, in \u001b[0;36mcall_func_with_variable_args\u001b[0;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[1;32m    346\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m run_manager\n\u001b[0;32m--> 347\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: StormInterviewGraph.__init__.<locals>.<lambda>() got an unexpected keyword argument 'name'"
     ]
    }
   ],
   "source": [
    "# 4. Generate questions\n",
    "\n",
    "question = await interview_graph.generate_question.invoke(initial_state)\n",
    "\n",
    "question[\"messages\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79ea824-c561-4949-bbd4-127281f3eeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35abdf40-80dc-434c-8116-d6b9cbde5572",
   "metadata": {},
   "source": [
    "## Expert Dialog\n",
    "\n",
    "Each wikipedia writer is primed to role-play using the perspectives presented above. It will ask a series of questions of a second \"domain expert\" with access to a search engine. This generate content to generate a refined outline as well as an updated index of reference documents.\n",
    "\n",
    "### Interview State\n",
    "\n",
    "The conversation is cyclic, so we will construct it within its own graph. The State will contain messages, the reference docs, and the editor (with its own \"persona\") to make it easy to parallelize these conversations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0639d2-e8a6-43d4-8e9a-eb5f4051578c",
   "metadata": {},
   "source": [
    "# Dialog Roles\n",
    "\n",
    "The graph will have two participants: the wikipedia editor (generate_question), who asks questions based on its assigned role, and a domain expert (`gen_answer_chain), who uses a search engine to answer the questions as accurately as possible.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d644c46c-4d23-49e1-9093-39b4f6c8c157",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gen_qn_prompt = get_chat_prompt_from_prompt_templates([prompts.gen_question_system_generator, prompts.generate_messages_placeholder()])\n",
    "\n",
    "\n",
    "# def swap_roles(state: InterviewState, name: str) -> InterviewState:\n",
    "\n",
    "#     # Normalize name\n",
    "#     name = cleanup_name(name)\n",
    "\n",
    "#     logger.info(f'Swapping roles for {name}')\n",
    "\n",
    "#     converted = []\n",
    "#     for message in state[\"messages\"]:\n",
    "#         if isinstance(message, AIMessage) and message.name != name:\n",
    "#             message = HumanMessage(**message.dict(exclude={\"type\"}))\n",
    "#         converted.append(message)\n",
    "    \n",
    "#     state['messages'] = converted\n",
    "    \n",
    "#     logger.info(f'Converted messages for {name} while swapping roles: {len(converted)} messages')\n",
    "#     return state\n",
    "\n",
    "\n",
    "# @as_runnable\n",
    "# async def generate_question(state: InterviewState) -> InterviewState:\n",
    "#     editor: Editor = state[\"editor\"]\n",
    "\n",
    "#     name = cleanup_name(editor.name)\n",
    "\n",
    "\n",
    "#     logger.info(f'Generating question for {name}')\n",
    "\n",
    "#     gn_chain = (\n",
    "#         RunnableLambda(swap_roles).bind(name=name)\n",
    "#         | gen_qn_prompt.partial(persona=editor.persona)\n",
    "#         | fast_llm\n",
    "#         | RunnableLambda(tag_with_name).bind(name=name)\n",
    "#     )\n",
    "#     result:AIMessage = await gn_chain.ainvoke(state)\n",
    "#     state[\"messages\"] = ([result])\n",
    "\n",
    "#     logger.info(f'Generated question for {name}')\n",
    "#     return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ac2384-a123-467f-bbc6-828be9dc04fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c95e50f7-4e2c-4307-853d-2fc8f0b3dd82",
   "metadata": {},
   "source": [
    "### Answer questions\n",
    "\n",
    "The `gen_answer_chain` first generates queries (query expansion) to answer the editor's question, then responds with citations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25baa82-da8e-41ec-b4e0-8a24c7cf737d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "queries = await gen_queries_chain.ainvoke(\n",
    "    {\"messages\": [HumanMessage(content=question[\"messages\"][0].content)]}\n",
    ")\n",
    "\n",
    "queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22273c9a-a505-40c3-bd6b-70f74393a61d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca74edcc-a272-4ec1-92fc-756bf0690e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "async def gen_answer(\n",
    "    state: InterviewState,\n",
    "    config: Optional[RunnableConfig] = None,\n",
    "    name: str = \"SubjectMatterExpert\",\n",
    "    max_str_len: int = 15000,\n",
    "):\n",
    "    name = cleanup_name(name)\n",
    "\n",
    "    logger.info(f'START - Generate answers for [{name}]')\n",
    "\n",
    "    swapped_state = swap_roles(state, name)  # Convert all other AI messages\n",
    "    \n",
    "    # Generate search engine queries\n",
    "    queries:Queries = await gen_queries_chain.ainvoke(swapped_state)\n",
    "\n",
    "    logger.info(f\"Got {len(queries.queries)} search engine queries for [{name}] -\\n\\t {queries.queries}\")\n",
    "\n",
    "    # Run search engine\n",
    "    query_results = await search_engine.abatch(\n",
    "        queries.queries, config, return_exceptions=True\n",
    "    )\n",
    "    successful_results = [\n",
    "        res for res in query_results if not isinstance(res, Exception)\n",
    "    ]\n",
    "\n",
    "    all_query_results = {\n",
    "        res[\"url\"]: res[\"content\"] for results in successful_results for res in results\n",
    "    }\n",
    "    \n",
    "    logger.info(f\"Got {len(successful_results)} search engine results for [{name}] - \\n\\t {all_query_results}\")\n",
    "\n",
    "    # We could be more precise about handling max token length if we wanted to here\n",
    "    dumped_successful_results = json.dumps(all_query_results)[:max_str_len]\n",
    "    \n",
    "    logger.info(f\"Dumped {len(dumped_successful_results)} characters for [{name}] - \\n\\t {dumped_successful_results}\")\n",
    "    \n",
    "    # Append Questions from Wikipedia and Answers from the search engine\n",
    "    ai_message_for_queries: AIMessage = get_ai_message(json.dumps(queries.as_dict()))\n",
    "    \n",
    "    tool_results_message = generate_human_message(dumped_successful_results)\n",
    "    \n",
    "    logger.debug(f\"Got {ai_message_for_queries} for [{name}]\")\n",
    "    \n",
    "    # tool_call = queries[\"raw\"].additional_kwargs[\"tool_calls\"][0]\n",
    "    # tool_id = tool_call[\"id\"]\n",
    "\n",
    "    # tool_message = ToolMessage(tool_call_id=tool_id, content=dumped)\n",
    "    \n",
    "\n",
    "    swapped_state[\"messages\"].extend([ai_message_for_queries, tool_results_message])\n",
    "    \n",
    "    # Only update the shared state with the final answer to avoid\n",
    "    # polluting the dialogue history with intermediate messages\n",
    "    try:\n",
    "        generated: AnswerWithCitations = await gen_answer_chain.ainvoke(swapped_state)\n",
    "        \n",
    "        logger.info(f\"Genreted final answer {generated} for [{name}] - \\n\\t {generated.as_str}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error generating answer for [{name}] - {e}\")\n",
    "        generated = AnswerWithCitations(answer=\"\", cited_urls=[])\n",
    "    \n",
    "    cited_urls = set(generated.cited_urls)\n",
    "    \n",
    "    # Save the retrieved information to a the shared state for future reference\n",
    "    cited_references = {k: v for k, v in all_query_results.items() if k in cited_urls}\n",
    "    \n",
    "    formatted_message = AIMessage(name=name, content=generated.as_str)\n",
    "    # Add message to shared state\n",
    "    # state[\"messages\"].append(formatted_message)\n",
    "    state[\"messages\"] = add_messages(state[\"messages\"], [formatted_message])\n",
    "    \n",
    "    # Update references with cited references\n",
    "    state[\"references\"] = update_references(state[\"references\"], cited_references)\n",
    "\n",
    "    logger.info(f'END - generate answer for [{name}]')\n",
    "    \n",
    "    return state\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934aafd6-7f0d-4a1b-8a52-45b89d8b3742",
   "metadata": {},
   "outputs": [],
   "source": [
    "intial_messages = [prompts.initial_question, generate_human_message(question[\"messages\"][0].content)]\n",
    "\n",
    "initial_state: InterviewState = {\n",
    "    \"editor\": perspectives.editors[0],\n",
    "    \"messages\": intial_messages,\n",
    "    \"references\": {}\n",
    "}\n",
    "\n",
    "example_answer = await gen_answer(initial_state)\n",
    "example_answer[\"messages\"][-1].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d779cb51",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_answer[\"messages\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998671bf-958d-44c0-8421-523a71bea01a",
   "metadata": {},
   "source": [
    "# Construct the Interview Graph\n",
    "\n",
    "Now that we've defined the editor and domain expert, we can compose them in a graph.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80347f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4800f958-00e0-4913-a246-c34dc3f0a00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "builder = StateGraph(InterviewState)\n",
    "\n",
    "builder.add_node(\"ask_question\", generate_question)\n",
    "builder.add_node(\"answer_question\", gen_answer)\n",
    "builder.add_conditional_edges(\"answer_question\", route_messages)\n",
    "builder.add_edge(\"ask_question\", \"answer_question\")\n",
    "\n",
    "builder.set_entry_point(\"ask_question\")\n",
    "interview_graph = builder.compile().with_config(run_name=\"Conduct Interviews\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c405fc-5b1c-44f5-b860-a10fe0d6616d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "# comment out if you have not installed pygraphviz\n",
    "# Image(interview_graph.get_graph().draw_png())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d47607-6c0b-493a-aebc-48356d0e0302",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "final_step = None\n",
    "\n",
    "initial_state = {\n",
    "    \"editor\": perspectives.editors[0],\n",
    "    \"messages\": [\n",
    "        AIMessage(\n",
    "            content=f\"So you said you were writing an article on {example_topic}?\",\n",
    "            name=\"SubjectMatterExpert\",\n",
    "        )\n",
    "    ],\n",
    "}\n",
    "async for step in interview_graph.astream(initial_state):\n",
    "    name = next(iter(step))\n",
    "    logger.info(f\"Processing step: {name}\")\n",
    "    logger.debug(\"-- \", str(step[name][\"messages\"])[:300])\n",
    "    if END in step:\n",
    "        final_step = step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7a97ec-09a1-4873-b559-275526971a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_state = next(iter(final_step.values()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22e50cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e14dcb",
   "metadata": {},
   "source": [
    "## Refine Outline\n",
    "\n",
    "At this point in STORM, we've conducted a large amount of research from different perspectives. It's time to refine the original outline based on these investigations. Below, create a chain using the LLM with a long context window to update the original outline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c284eb72-3856-406d-8582-5a1c92fd292e",
   "metadata": {},
   "outputs": [],
   "source": [
    "refine_outline_prompt = get_chat_prompt_from_prompt_templates([prompts.pmt_s_refine_outline, prompts.pmt_h_refine_outline])\n",
    "\n",
    "# Using turbo preview since the context can get quite long\n",
    "refine_outline_chain = get_chain_with_outputparser(refine_outline_prompt, fast_llm, outline_parser)\\\n",
    "    .with_config(run_name=\"Refine Outline\")\n",
    "\n",
    "# refine_outline_prompt.partial(format_instructions=outline_parser.get_format_instructions()) | long_context_llm | outline_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19e58c5-086f-49ba-b921-791669d04b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "refined_outline = refine_outline_chain.invoke(\n",
    "    {\n",
    "        \"topic\": example_topic,\n",
    "        \"old_outline\": initial_outline.as_str,\n",
    "        \"conversations\": \"\\n\\n\".join(\n",
    "            f\"### {m.name}\\n\\n{m.content}\" for m in final_state[\"messages\"]\n",
    "        ),\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c397dc5-e614-4a7f-9f78-67dffc9b8c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(refined_outline.as_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8b76b6",
   "metadata": {},
   "source": [
    "## Generate Article\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563b2a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=250, chunk_overlap=25\n",
    ")\n",
    "\n",
    "\n",
    "reference_docs = [\n",
    "    Document(page_content=v, metadata={\"source\": k})\n",
    "    for k, v in final_state[\"references\"].items()\n",
    "]\n",
    "\n",
    "logger.info(f\"Number of references: {len(reference_docs)}\")\n",
    "\n",
    "vectorstore = get_inmemory_db(reference_docs, embeddings)\n",
    "\n",
    "# Get contents of the references\n",
    "full_docs = await fetch_pages_from_refs(reference_docs[:])\n",
    "\n",
    "# Summarize\n",
    "summaries = summarize_full_docs(fast_llm, example_topic, full_docs)\n",
    "\n",
    "# f1 = list(chain.from_iterable(summaries.values()))\n",
    "f1 = summaries.values()\n",
    "full_split_docs = text_splitter.split_documents(f1)\n",
    "\n",
    "vectorstore.add_documents(full_split_docs)\n",
    "\n",
    "retriever = vectorstore.as_retriever()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8c6fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_split_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = retriever.invoke(\"What did the red cross do in Zimbabwe?\")\n",
    "print(d1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e708d31a",
   "metadata": {},
   "source": [
    "#### Generate Sections\n",
    "\n",
    "Now you can generate the sections using the indexed docs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524f4c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "section_writer_prompt = get_chat_prompt_from_prompt_templates([prompts.pmt_s_section_writer, prompts.pmt_h_section_writer])\n",
    "\n",
    "\n",
    "async def retrieve(inputs: dict):\n",
    "    docs = await retriever.ainvoke(inputs[\"topic\"] + \": \" + inputs[\"section\"])\n",
    "    formatted = \"\\n\".join(\n",
    "        [\n",
    "            f'<Document href=\"{doc.metadata[\"source\"]}\"/>\\n{doc.page_content}\\n</Document>'\n",
    "            for doc in docs\n",
    "        ]\n",
    "    )\n",
    "    return {\"docs\": formatted, **inputs}\n",
    "\n",
    "wiki_parser = get_pydantic_parser(WikiSection)\n",
    "\n",
    "section_writer = (\n",
    "    retrieve\n",
    "    | section_writer_prompt.partial(format_instructions=wiki_parser.get_format_instructions())\n",
    "    | long_context_llm\n",
    "    | wiki_parser\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03723e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "section = await section_writer.ainvoke(\n",
    "    {\n",
    "        \"outline\": refined_outline.as_str,\n",
    "        \"section\": refined_outline.sections[1].section_title,\n",
    "        \"topic\": example_topic,\n",
    "    }\n",
    ")\n",
    "print(section.as_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b75ea24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0afd728d",
   "metadata": {},
   "source": [
    "#### Generate final article\n",
    "\n",
    "Now we can rewrite the draft to appropriately group all the citations and maintain a consistent voice.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1831e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts.pmt_s_writer = generate_system_chat_prompt(\"\"\"\n",
    "You are an expert Wikipedia author. Write the complete wiki article on {topic} using the following section drafts:\n",
    "\n",
    "{draft}\n",
    "\n",
    "Strictly follow Wikipedia format guidelines.\n",
    "\"\"\")\n",
    "\n",
    "prompts.pmt_h_writer = generate_human_chat_prompt(\"\"\"\n",
    "Write the complete Wiki article using markdown format. Organize citations using footnotes like \"[1]\",\"\" avoiding duplicates in the footer. Include URLs in the footer.'\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05089f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "writer_prompt = get_chat_prompt_from_prompt_templates([prompts.pmt_s_writer, prompts.pmt_h_writer])\n",
    "\n",
    "writer = writer_prompt | long_context_llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6e6e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for tok in writer.stream({\"topic\": example_topic, \"draft\": section.as_str}):\n",
    "    print(tok, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6ab734",
   "metadata": {},
   "source": [
    "## Final Flow\n",
    "\n",
    "Now it's time to string everything together. We will have 6 main stages in sequence:\n",
    ".\n",
    "\n",
    "1. Generate the initial outline + perspectives\n",
    "2. Batch converse with each perspective to expand the content for the article\n",
    "3. Refine the outline based on the conversations\n",
    "4. Index the reference docs from the conversations\n",
    "5. Write the individual sections of the article\n",
    "6. Write the final wiki\n",
    "\n",
    "The state tracks the outputs of each stage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e775ba8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResearchState(TypedDict):\n",
    "    topic: str\n",
    "    outline: Outline\n",
    "    editors: List[Editor]\n",
    "    interview_results: List[InterviewState]\n",
    "    # The final sections output\n",
    "    sections: List[WikiSection]\n",
    "    article: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1854d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "\n",
    "async def initialize_research(state: ResearchState):\n",
    "    topic = state[\"topic\"]\n",
    "    coros = (\n",
    "        generate_outline_direct.ainvoke({\"topic\": topic}),\n",
    "        survey_subjects.ainvoke(topic),\n",
    "    )\n",
    "    results = await asyncio.gather(*coros)\n",
    "    return {\n",
    "        **state,\n",
    "        \"outline\": results[0],\n",
    "        \"editors\": results[1].editors,\n",
    "    }\n",
    "\n",
    "\n",
    "async def conduct_interviews(state: ResearchState):\n",
    "    topic = state[\"topic\"]\n",
    "    initial_states = [\n",
    "        {\n",
    "            \"editor\": editor,\n",
    "            \"messages\": [\n",
    "                AIMessage(\n",
    "                    content=f\"So you said you were writing an article on {topic}?\",\n",
    "                    name=\"SubjectMatterExpert\",\n",
    "                )\n",
    "            ],\n",
    "        }\n",
    "        for editor in state[\"editors\"]\n",
    "    ]\n",
    "    # We call in to the sub-graph here to parallelize the interviews\n",
    "    interview_results = await interview_graph.abatch(initial_states)\n",
    "\n",
    "    return {\n",
    "        **state,\n",
    "        \"interview_results\": interview_results,\n",
    "    }\n",
    "\n",
    "\n",
    "def format_conversation(interview_state):\n",
    "    messages = interview_state[\"messages\"]\n",
    "    convo = \"\\n\".join(f\"{m.name}: {m.content}\" for m in messages)\n",
    "    return f'Conversation with {interview_state[\"editor\"].name}\\n\\n' + convo\n",
    "\n",
    "\n",
    "async def refine_outline(state: ResearchState):\n",
    "    convos = \"\\n\\n\".join(\n",
    "        [\n",
    "            format_conversation(interview_state)\n",
    "            for interview_state in state[\"interview_results\"]\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    updated_outline = await refine_outline_chain.ainvoke(\n",
    "        {\n",
    "            \"topic\": state[\"topic\"],\n",
    "            \"old_outline\": state[\"outline\"].as_str,\n",
    "            \"conversations\": convos,\n",
    "        }\n",
    "    )\n",
    "    return {**state, \"outline\": updated_outline}\n",
    "\n",
    "\n",
    "async def index_references(state: ResearchState):\n",
    "    all_docs = []\n",
    "    for interview_state in state[\"interview_results\"]:\n",
    "        reference_docs = [\n",
    "            Document(page_content=v, metadata={\"source\": k})\n",
    "            for k, v in interview_state[\"references\"].items()\n",
    "        ]\n",
    "        all_docs.extend(reference_docs)\n",
    "    await vectorstore.aadd_documents(all_docs)\n",
    "    return state\n",
    "\n",
    "\n",
    "async def write_sections(state: ResearchState):\n",
    "    outline = state[\"outline\"]\n",
    "    sections = await section_writer.abatch(\n",
    "        [\n",
    "            {\n",
    "                \"outline\": refined_outline.as_str,\n",
    "                \"section\": section.section_title,\n",
    "                \"topic\": state[\"topic\"],\n",
    "            }\n",
    "            for section in outline.sections\n",
    "        ]\n",
    "    )\n",
    "    return {\n",
    "        **state,\n",
    "        \"sections\": sections,\n",
    "    }\n",
    "\n",
    "\n",
    "async def write_article(state: ResearchState):\n",
    "    topic = state[\"topic\"]\n",
    "    sections = state[\"sections\"]\n",
    "    draft = \"\\n\\n\".join([section.as_str for section in sections])\n",
    "    article = await writer.ainvoke({\"topic\": topic, \"draft\": draft})\n",
    "    return {\n",
    "        **state,\n",
    "        \"article\": article,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87881e3",
   "metadata": {},
   "source": [
    "#### Create the graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3b4be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "builder_of_storm = StateGraph(ResearchState)\n",
    "\n",
    "nodes = [\n",
    "    (\"init_research\", initialize_research),\n",
    "    (\"conduct_interviews\", conduct_interviews),\n",
    "    (\"refine_outline\", refine_outline),\n",
    "    (\"index_references\", index_references),\n",
    "    (\"write_sections\", write_sections),\n",
    "    (\"write_article\", write_article),\n",
    "]\n",
    "for i in range(len(nodes)):\n",
    "    name, node = nodes[i]\n",
    "    builder_of_storm.add_node(name, node)\n",
    "    if i > 0:\n",
    "        builder_of_storm.add_edge(nodes[i - 1][0], name)\n",
    "\n",
    "builder_of_storm.set_entry_point(nodes[0][0])\n",
    "builder_of_storm.set_finish_point(nodes[-1][0])\n",
    "storm = builder_of_storm.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a815f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# async for step in storm.astream(\n",
    "#     {\n",
    "#         \"topic\": example_topic,\n",
    "#     }\n",
    "# ):\n",
    "#     name = next(iter(step))\n",
    "#     print(name)\n",
    "#     logger.info(\"-- \", str(step[name])[:300])\n",
    "#     if END in step:\n",
    "#         results = step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bef7021",
   "metadata": {},
   "outputs": [],
   "source": [
    "article = results[END][\"article\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b094067",
   "metadata": {},
   "source": [
    "## Render the Wiki\n",
    "\n",
    "Now we can render the final wiki page!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7750c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "# We will down-header the sections to create less confusion in this notebook\n",
    "Markdown(article.replace(\"\\n#\", \"\\n##\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e24611",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write article to file\n",
    "with open(f\"{example_topic}_article.md\", \"w\") as f:\n",
    "    f.write(article.replace(\"\\n#\", \"\\n##\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b97c926",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8466155",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
